{
  long file=batch.getBatchKey().file;
  TreeReader[] treeReaders=new TreeReader[numCols];
  for (int i=0; i < numCols; i++) {
    int columnIndex=batch.getColumnIxs()[i];
    ColumnStreamData[] streamBuffers=batch.getColumnData()[i];
    OrcProto.Type columnType=types.get(columnIndex);
    OrcProto.ColumnEncoding columnEncoding=encodings.get(columnIndex);
    ColumnStreamData present=streamBuffers[Kind.PRESENT_VALUE], data=streamBuffers[Kind.DATA_VALUE], dictionary=streamBuffers[Kind.DICTIONARY_DATA_VALUE], lengths=streamBuffers[Kind.LENGTH_VALUE], secondary=streamBuffers[Kind.SECONDARY_VALUE];
switch (columnType.getKind()) {
case BINARY:
      treeReaders[i]=BinaryStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setLengthStream(lengths).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
    break;
case BOOLEAN:
  treeReaders[i]=BooleanStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).build();
break;
case BYTE:
treeReaders[i]=ByteStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).build();
break;
case SHORT:
treeReaders[i]=ShortStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
case INT:
treeReaders[i]=IntStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
case LONG:
treeReaders[i]=LongStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).setColumnEncoding(columnEncoding).skipCorrupt(skipCorrupt).build();
break;
case FLOAT:
treeReaders[i]=FloatStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).build();
break;
case DOUBLE:
treeReaders[i]=DoubleStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).build();
break;
case CHAR:
treeReaders[i]=CharStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setMaxLength(columnType.getMaximumLength()).setPresentStream(present).setDataStream(data).setLengthStream(lengths).setDictionaryStream(dictionary).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
case VARCHAR:
treeReaders[i]=VarcharStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setMaxLength(columnType.getMaximumLength()).setPresentStream(present).setDataStream(data).setLengthStream(lengths).setDictionaryStream(dictionary).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
case STRING:
treeReaders[i]=StringStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setLengthStream(lengths).setDictionaryStream(dictionary).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
case DECIMAL:
treeReaders[i]=DecimalStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPrecision(columnType.getPrecision()).setScale(columnType.getScale()).setPresentStream(present).setValueStream(data).setScaleStream(secondary).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
case TIMESTAMP:
treeReaders[i]=TimestampStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setSecondsStream(data).setNanosStream(secondary).setCompressionCodec(codec).setColumnEncoding(columnEncoding).skipCorrupt(skipCorrupt).build();
break;
case DATE:
treeReaders[i]=DateStreamReader.builder().setFileId(file).setColumnIndex(columnIndex).setPresentStream(present).setDataStream(data).setCompressionCodec(codec).setColumnEncoding(columnEncoding).build();
break;
default :
throw new UnsupportedOperationException("Data type not supported yet! " + columnType);
}
}
return treeReaders;
}
