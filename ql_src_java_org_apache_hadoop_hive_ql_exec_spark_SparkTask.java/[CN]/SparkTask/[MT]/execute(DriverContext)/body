{
  int rc=0;
  SparkSession sparkSession=null;
  SparkSessionManager sparkSessionManager=null;
  try {
    printConfigInfo();
    sparkSessionManager=SparkSessionManagerImpl.getInstance();
    sparkSession=SparkUtilities.getSparkSession(conf,sparkSessionManager);
    SparkWork sparkWork=getWork();
    sparkWork.setRequiredCounterPrefix(getCounterPrefixes());
    SparkJobRef jobRef=sparkSession.submit(driverContext,sparkWork);
    SparkJobStatus sparkJobStatus=jobRef.getSparkJobStatus();
    if (sparkJobStatus != null) {
      SparkJobMonitor monitor=new SparkJobMonitor(sparkJobStatus);
      rc=monitor.startMonitor();
      sparkCounters=sparkJobStatus.getCounter();
      SparkStatistics sparkStatistics=sparkJobStatus.getSparkStatistics();
      if (LOG.isInfoEnabled() && sparkStatistics != null) {
        LOG.info(String.format("=====Spark Job[%s] statistics=====",jobRef.getJobId()));
        logSparkStatistic(sparkStatistics);
      }
      sparkJobStatus.cleanup();
    }
  }
 catch (  Exception e) {
    String msg="Failed to execute spark task, with exception '" + Utilities.getNameMessage(e) + "'";
    console.printError(msg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    LOG.error(msg,e);
    rc=1;
  }
 finally {
    if (sparkSession != null && sparkSessionManager != null) {
      rc=close(rc);
      try {
        sparkSessionManager.returnSession(sparkSession);
      }
 catch (      HiveException ex) {
        LOG.error("Failed to return the session to SessionManager",ex);
      }
    }
  }
  return rc;
}
