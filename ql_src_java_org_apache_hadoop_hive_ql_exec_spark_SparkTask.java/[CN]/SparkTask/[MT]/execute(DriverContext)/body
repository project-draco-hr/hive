{
  int rc=1;
  SparkSession sparkSession=null;
  SparkSessionManager sparkSessionManager=null;
  try {
    printConfigInfo();
    sparkSessionManager=SparkSessionManagerImpl.getInstance();
    sparkSession=SessionState.get().getSparkSession();
    if (conf.getSparkConfigUpdated()) {
      sparkSessionManager.closeSession(sparkSession);
      sparkSession=null;
      conf.setSparkConfigUpdated(false);
    }
    sparkSession=sparkSessionManager.getSession(sparkSession,conf,true);
    SessionState.get().setSparkSession(sparkSession);
    SparkWork sparkWork=getWork();
    sparkWork.setRequiredCounterPrefix(getCounterPrefixes());
    SparkJobRef jobRef=sparkSession.submit(driverContext,sparkWork);
    SparkJobStatus sparkJobStatus=jobRef.getSparkJobStatus();
    if (sparkJobStatus != null) {
      sparkCounters=sparkJobStatus.getCounter();
      SparkJobMonitor monitor=new SparkJobMonitor(sparkJobStatus);
      monitor.startMonitor();
      SparkStatistics sparkStatistics=sparkJobStatus.getSparkStatistics();
      if (LOG.isInfoEnabled() && sparkStatistics != null) {
        LOG.info(String.format("=====Spark Job[%s] statistics=====",jobRef.getJobId()));
        logSparkStatistic(sparkStatistics);
      }
      sparkJobStatus.cleanup();
    }
    rc=0;
  }
 catch (  Exception e) {
    LOG.error("Failed to execute spark task.",e);
    return 1;
  }
 finally {
    if (sparkSession != null && sparkSessionManager != null) {
      rc=close(rc);
      try {
        sparkSessionManager.returnSession(sparkSession);
      }
 catch (      HiveException ex) {
        LOG.error("Failed to return the session to SessionManager",ex);
      }
    }
  }
  return rc;
}
