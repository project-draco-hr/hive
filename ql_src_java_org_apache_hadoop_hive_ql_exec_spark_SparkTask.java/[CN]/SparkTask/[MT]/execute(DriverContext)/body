{
  int rc=0;
  SparkSession sparkSession=null;
  SparkSessionManager sparkSessionManager=null;
  try {
    printConfigInfo();
    sparkSessionManager=SparkSessionManagerImpl.getInstance();
    sparkSession=SparkUtilities.getSparkSession(conf,sparkSessionManager);
    SparkWork sparkWork=getWork();
    sparkWork.setRequiredCounterPrefix(getOperatorCounters());
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.SPARK_SUBMIT_JOB);
    SparkJobRef jobRef=sparkSession.submit(driverContext,sparkWork);
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.SPARK_SUBMIT_JOB);
    addToHistory(jobRef);
    rc=jobRef.monitorJob();
    SparkJobStatus sparkJobStatus=jobRef.getSparkJobStatus();
    if (rc == 0) {
      SparkStatistics sparkStatistics=sparkJobStatus.getSparkStatistics();
      if (LOG.isInfoEnabled() && sparkStatistics != null) {
        LOG.info(String.format("=====Spark Job[%s] statistics=====",jobRef.getJobId()));
        logSparkStatistic(sparkStatistics);
      }
      LOG.info("Execution completed successfully");
    }
 else     if (rc == 2) {
      jobRef.cancelJob();
    }
    sparkJobStatus.cleanup();
  }
 catch (  Exception e) {
    String msg="Failed to execute spark task, with exception '" + Utilities.getNameMessage(e) + "'";
    console.printError(msg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    LOG.error(msg,e);
    rc=1;
  }
 finally {
    Utilities.clearWork(conf);
    if (sparkSession != null && sparkSessionManager != null) {
      rc=close(rc);
      try {
        sparkSessionManager.returnSession(sparkSession);
      }
 catch (      HiveException ex) {
        LOG.error("Failed to return the session to SessionManager",ex);
      }
    }
  }
  return rc;
}
