{
  if (jobConf == null) {
    jobConf=KryoSerializer.deserializeJobConf(this.buffer);
    jobConf.set("mapred.reducer.class",ExecReducer.class.getName());
    reducer=new ExecReducer();
    reducer.configure(jobConf);
    collector=new SparkCollector();
  }
  collector.clear();
  Map<BytesWritable,List<BytesWritable>> clusteredRows=new HashMap<BytesWritable,List<BytesWritable>>();
  while (it.hasNext()) {
    Tuple2<BytesWritable,BytesWritable> input=it.next();
    BytesWritable key=input._1();
    BytesWritable value=input._2();
    System.out.println("reducer row: " + key + "/"+ value);
    List<BytesWritable> valueList=clusteredRows.get(key);
    if (valueList == null) {
      valueList=new ArrayList<BytesWritable>();
      clusteredRows.put(key,valueList);
    }
    valueList.add(value);
  }
  for (  Map.Entry<BytesWritable,List<BytesWritable>> entry : clusteredRows.entrySet()) {
    reducer.reduce(entry.getKey(),entry.getValue().iterator(),collector,Reporter.NULL);
  }
  reducer.close();
  return collector.getResult();
}
