{
  String dbName="default";
  String tblName="cws";
  List<String> colNames=Arrays.asList("a","b");
  String columnNamesProperty="a,b";
  String columnTypesProperty="int:string";
  executeStatementOnDriver("drop table if exists " + tblName,driver);
  executeStatementOnDriver("CREATE TABLE " + tblName + "(a INT, b STRING) "+ " CLUSTERED BY(a) INTO 1 BUCKETS"+ " STORED AS ORC  TBLPROPERTIES ('transactional'='true') ",driver);
  HiveEndPoint endPt=new HiveEndPoint(null,dbName,tblName,null);
  DelimitedInputWriter writer=new DelimitedInputWriter(new String[]{"a","b"},",",endPt);
  StreamingConnection connection=endPt.newConnection(false,"UT_" + Thread.currentThread().getName());
  try {
    for (int i=0; i < 2; i++) {
      writeBatch(connection,writer,false);
    }
    writeBatch(connection,writer,true);
    TxnStore txnHandler=TxnUtils.getTxnStore(conf);
    txnHandler.compact(new CompactionRequest(dbName,tblName,CompactionType.MAJOR));
    Worker t=new Worker();
    t.setThreadId((int)t.getId());
    t.setHiveConf(conf);
    AtomicBoolean stop=new AtomicBoolean(true);
    AtomicBoolean looped=new AtomicBoolean();
    t.init(stop,looped);
    t.run();
    IMetaStoreClient msClient=new HiveMetaStoreClient(conf);
    Table table=msClient.getTable(dbName,tblName);
    FileSystem fs=FileSystem.get(conf);
    FileStatus[] stat=fs.listStatus(new Path(table.getSd().getLocation()),AcidUtils.baseFileFilter);
    if (1 != stat.length) {
      Assert.fail("Expecting 1 file \"base_0000004\" and found " + stat.length + " files "+ Arrays.toString(stat));
    }
    String name=stat[0].getPath().getName();
    Assert.assertEquals(name,"base_0000004");
    checkExpectedTxnsPresent(stat[0].getPath(),null,columnNamesProperty,columnTypesProperty,0,1L,4L);
  }
  finally {
    connection.close();
  }
}
