{
  Map<String,String> parameters=Maps.newHashMap();
  parameters.put(serdeConstants.SERIALIZATION_CLASS,"org.apache.hadoop.hive.serde2.thrift.test.IntString");
  parameters.put(serdeConstants.SERIALIZATION_FORMAT,"org.apache.thrift.protocol.TBinaryProtocol");
  SerDeInfo serDeInfo=new SerDeInfo(null,"org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer",parameters);
  StorageDescriptor sd=new StorageDescriptor(new ArrayList<FieldSchema>(),"location","org.apache.hadoop.mapred.TextInputFormat","org.apache.hadoop.mapred.TextOutputFormat",false,-1,serDeInfo,new ArrayList<String>(),new ArrayList<Order>(),new HashMap<String,String>());
  org.apache.hadoop.hive.metastore.api.Table apiTable=new org.apache.hadoop.hive.metastore.api.Table("test_tblname","test_dbname","test_owner",0,0,0,sd,new ArrayList<FieldSchema>(),new HashMap<String,String>(),"viewOriginalText","viewExpandedText",TableType.EXTERNAL_TABLE.name());
  Table table=new Table(apiTable);
  List<HCatFieldSchema> expectedHCatSchema=Lists.newArrayList(new HCatFieldSchema("myint",HCatFieldSchema.Type.INT,null),new HCatFieldSchema("mystring",HCatFieldSchema.Type.STRING,null),new HCatFieldSchema("underscore_int",HCatFieldSchema.Type.INT,null));
  Assert.assertEquals(new HCatSchema(expectedHCatSchema),HCatUtil.getTableSchemaWithPtnCols(table));
}
