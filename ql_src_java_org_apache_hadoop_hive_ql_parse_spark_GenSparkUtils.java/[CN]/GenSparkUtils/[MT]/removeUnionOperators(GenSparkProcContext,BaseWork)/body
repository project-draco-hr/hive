{
  List<Operator<?>> roots=new ArrayList<Operator<?>>();
  if (work instanceof MapWork) {
    roots.addAll(((MapWork)work).getAliasToWork().values());
  }
 else {
    roots.addAll(work.getAllRootOperators());
  }
  if (work.getDummyOps() != null) {
    roots.addAll(work.getDummyOps());
  }
  List<Operator<?>> newRoots=SerializationUtilities.cloneOperatorTree(roots);
  Iterator<Operator<?>> newRootsIt=newRoots.iterator();
  for (  Operator<?> root : roots) {
    Operator<?> newRoot=newRootsIt.next();
    List<Operator<?>> newOpQueue=new LinkedList<Operator<?>>();
    collectOperators(newRoot,newOpQueue);
    List<Operator<?>> opQueue=new LinkedList<Operator<?>>();
    collectOperators(root,opQueue);
    Iterator<Operator<?>> newOpQueueIt=newOpQueue.iterator();
    for (    Operator<?> op : opQueue) {
      Operator<?> newOp=newOpQueueIt.next();
      if (context.rootToWorkMap.containsKey(op)) {
        context.rootToWorkMap.put(newOp,context.rootToWorkMap.get(op));
      }
      if (op instanceof FileSinkOperator) {
        List<FileSinkOperator> fileSinkList=context.fileSinkMap.get(op);
        if (fileSinkList == null) {
          fileSinkList=new LinkedList<FileSinkOperator>();
        }
        fileSinkList.add((FileSinkOperator)newOp);
        context.fileSinkMap.put((FileSinkOperator)op,fileSinkList);
      }
 else       if (op instanceof SparkPartitionPruningSinkOperator) {
        SparkPartitionPruningSinkOperator oldPruningSink=(SparkPartitionPruningSinkOperator)op;
        SparkPartitionPruningSinkOperator newPruningSink=(SparkPartitionPruningSinkOperator)newOp;
        newPruningSink.getConf().setTableScan(oldPruningSink.getConf().getTableScan());
        context.pruningSinkSet.add(newPruningSink);
        context.pruningSinkSet.remove(oldPruningSink);
      }
    }
  }
  Map<Operator<?>,Operator<?>> replacementMap=new HashMap<Operator<?>,Operator<?>>();
  List<HashTableDummyOperator> dummyOps=new LinkedList<HashTableDummyOperator>();
  Iterator<Operator<?>> it=newRoots.iterator();
  for (  Operator<?> orig : roots) {
    Operator<?> newRoot=it.next();
    if (newRoot instanceof HashTableDummyOperator) {
      dummyOps.add((HashTableDummyOperator)newRoot);
      it.remove();
    }
 else {
      replacementMap.put(orig,newRoot);
    }
  }
  Deque<Operator<?>> operators=new LinkedList<Operator<?>>();
  operators.addAll(newRoots);
  Set<Operator<?>> seen=new HashSet<Operator<?>>();
  while (!operators.isEmpty()) {
    Operator<?> current=operators.pop();
    seen.add(current);
    if (current instanceof UnionOperator) {
      Operator<?> parent=null;
      int count=0;
      for (      Operator<?> op : current.getParentOperators()) {
        if (seen.contains(op)) {
          ++count;
          parent=op;
        }
      }
      Preconditions.checkArgument(count <= 1,"AssertionError: expected count to be <= 1, but was " + count);
      if (parent == null) {
        replacementMap.put(current,current.getChildOperators().get(0));
      }
 else {
        parent.removeChildAndAdoptItsChildren(current);
      }
    }
    if (current instanceof FileSinkOperator || current instanceof ReduceSinkOperator) {
      current.setChildOperators(null);
    }
 else {
      operators.addAll(current.getChildOperators());
    }
  }
  work.setDummyOps(dummyOps);
  work.replaceRoots(replacementMap);
}
