{
  Preconditions.checkArgument(!root.getParentOperators().isEmpty(),"AssertionError: expected root.getParentOperators() to be non-empty");
  ReduceWork reduceWork=new ReduceWork("Reducer " + (++sequenceNumber));
  logger.debug("Adding reduce work (" + reduceWork.getName() + ") for "+ root);
  reduceWork.setReducer(root);
  reduceWork.setNeedsTagging(GenMapRedUtils.needsTagging(reduceWork));
  Preconditions.checkArgument(context.parentOfRoot instanceof ReduceSinkOperator,"AssertionError: expected context.parentOfRoot to be an instance of ReduceSinkOperator, but was " + context.parentOfRoot.getClass().getName());
  ReduceSinkOperator reduceSink=(ReduceSinkOperator)context.parentOfRoot;
  reduceWork.setNumReduceTasks(reduceSink.getConf().getNumReducers());
  setupReduceSink(context,reduceWork,reduceSink);
  sparkWork.add(reduceWork);
  SparkEdgeProperty edgeProp=new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_GROUP,reduceWork.getNumReduceTasks());
  String sortOrder=Strings.nullToEmpty(reduceSink.getConf().getOrder()).trim();
  if (!sortOrder.isEmpty() && isSortNecessary(reduceSink)) {
    edgeProp.setShuffleSort();
  }
  if (reduceWork.getReducer() instanceof JoinOperator) {
    edgeProp.setMRShuffle();
  }
  FileSinkOperator fso=getChildOperator(reduceWork.getReducer(),FileSinkOperator.class);
  if (fso != null) {
    String bucketCount=fso.getConf().getTableInfo().getProperties().getProperty(hive_metastoreConstants.BUCKET_COUNT);
    if (bucketCount != null && Integer.valueOf(bucketCount) > 1) {
      edgeProp.setMRShuffle();
    }
  }
  sparkWork.connect(context.preceedingWork,reduceWork,edgeProp);
  context.connectedReduceSinks.add(reduceSink);
  return reduceWork;
}
