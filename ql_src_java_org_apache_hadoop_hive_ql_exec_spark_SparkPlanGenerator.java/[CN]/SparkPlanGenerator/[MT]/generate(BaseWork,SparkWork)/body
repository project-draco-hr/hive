{
  initStatsPublisher(work);
  JobConf newJobConf=cloneJobConf(work);
  checkSpecs(work,newJobConf);
  byte[] confBytes=KryoSerializer.serializeJobConf(newJobConf);
  boolean caching=isCachingWork(work,sparkWork);
  if (work instanceof MapWork) {
    MapTran mapTran=new MapTran(caching);
    HiveMapFunction mapFunc=new HiveMapFunction(confBytes,sparkReporter);
    mapTran.setMapFunction(mapFunc);
    return mapTran;
  }
 else   if (work instanceof ReduceWork) {
    ReduceTran reduceTran=new ReduceTran(caching);
    HiveReduceFunction reduceFunc=new HiveReduceFunction(confBytes,sparkReporter);
    reduceTran.setReduceFunction(reduceFunc);
    return reduceTran;
  }
 else {
    throw new IllegalStateException("AssertionError: expected either MapWork or ReduceWork, " + "but found " + work.getClass().getName());
  }
}
