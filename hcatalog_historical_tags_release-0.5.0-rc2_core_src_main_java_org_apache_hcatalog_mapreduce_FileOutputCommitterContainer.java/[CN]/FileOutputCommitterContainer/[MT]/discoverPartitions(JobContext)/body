{
  if (!partitionsDiscovered) {
    OutputJobInfo jobInfo=HCatOutputFormat.getJobInfo(context);
    harProcessor.setEnabled(jobInfo.getHarRequested());
    List<Integer> dynamicPartCols=jobInfo.getPosOfDynPartCols();
    int maxDynamicPartitions=jobInfo.getMaxDynamicPartitions();
    Path loadPath=new Path(jobInfo.getLocation());
    FileSystem fs=loadPath.getFileSystem(context.getConfiguration());
    String dynPathSpec=loadPath.toUri().getPath();
    dynPathSpec=dynPathSpec.replaceAll("__HIVE_DEFAULT_PARTITION__","*");
    Path pathPattern=new Path(dynPathSpec);
    FileStatus[] status=fs.globStatus(pathPattern);
    partitionsDiscoveredByPath=new LinkedHashMap<String,Map<String,String>>();
    contextDiscoveredByPath=new LinkedHashMap<String,JobContext>();
    if (status.length == 0) {
    }
 else {
      if ((maxDynamicPartitions != -1) && (status.length > maxDynamicPartitions)) {
        this.partitionsDiscovered=true;
        throw new HCatException(ErrorType.ERROR_TOO_MANY_DYNAMIC_PTNS,"Number of dynamic partitions being created " + "exceeds configured max allowable partitions[" + maxDynamicPartitions + "], increase parameter ["+ HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTS.varname+ "] if needed.");
      }
      for (      FileStatus st : status) {
        LinkedHashMap<String,String> fullPartSpec=new LinkedHashMap<String,String>();
        Warehouse.makeSpecFromName(fullPartSpec,st.getPath());
        partitionsDiscoveredByPath.put(st.getPath().toString(),fullPartSpec);
        JobConf jobConf=(JobConf)context.getConfiguration();
        JobContext currContext=HCatMapRedUtil.createJobContext(jobConf,context.getJobID(),InternalUtil.createReporter(HCatMapRedUtil.createTaskAttemptContext(jobConf,HCatHadoopShims.Instance.get().createTaskAttemptID())));
        HCatOutputFormat.configureOutputStorageHandler(currContext,jobInfo,fullPartSpec);
        contextDiscoveredByPath.put(st.getPath().toString(),currContext);
      }
    }
    this.partitionsDiscovered=true;
  }
}
