{
  if (firstRow) {
    firstRow=false;
    try {
      String[] cmdArgs=splitArgs(conf.getScriptCmd());
      String prog=cmdArgs[0];
      File currentDir=new File(".").getAbsoluteFile();
      if (!new File(prog).isAbsolute()) {
        PathFinder finder=new PathFinder("PATH");
        finder.prependPathComponent(currentDir.toString());
        File f=finder.getAbsolutePath(prog);
        if (f != null) {
          cmdArgs[0]=f.getAbsolutePath();
        }
        f=null;
      }
      String[] wrappedCmdArgs=addWrapper(cmdArgs);
      LOG.info("Executing " + Arrays.asList(wrappedCmdArgs));
      LOG.info("tablename=" + hconf.get(HiveConf.ConfVars.HIVETABLENAME.varname));
      LOG.info("partname=" + hconf.get(HiveConf.ConfVars.HIVEPARTITIONNAME.varname));
      LOG.info("alias=" + alias);
      ProcessBuilder pb=new ProcessBuilder(wrappedCmdArgs);
      Map<String,String> env=pb.environment();
      addJobConfToEnvironment(hconf,env);
      env.put(safeEnvVarName(HiveConf.ConfVars.HIVEALIAS.varname),String.valueOf(alias));
      String idEnvVarName=HiveConf.getVar(hconf,HiveConf.ConfVars.HIVESCRIPTIDENVVAR);
      String idEnvVarVal=getOperatorId();
      env.put(safeEnvVarName(idEnvVarName),idEnvVarVal);
      scriptPid=pb.start();
      DataOutputStream scriptOut=new DataOutputStream(new BufferedOutputStream(scriptPid.getOutputStream()));
      DataInputStream scriptIn=new DataInputStream(new BufferedInputStream(scriptPid.getInputStream()));
      DataInputStream scriptErr=new DataInputStream(new BufferedInputStream(scriptPid.getErrorStream()));
      scriptOutWriter=conf.getInRecordWriterClass().newInstance();
      scriptOutWriter.initialize(scriptOut,hconf);
      RecordReader scriptOutputReader=conf.getOutRecordReaderClass().newInstance();
      scriptOutputReader.initialize(scriptIn,hconf,conf.getScriptOutputInfo().getProperties());
      outThread=new StreamThread(scriptOutputReader,new OutputStreamProcessor(scriptOutputDeserializer.getObjectInspector()),"OutputProcessor");
      RecordReader scriptErrReader=conf.getErrRecordReaderClass().newInstance();
      scriptErrReader.initialize(scriptErr,hconf,conf.getScriptErrInfo().getProperties());
      errThread=new StreamThread(scriptErrReader,new ErrorStreamProcessor(HiveConf.getIntVar(hconf,HiveConf.ConfVars.SCRIPTERRORLIMIT)),"ErrorProcessor");
      if (HiveConf.getBoolVar(hconf,HiveConf.ConfVars.HIVESCRIPTAUTOPROGRESS)) {
        autoProgressor=new AutoProgressor(this.getClass().getName(),reporter,Utilities.getDefaultNotificationInterval(hconf),HiveConf.getIntVar(hconf,HiveConf.ConfVars.HIVES_AUTO_PROGRESS_TIMEOUT) * 1000);
        autoProgressor.go();
      }
      outThread.start();
      errThread.start();
    }
 catch (    Exception e) {
      throw new HiveException("Cannot initialize ScriptOperator",e);
    }
  }
  if (scriptError != null) {
    throw new HiveException(scriptError);
  }
  try {
    Writable res=scriptInputSerializer.serialize(row,inputObjInspectors[tag]);
    scriptOutWriter.write(res);
  }
 catch (  SerDeException e) {
    LOG.error("Error in serializing the row: " + e.getMessage());
    scriptError=e;
    serialize_error_count.set(serialize_error_count.get() + 1);
    throw new HiveException(e);
  }
catch (  IOException e) {
    if (isBrokenPipeException(e) && allowPartialConsumption()) {
      setDone(true);
      LOG.warn("Got broken pipe during write: ignoring exception and setting operator to done");
    }
 else {
      LOG.error("Error in writing to script: " + e.getMessage());
      if (isBrokenPipeException(e)) {
        displayBrokenPipeInfo();
      }
      scriptError=e;
      throw new HiveException(e);
    }
  }
}
