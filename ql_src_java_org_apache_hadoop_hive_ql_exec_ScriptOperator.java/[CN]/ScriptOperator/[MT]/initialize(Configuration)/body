{
  super.initialize(hconf);
  statsMap.put(Counter.DESERIALIZE_ERRORS,deserialize_error_count);
  statsMap.put(Counter.SERIALIZE_ERRORS,serialize_error_count);
  try {
    this.hconf=hconf;
    tableDesc td=conf.getScriptOutputInfo();
    if (td == null) {
      td=Utilities.defaultTabTd;
    }
    decoder=td.getSerdeClass().newInstance();
    decoder.initialize(hconf,td.getProperties());
    hos=new NaiiveJSONSerializer();
    Properties p=new Properties();
    p.setProperty(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT,"" + Utilities.tabCode);
    hos.initialize(p);
    String[] cmdArgs=splitArgs(conf.getScriptCmd());
    String[] wrappedCmdArgs=addWrapper(cmdArgs);
    l4j.info("Executing " + Arrays.asList(wrappedCmdArgs));
    l4j.info("tablename=" + hconf.get(HiveConf.ConfVars.HIVETABLENAME.varname));
    l4j.info("partname=" + hconf.get(HiveConf.ConfVars.HIVEPARTITIONNAME.varname));
    l4j.info("alias=" + alias);
    ProcessBuilder pb=new ProcessBuilder(wrappedCmdArgs);
    Map<String,String> env=pb.environment();
    addJobConfToEnvironment(hconf,env);
    env.put(safeEnvVarName(HiveConf.ConfVars.HIVEALIAS.varname),String.valueOf(alias));
    scriptPid=pb.start();
    scriptOut=new DataOutputStream(new BufferedOutputStream(scriptPid.getOutputStream()));
    scriptIn=new DataInputStream(new BufferedInputStream(scriptPid.getInputStream()));
    scriptErr=new DataInputStream(new BufferedInputStream(scriptPid.getErrorStream()));
    outThread=new StreamThread(scriptIn,new OutputStreamProcessor(),"OutputProcessor");
    outThread.start();
    errThread=new StreamThread(scriptErr,new ErrorStreamProcessor(),"ErrorProcessor");
    errThread.start();
  }
 catch (  Exception e) {
    e.printStackTrace();
    throw new HiveException("Cannot initialize ScriptOperator",e);
  }
}
