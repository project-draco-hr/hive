{
  Map<String,PartitionDesc> pathToParts=work.getMapWork().getPathToPartitionInfo();
  List<String> toRemovePaths=new ArrayList<String>();
  Map<String,PartitionDesc> toAddPathToPart=new HashMap<String,PartitionDesc>();
  Map<String,ArrayList<String>> pathToAliases=work.getMapWork().getPathToAliases();
  for (  Map.Entry<String,PartitionDesc> pathPartEntry : pathToParts.entrySet()) {
    String path=pathPartEntry.getKey();
    PartitionDesc partDesc=pathPartEntry.getValue();
    if (partDesc.getInputFileFormatClass().equals(SymlinkTextInputFormat.class)) {
      partDesc.setInputFileFormatClass(TextInputFormat.class);
      Path symlinkDir=new Path(path);
      FileSystem fileSystem=symlinkDir.getFileSystem(job);
      FileStatus fStatus=fileSystem.getFileStatus(symlinkDir);
      FileStatus[] symlinks=null;
      if (!fStatus.isDir()) {
        symlinks=new FileStatus[]{fStatus};
      }
 else {
        symlinks=fileSystem.listStatus(symlinkDir,FileUtils.HIDDEN_FILES_PATH_FILTER);
      }
      toRemovePaths.add(path);
      ArrayList<String> aliases=pathToAliases.remove(path);
      for (      FileStatus symlink : symlinks) {
        BufferedReader reader=null;
        try {
          reader=new BufferedReader(new InputStreamReader(fileSystem.open(symlink.getPath())));
          partDesc.setInputFileFormatClass(TextInputFormat.class);
          String line;
          while ((line=reader.readLine()) != null) {
            FileStatus[] matches=fileSystem.globStatus(new Path(line));
            for (            FileStatus fileStatus : matches) {
              toAddPathToPart.put(fileStatus.getPath().toUri().getPath(),partDesc);
              pathToAliases.put(fileStatus.getPath().toUri().getPath(),aliases);
            }
          }
        }
  finally {
          org.apache.hadoop.io.IOUtils.closeStream(reader);
        }
      }
    }
  }
  pathToParts.putAll(toAddPathToPart);
  for (  String toRemove : toRemovePaths) {
    pathToParts.remove(toRemove);
  }
}
