{
  try {
    String hadoopExec=conf.getVar(HiveConf.ConfVars.HADOOPBIN);
    String hiveJar=conf.getJar();
    String hiveConfArgs=ExecDriver.generateCmdLine(conf);
    String auxJars=conf.getAuxJars();
    if (!StringUtils.isEmpty(auxJars)) {
      auxJars=" -libjars " + auxJars + " ";
    }
 else {
      auxJars=" ";
    }
    mapredWork plan=getWork();
    File planFile=File.createTempFile("plan",".xml");
    LOG.info("Generating plan file " + planFile.toString());
    FileOutputStream out=new FileOutputStream(planFile);
    Utilities.serializeMapRedWork(plan,out);
    String cmdLine=hadoopExec + " jar " + auxJars+ " "+ hiveJar+ " org.apache.hadoop.hive.ql.exec.ExecDriver -plan "+ planFile.toString()+ " "+ hiveConfArgs;
    String files=ExecDriver.getRealFiles(conf);
    if (!files.isEmpty()) {
      cmdLine=cmdLine + " -files " + files;
    }
    LOG.info("Executing: " + cmdLine);
    Process executor=Runtime.getRuntime().exec(cmdLine);
    StreamPrinter outPrinter=new StreamPrinter(executor.getInputStream(),null,System.out);
    StreamPrinter errPrinter=new StreamPrinter(executor.getErrorStream(),null,System.err);
    outPrinter.start();
    errPrinter.start();
    int exitVal=executor.waitFor();
    if (exitVal != 0) {
      LOG.error("Execution failed with exit status: " + exitVal);
    }
 else {
      LOG.info("Execution completed successfully");
    }
    return exitVal;
  }
 catch (  Exception e) {
    e.printStackTrace();
    LOG.error("Exception: " + e.getMessage());
    return (1);
  }
}
