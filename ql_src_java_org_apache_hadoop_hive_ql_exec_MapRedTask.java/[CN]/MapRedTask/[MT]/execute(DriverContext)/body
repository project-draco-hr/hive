{
  Context ctx=driverContext.getCtx();
  boolean ctxCreated=false;
  try {
    if (ctx == null) {
      ctx=new Context(conf);
      ctxCreated=true;
    }
    setNumberOfReducers();
    if (!ctx.isLocalOnlyExecutionMode() && conf.getBoolVar(HiveConf.ConfVars.LOCALMODEAUTO)) {
      if (inputSummary == null) {
        inputSummary=Utilities.getInputSummary(driverContext.getCtx(),work,null);
      }
      estimateInputSize();
      int numReducers=work.getNumReduceTasks();
      if (LOG.isDebugEnabled()) {
        LOG.debug("Task: " + getId() + ", Summary: "+ totalInputFileSize+ ","+ totalInputNumFiles+ ","+ numReducers);
      }
      String reason=MapRedTask.isEligibleForLocalMode(conf,numReducers,totalInputFileSize,totalInputNumFiles);
      if (reason == null) {
        cloneConf();
        ShimLoader.getHadoopShims().setJobLauncherRpcAddress(conf,"local");
        console.printInfo("Selecting local mode for task: " + getId());
        this.setLocalMode(true);
      }
 else {
        console.printInfo("Cannot run job locally: " + reason);
        this.setLocalMode(false);
      }
    }
    runningViaChild=ShimLoader.getHadoopShims().isLocalMode(conf) || conf.getBoolVar(HiveConf.ConfVars.SUBMITVIACHILD);
    if (!runningViaChild) {
      return super.execute(driverContext);
    }
    cloneConf();
    super.setInputAttributes(conf);
    String hadoopExec=conf.getVar(HiveConf.ConfVars.HADOOPBIN);
    String hiveJar=conf.getJar();
    String libJarsOption;
    String addedJars=Utilities.getResourceFiles(conf,SessionState.ResourceType.JAR);
    conf.setVar(ConfVars.HIVEADDEDJARS,addedJars);
    String auxJars=conf.getAuxJars();
    if (StringUtils.isEmpty(addedJars)) {
      if (StringUtils.isEmpty(auxJars)) {
        libJarsOption=" ";
      }
 else {
        libJarsOption=" -libjars " + auxJars + " ";
      }
    }
 else {
      if (StringUtils.isEmpty(auxJars)) {
        libJarsOption=" -libjars " + addedJars + " ";
      }
 else {
        libJarsOption=" -libjars " + addedJars + ","+ auxJars+ " ";
      }
    }
    String hiveConfArgs=generateCmdLine(conf,ctx);
    Path planPath=new Path(ctx.getLocalTmpFileURI(),"plan.xml");
    OutputStream out=FileSystem.getLocal(conf).create(planPath);
    MapredWork plan=getWork();
    LOG.info("Generating plan file " + planPath.toString());
    Utilities.serializeMapRedWork(plan,out);
    String isSilent="true".equalsIgnoreCase(System.getProperty("test.silent")) ? "-nolog" : "";
    String jarCmd;
    if (ShimLoader.getHadoopShims().usesJobShell()) {
      jarCmd=libJarsOption + hiveJar + " "+ ExecDriver.class.getName();
    }
 else {
      jarCmd=hiveJar + " " + ExecDriver.class.getName()+ libJarsOption;
    }
    String cmdLine=hadoopExec + " jar " + jarCmd+ " -plan "+ planPath.toString()+ " "+ isSilent+ " "+ hiveConfArgs;
    String workDir=(new File(".")).getCanonicalPath();
    String files=Utilities.getResourceFiles(conf,SessionState.ResourceType.FILE);
    if (!files.isEmpty()) {
      cmdLine=cmdLine + " -files " + files;
      workDir=(new Path(ctx.getLocalTmpFileURI())).toUri().getPath();
      if (!(new File(workDir)).mkdir()) {
        throw new IOException("Cannot create tmp working dir: " + workDir);
      }
      for (      String f : StringUtils.split(files,',')) {
        Path p=new Path(f);
        String target=p.toUri().getPath();
        String link=workDir + Path.SEPARATOR + p.getName();
        if (FileUtil.symLink(target,link) != 0) {
          throw new IOException("Cannot link to added file: " + target + " from: "+ link);
        }
      }
    }
    LOG.info("Executing: " + cmdLine);
    Process executor=null;
    String hadoopOpts;
    StringBuilder sb=new StringBuilder();
    Properties p=System.getProperties();
    for (    String element : HIVE_SYS_PROP) {
      if (p.containsKey(element)) {
        sb.append(" -D" + element + "="+ p.getProperty(element));
      }
    }
    hadoopOpts=sb.toString();
    String[] env;
    Map<String,String> variables=new HashMap(System.getenv());
    if (ShimLoader.getHadoopShims().isLocalMode(conf)) {
      int hadoopMem=conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);
      if (hadoopMem == 0) {
        variables.remove(HADOOP_MEM_KEY);
      }
 else {
        variables.put(HADOOP_MEM_KEY,String.valueOf(hadoopMem));
      }
    }
 else {
    }
    if (variables.containsKey(HADOOP_OPTS_KEY)) {
      variables.put(HADOOP_OPTS_KEY,variables.get(HADOOP_OPTS_KEY) + hadoopOpts);
    }
 else {
      variables.put(HADOOP_OPTS_KEY,hadoopOpts);
    }
    if (variables.containsKey(HIVE_DEBUG_RECURSIVE)) {
      configureDebugVariablesForChildJVM(variables);
    }
    env=new String[variables.size()];
    int pos=0;
    for (    Map.Entry<String,String> entry : variables.entrySet()) {
      String name=entry.getKey();
      String value=entry.getValue();
      env[pos++]=name + "=" + value;
    }
    executor=Runtime.getRuntime().exec(cmdLine,env,new File(workDir));
    CachingPrintStream errPrintStream=new CachingPrintStream(SessionState.getConsole().getChildErrStream());
    StreamPrinter outPrinter=new StreamPrinter(executor.getInputStream(),null,SessionState.getConsole().getChildOutStream());
    StreamPrinter errPrinter=new StreamPrinter(executor.getErrorStream(),null,errPrintStream);
    outPrinter.start();
    errPrinter.start();
    int exitVal=jobExecHelper.progressLocal(executor,getId());
    if (exitVal != 0) {
      LOG.error("Execution failed with exit status: " + exitVal);
      if (SessionState.get() != null) {
        SessionState.get().addLocalMapRedErrors(getId(),errPrintStream.getOutput());
      }
    }
 else {
      LOG.info("Execution completed successfully");
    }
    return exitVal;
  }
 catch (  Exception e) {
    e.printStackTrace();
    LOG.error("Exception: " + e.getMessage());
    return (1);
  }
 finally {
    try {
      if (ctxCreated) {
        ctx.clear();
      }
    }
 catch (    Exception e) {
      LOG.error("Exception: " + e.getMessage());
    }
  }
}
