{
  final int BUCKET=0;
  Configuration conf=new Configuration();
  OrcOutputFormat of=new OrcOutputFormat();
  FileSystem fs=FileSystem.getLocal(conf);
  Path root=new Path(tmpDir,"testRecordReaderDelta").makeQualified(fs);
  fs.delete(root,true);
  ObjectInspector inspector;
synchronized (TestOrcFile.class) {
    inspector=ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
  }
  AcidOutputFormat.Options options=new AcidOutputFormat.Options(conf).bucket(BUCKET).inspector(inspector).filesystem(fs).writingBase(false).minimumTransactionId(1).maximumTransactionId(1);
  RecordUpdater ru=of.getRecordUpdater(root,options);
  String[] values=new String[]{"a","b","c","d","e"};
  for (int i=0; i < values.length; ++i) {
    ru.insert(1,new MyRow(values[i]));
  }
  ru.close(false);
  options.minimumTransactionId(2).maximumTransactionId(2);
  ru=of.getRecordUpdater(root,options);
  values=new String[]{"f","g","h","i","j"};
  for (int i=0; i < values.length; ++i) {
    ru.insert(2,new MyRow(values[i]));
  }
  ru.close(false);
  InputFormat inf=new OrcInputFormat();
  JobConf job=new JobConf();
  job.set("mapred.min.split.size","1");
  job.set("mapred.max.split.size","2");
  job.set("mapred.input.dir",root.toString());
  job.set("bucket_count","1");
  InputSplit[] splits=inf.getSplits(job,5);
  assertEquals(1,splits.length);
  org.apache.hadoop.mapred.RecordReader<NullWritable,OrcStruct> rr;
  rr=inf.getRecordReader(splits[0],job,Reporter.NULL);
  values=new String[]{"a","b","c","d","e","f","g","h","i","j"};
  OrcStruct row=rr.createValue();
  for (int i=0; i < values.length; ++i) {
    System.out.println("Checking " + i);
    assertEquals(true,rr.next(NullWritable.get(),row));
    assertEquals(values[i],row.getFieldValue(0).toString());
  }
  assertEquals(false,rr.next(NullWritable.get(),row));
}
