{
  final int BUCKET=11;
  Configuration conf=new Configuration();
  OrcOutputFormat of=new OrcOutputFormat();
  FileSystem fs=FileSystem.getLocal(conf);
  Path root=new Path(tmpDir,"testRecordReaderNewBaseAndDelta").makeQualified(fs);
  fs.delete(root,true);
  ObjectInspector inspector;
synchronized (TestOrcFile.class) {
    inspector=ObjectInspectorFactory.getReflectionObjectInspector(BigRow.class,ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
  }
  MemoryManager mgr=new MemoryManager(conf){
    int rowsAddedSinceCheck=0;
    @Override public synchronized void addedRow(    int rows) throws IOException {
      rowsAddedSinceCheck+=rows;
      if (rowsAddedSinceCheck >= 2) {
        notifyWriters();
        rowsAddedSinceCheck=0;
      }
    }
  }
;
  OrcRecordUpdater.OrcOptions options=(OrcRecordUpdater.OrcOptions)new OrcRecordUpdater.OrcOptions(conf).writingBase(true).minimumTransactionId(0).maximumTransactionId(0).bucket(BUCKET).inspector(inspector).filesystem(fs);
  options.orcOptions(OrcFile.writerOptions(conf).stripeSize(1).blockPadding(false).compress(CompressionKind.NONE).memory(mgr).batchSize(2));
  options.finalDestination(root);
  RecordUpdater ru=of.getRecordUpdater(root,options);
  String[] values=new String[]{"ignore.1","0.1","ignore.2","ignore.3","2.0","2.1","3.0","ignore.4","ignore.5","ignore.6"};
  for (int i=0; i < values.length; ++i) {
    ru.insert(0,new BigRow(i,i,values[i],i,i));
  }
  ru.close(false);
  options.writingBase(false).minimumTransactionId(1).maximumTransactionId(1).recordIdColumn(5);
  ru=of.getRecordUpdater(root,options);
  values=new String[]{"0.0",null,null,"1.1",null,null,null,"ignore.7"};
  for (int i=0; i < values.length; ++i) {
    if (values[i] != null) {
      ru.update(1,new BigRow(i,i,values[i],i,i,i,0,BUCKET));
    }
  }
  ru.delete(100,new BigRow(9,0,BUCKET));
  ru.close(false);
  options.minimumTransactionId(2).maximumTransactionId(2);
  ru=of.getRecordUpdater(root,options);
  values=new String[]{null,null,"1.0",null,null,null,null,"3.1"};
  for (int i=0; i < values.length; ++i) {
    if (values[i] != null) {
      ru.update(2,new BigRow(i,i,values[i],i,i,i,0,BUCKET));
    }
  }
  ru.delete(100,new BigRow(8,0,BUCKET));
  ru.close(false);
  InputFormat inf=new OrcInputFormat();
  JobConf job=new JobConf();
  job.set("mapred.min.split.size","1");
  job.set("mapred.max.split.size","2");
  job.set("mapred.input.dir",root.toString());
  job.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS,BigRow.getColumnNamesProperty());
  job.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES,BigRow.getColumnTypesProperty());
  HiveConf.setBoolVar(job,HiveConf.ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN,true);
  InputSplit[] splits=inf.getSplits(job,5);
  assertEquals(5,splits.length);
  org.apache.hadoop.mapred.RecordReader<NullWritable,OrcStruct> rr;
  for (int i=0; i < 4; ++i) {
    System.out.println("starting split " + i + " = "+ splits[i]);
    rr=inf.getRecordReader(splits[i],job,Reporter.NULL);
    NullWritable key=rr.createKey();
    OrcStruct value=rr.createValue();
    for (int j=0; j < 2; ++j) {
      System.out.println("i = " + i + ", j = "+ j);
      assertEquals(true,rr.next(key,value));
      System.out.println("record = " + value);
      assertEquals(i + "." + j,value.getFieldValue(2).toString());
    }
    assertEquals(false,rr.next(key,value));
  }
  rr=inf.getRecordReader(splits[4],job,Reporter.NULL);
  assertEquals(false,rr.next(rr.createKey(),rr.createValue()));
}
