{
  HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,CombineHiveInputFormat.class.getName());
  success=true;
  ShimLoader.getHadoopShims().prepareJobOutput(job);
  job.setOutputFormat(HiveOutputFormatImpl.class);
  job.setMapperClass(work.getMapperClass());
  Context ctx=driverContext.getCtx();
  boolean ctxCreated=false;
  try {
    if (ctx == null) {
      ctx=new Context(job);
      ctxCreated=true;
    }
  }
 catch (  IOException e) {
    e.printStackTrace();
    console.printError("Error launching map-reduce job","\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  job.setMapOutputKeyClass(NullWritable.class);
  job.setMapOutputValueClass(NullWritable.class);
  if (work.getNumMapTasks() != null) {
    job.setNumMapTasks(work.getNumMapTasks());
  }
  job.setNumReduceTasks(0);
  if (work.getMinSplitSize() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZE,work.getMinSplitSize().longValue());
  }
  if (work.getInputformat() != null) {
    HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,work.getInputformat());
  }
  String inpFormat=HiveConf.getVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT);
  if ((inpFormat == null) || (!StringUtils.isNotBlank(inpFormat))) {
    inpFormat=ShimLoader.getHadoopShims().getInputFormatClassName();
  }
  LOG.info("Using " + inpFormat);
  try {
    job.setInputFormat((Class<? extends InputFormat>)(Class.forName(inpFormat)));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage());
  }
  job.setOutputKeyClass(NullWritable.class);
  job.setOutputValueClass(NullWritable.class);
  int returnVal=0;
  RunningJob rj=null;
  boolean noName=StringUtils.isEmpty(HiveConf.getVar(job,HiveConf.ConfVars.HADOOPJOBNAME));
  String jobName=null;
  if (noName && this.getQueryPlan() != null) {
    int maxlen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
    jobName=Utilities.abbreviate(this.getQueryPlan().getQueryStr(),maxlen - 6);
  }
  if (noName) {
    HiveConf.setVar(job,HiveConf.ConfVars.HADOOPJOBNAME,jobName != null ? jobName : "JOB" + Utilities.randGen.nextInt());
  }
  HiveConf.setVar(job,HiveConf.ConfVars.HIVE_STATS_KEY_PREFIX,work.getAggKey());
  try {
    addInputPaths(job,work);
    MapredWork mrWork=new MapredWork();
    mrWork.setMapWork(work);
    Utilities.setMapRedWork(job,mrWork,ctx.getMRTmpPath());
    String pwd=HiveConf.getVar(job,HiveConf.ConfVars.METASTOREPWD);
    if (pwd != null) {
      HiveConf.setVar(job,HiveConf.ConfVars.METASTOREPWD,"HIVE");
    }
    JobClient jc=new JobClient(job);
    String addedJars=Utilities.getResourceFiles(job,SessionState.ResourceType.JAR);
    if (!addedJars.isEmpty()) {
      job.set("tmpjars",addedJars);
    }
    Throttle.checkJobTracker(job,LOG);
    if (work.isGatheringStats()) {
      StatsPublisher statsPublisher;
      StatsFactory factory=StatsFactory.newFactory(job);
      if (factory != null) {
        statsPublisher=factory.getStatsPublisher();
        if (!statsPublisher.init(job)) {
          if (HiveConf.getBoolVar(job,HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
            throw new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
          }
        }
      }
    }
    rj=jc.submitJob(job);
    returnVal=jobExecHelper.progress(rj,jc,null);
    success=(returnVal == 0);
  }
 catch (  Exception e) {
    e.printStackTrace();
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    try {
      if (ctxCreated) {
        ctx.clear();
      }
      if (rj != null) {
        if (returnVal != 0) {
          rj.killJob();
        }
        HadoopJobExecHelper.runningJobs.remove(rj);
        jobID=rj.getID().toString();
      }
    }
 catch (    Exception e) {
    }
  }
  return (returnVal);
}
