{
  GenSparkProcContext context=(GenSparkProcContext)procContext;
  Preconditions.checkArgument(context != null,"AssertionError: expected context to be not null");
  Preconditions.checkArgument(context.currentTask != null,"AssertionError: expected context.currentTask to be not null");
  Preconditions.checkArgument(context.currentRootOperator != null,"AssertionError: expected context.currentRootOperator to be not null");
  Operator<?> operator=(Operator<?>)nd;
  Operator<?> root=context.currentRootOperator;
  LOG.debug("Root operator: " + root);
  LOG.debug("Leaf operator: " + operator);
  if (context.clonedReduceSinks.contains(operator)) {
    return null;
  }
  SparkWork sparkWork=context.currentTask.getWork();
  BaseWork work;
  if (context.rootToWorkMap.containsKey(root)) {
    work=context.rootToWorkMap.get(root);
  }
 else {
    if (context.preceedingWork == null) {
      work=utils.createMapWork(context,root,sparkWork,null);
    }
 else {
      work=utils.createReduceWork(context,root,sparkWork);
    }
    context.rootToWorkMap.put(root,work);
  }
  if (!context.childToWorkMap.containsKey(operator)) {
    List<BaseWork> workItems=new LinkedList<BaseWork>();
    workItems.add(work);
    context.childToWorkMap.put(operator,workItems);
  }
 else {
    context.childToWorkMap.get(operator).add(work);
  }
  if (!context.currentMapJoinOperators.isEmpty()) {
    for (    MapJoinOperator mj : context.currentMapJoinOperators) {
      LOG.debug("Processing map join: " + mj);
      if (!context.mapJoinWorkMap.containsKey(mj)) {
        List<BaseWork> workItems=new LinkedList<BaseWork>();
        workItems.add(work);
        context.mapJoinWorkMap.put(mj,workItems);
      }
 else {
        context.mapJoinWorkMap.get(mj).add(work);
      }
      if (context.linkOpWithWorkMap.containsKey(mj)) {
        Map<BaseWork,SparkEdgeProperty> linkWorkMap=context.linkOpWithWorkMap.get(mj);
        if (linkWorkMap != null) {
          if (context.linkChildOpWithDummyOp.containsKey(mj)) {
            for (            Operator<?> dummy : context.linkChildOpWithDummyOp.get(mj)) {
              work.addDummyOp((HashTableDummyOperator)dummy);
            }
          }
          for (          Entry<BaseWork,SparkEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {
            BaseWork parentWork=parentWorkMap.getKey();
            LOG.debug("connecting " + parentWork.getName() + " with "+ work.getName());
            SparkEdgeProperty edgeProp=parentWorkMap.getValue();
            sparkWork.connect(parentWork,work,edgeProp);
            for (            ReduceSinkOperator r : context.linkWorkWithReduceSinkMap.get(parentWork)) {
              if (r.getConf().getOutputName() != null) {
                LOG.debug("Cloning reduce sink for multi-child broadcast edge");
                r=(ReduceSinkOperator)OperatorFactory.getAndMakeChild((ReduceSinkDesc)r.getConf().clone(),r.getParentOperators());
                context.clonedReduceSinks.add(r);
              }
              r.getConf().setOutputName(work.getName());
              context.connectedReduceSinks.add(r);
            }
          }
        }
      }
    }
    context.currentMapJoinOperators.clear();
  }
  for (  Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {
    context.leafOperatorToFollowingWork.put(parent,work);
    LOG.debug("Removing " + parent + " as parent from "+ root);
    root.removeParent(parent);
  }
  if (!context.currentUnionOperators.isEmpty()) {
    UnionWork unionWork;
    if (context.unionWorkMap.containsKey(operator)) {
      Preconditions.checkArgument(operator.getChildOperators().isEmpty(),"AssertionError: expected operator.getChildOperators() to be empty");
      unionWork=(UnionWork)context.unionWorkMap.get(operator);
    }
 else {
      unionWork=utils.createUnionWork(context,operator,sparkWork);
    }
    LOG.debug("Connecting union work (" + unionWork + ") with work ("+ work+ ")");
    SparkEdgeProperty edgeProp=new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_NONE);
    sparkWork.connect(work,unionWork,edgeProp);
    unionWork.addUnionOperators(context.currentUnionOperators);
    context.currentUnionOperators.clear();
    context.workWithUnionOperators.add(work);
    work=unionWork;
  }
  if (context.leafOperatorToFollowingWork.containsKey(operator)) {
    BaseWork followingWork=context.leafOperatorToFollowingWork.get(operator);
    long bytesPerReducer=context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);
    LOG.debug("Second pass. Leaf operator: " + operator + " has common downstream work:"+ followingWork);
    Preconditions.checkArgument(operator instanceof ReduceSinkOperator,"AssertionError: expected operator to be an instance of ReduceSinkOperator, but was " + operator.getClass().getName());
    Preconditions.checkArgument(followingWork instanceof ReduceWork,"AssertionError: expected followingWork to be an instance of ReduceWork, but was " + followingWork.getClass().getName());
    ReduceSinkOperator rs=(ReduceSinkOperator)operator;
    ReduceWork rWork=(ReduceWork)followingWork;
    GenMapRedUtils.setKeyAndValueDesc(rWork,rs);
    rWork.getTagToInput().put(rs.getConf().getTag(),work.getName());
    rs.getConf().setOutputName(rWork.getName());
    if (!context.connectedReduceSinks.contains(rs)) {
      SparkEdgeProperty edgeProp=new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_GROUP,rs.getConf().getNumReducers());
      String sortOrder=Strings.nullToEmpty(rs.getConf().getOrder()).trim();
      if (!sortOrder.isEmpty() && GenSparkUtils.isSortNecessary(rs)) {
        edgeProp.setShuffleSort();
      }
      sparkWork.connect(work,rWork,edgeProp);
      context.connectedReduceSinks.add(rs);
    }
  }
 else {
    LOG.debug("First pass. Leaf operator: " + operator);
  }
  if (!operator.getChildOperators().isEmpty()) {
    Preconditions.checkArgument(operator.getChildOperators().size() == 1,"AssertionError: expected operator.getChildOperators().size() to be 1, but was " + operator.getChildOperators().size());
    context.parentOfRoot=operator;
    context.currentRootOperator=operator.getChildOperators().get(0);
    context.preceedingWork=work;
  }
  return null;
}
