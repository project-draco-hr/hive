{
  GenSparkProcContext context=(GenSparkProcContext)procContext;
  Preconditions.checkArgument(context != null,"AssertionError: expected context to be not null");
  Preconditions.checkArgument(context.currentTask != null,"AssertionError: expected context.currentTask to be not null");
  Preconditions.checkArgument(context.currentRootOperator != null,"AssertionError: expected context.currentRootOperator to be not null");
  @SuppressWarnings("unchecked") Operator<? extends OperatorDesc> operator=(Operator<? extends OperatorDesc>)nd;
  Operator<?> root=context.currentRootOperator;
  LOG.debug("Root operator: " + root);
  LOG.debug("Leaf operator: " + operator);
  if (context.clonedReduceSinks.contains(operator)) {
    return null;
  }
  SparkWork sparkWork=context.currentTask.getWork();
  SMBMapJoinOperator smbOp=GenSparkUtils.getChildOperator(root,SMBMapJoinOperator.class);
  BaseWork work;
  if (context.rootToWorkMap.containsKey(root)) {
    work=context.rootToWorkMap.get(root);
  }
 else {
    if (context.preceedingWork == null) {
      if (smbOp != null) {
        work=utils.createMapWork(context,root,sparkWork,null,true);
        if (context.smbJoinWorkMap.get(smbOp) != null) {
          throw new SemanticException("Each SMBMapJoin should be associated only with one Mapwork");
        }
        context.smbJoinWorkMap.put(smbOp,(MapWork)work);
      }
 else {
        work=utils.createMapWork(context,root,sparkWork,null);
      }
    }
 else {
      work=utils.createReduceWork(context,root,sparkWork);
    }
    context.rootToWorkMap.put(root,work);
  }
  if (!context.childToWorkMap.containsKey(operator)) {
    List<BaseWork> workItems=new LinkedList<BaseWork>();
    workItems.add(work);
    context.childToWorkMap.put(operator,workItems);
  }
 else {
    context.childToWorkMap.get(operator).add(work);
  }
  if (!context.currentMapJoinOperators.isEmpty()) {
    for (    MapJoinOperator mj : context.currentMapJoinOperators) {
      LOG.debug("Processing map join: " + mj);
      if (!context.mapJoinWorkMap.containsKey(mj)) {
        List<BaseWork> workItems=new LinkedList<BaseWork>();
        workItems.add(work);
        context.mapJoinWorkMap.put(mj,workItems);
      }
 else {
        context.mapJoinWorkMap.get(mj).add(work);
      }
      if (context.linkOpWithWorkMap.containsKey(mj)) {
        Map<BaseWork,SparkEdgeProperty> linkWorkMap=context.linkOpWithWorkMap.get(mj);
        if (linkWorkMap != null) {
          if (context.linkChildOpWithDummyOp.containsKey(mj)) {
            for (            Operator<?> dummy : context.linkChildOpWithDummyOp.get(mj)) {
              work.addDummyOp((HashTableDummyOperator)dummy);
            }
          }
          for (          Entry<BaseWork,SparkEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {
            BaseWork parentWork=parentWorkMap.getKey();
            LOG.debug("connecting " + parentWork.getName() + " with "+ work.getName());
            SparkEdgeProperty edgeProp=parentWorkMap.getValue();
            sparkWork.connect(parentWork,work,edgeProp);
            for (            ReduceSinkOperator r : context.linkWorkWithReduceSinkMap.get(parentWork)) {
              if (r.getConf().getOutputName() != null) {
                LOG.debug("Cloning reduce sink for multi-child broadcast edge");
                r=(ReduceSinkOperator)OperatorFactory.getAndMakeChild((ReduceSinkDesc)r.getConf().clone(),r.getParentOperators());
                context.clonedReduceSinks.add(r);
              }
              r.getConf().setOutputName(work.getName());
            }
          }
        }
      }
    }
    context.currentMapJoinOperators.clear();
  }
  if (root.getNumParent() > 0) {
    Preconditions.checkArgument(work instanceof ReduceWork,"AssertionError: expected work to be a ReduceWork, but was " + work.getClass().getName());
    ReduceWork reduceWork=(ReduceWork)work;
    for (    Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {
      Preconditions.checkArgument(parent instanceof ReduceSinkOperator,"AssertionError: expected operator to be a ReduceSinkOperator, but was " + parent.getClass().getName());
      ReduceSinkOperator rsOp=(ReduceSinkOperator)parent;
      SparkEdgeProperty edgeProp=GenSparkUtils.getEdgeProperty(rsOp,reduceWork);
      rsOp.getConf().setOutputName(reduceWork.getName());
      GenMapRedUtils.setKeyAndValueDesc(reduceWork,rsOp);
      context.leafOpToFollowingWorkInfo.put(rsOp,ObjectPair.create(edgeProp,reduceWork));
      LOG.debug("Removing " + parent + " as parent from "+ root);
      root.removeParent(parent);
    }
  }
  if (!context.currentUnionOperators.isEmpty()) {
    context.currentUnionOperators.clear();
    context.workWithUnionOperators.add(work);
  }
  if (context.leafOpToFollowingWorkInfo.containsKey(operator)) {
    ObjectPair<SparkEdgeProperty,ReduceWork> childWorkInfo=context.leafOpToFollowingWorkInfo.get(operator);
    SparkEdgeProperty edgeProp=childWorkInfo.getFirst();
    ReduceWork childWork=childWorkInfo.getSecond();
    LOG.debug("Second pass. Leaf operator: " + operator + " has common downstream work:"+ childWork);
    if (sparkWork.getEdgeProperty(work,childWork) == null) {
      sparkWork.connect(work,childWork,edgeProp);
    }
 else {
      LOG.debug("work " + work.getName() + " is already connected to "+ childWork.getName()+ " before");
    }
  }
 else {
    LOG.debug("First pass. Leaf operator: " + operator);
  }
  if (!operator.getChildOperators().isEmpty()) {
    Preconditions.checkArgument(operator.getChildOperators().size() == 1,"AssertionError: expected operator.getChildOperators().size() to be 1, but was " + operator.getChildOperators().size());
    context.parentOfRoot=operator;
    context.currentRootOperator=operator.getChildOperators().get(0);
    context.preceedingWork=work;
  }
  return null;
}
