{
  mr.setNumReduceTasks(Integer.valueOf(1));
  Operator<reduceSinkDesc> op1=OperatorFactory.get(new reduceSinkDesc(Utilities.makeList(new exprNodeColumnDesc(String.class,"tkey")),Utilities.makeList(new exprNodeColumnDesc(String.class,"tkey"),new exprNodeColumnDesc(String.class,"tvalue")),1));
  Operator<scriptDesc> op0=OperatorFactory.get(new scriptDesc("\'/bin/cat\'",new tableDesc(org.apache.hadoop.hive.serde.simple_meta.MetadataTypedColumnsetSerDe.class,TextInputFormat.class,IgnoreKeyTextOutputFormat.class,Utilities.makeProperties("serialization.format","9","columns","tkey,tvalue"))),op1);
  Operator<selectDesc> op4=OperatorFactory.get(new selectDesc(Utilities.makeList(new exprNodeColumnDesc(String.class,"key"),new exprNodeColumnDesc(String.class,"value"))),op0);
  Utilities.addMapWork(mr,src,"a",op4);
  Operator<fileSinkDesc> op3=OperatorFactory.get(new fileSinkDesc(tmpdir + "mapredplan6.out",Utilities.defaultTd));
  Operator<filterDesc> op2=OperatorFactory.get(getTestFilterDesc("0"),op3);
  Operator<extractDesc> op5=OperatorFactory.get(new extractDesc(new exprNodeColumnDesc(String.class,Utilities.ReduceField.VALUE.toString())),op2);
  mr.setReducer(op5);
}
