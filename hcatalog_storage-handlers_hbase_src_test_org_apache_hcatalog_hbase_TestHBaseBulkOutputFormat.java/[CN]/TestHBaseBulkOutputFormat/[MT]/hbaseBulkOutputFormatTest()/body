{
  String testName="hbaseBulkOutputFormatTest";
  Path methodTestDir=new Path(getTestDir(),testName);
  LOG.info("starting: " + testName);
  String tableName=newTableName(testName).toLowerCase();
  String familyName="my_family";
  byte[] familyNameBytes=Bytes.toBytes(familyName);
  Configuration conf=new Configuration(allConf);
  conf.set(HBaseConstants.PROPERTY_OUTPUT_TABLE_NAME_KEY,tableName);
  conf.set("yarn.scheduler.capacity.root.queues","default");
  conf.set("yarn.scheduler.capacity.root.default.capacity","100");
  createTable(tableName,new String[]{familyName});
  String data[]={"1,english:one,spanish:uno","2,english:two,spanish:dos","3,english:three,spanish:tres"};
  Path inputPath=new Path(methodTestDir,"mr_input");
  FSDataOutputStream os=getFileSystem().create(new Path(inputPath,"inputFile.txt"));
  for (  String line : data)   os.write(Bytes.toBytes(line + "\n"));
  os.close();
  Path interPath=new Path(methodTestDir,"inter");
  JobConf job=new JobConf(conf);
  job.setWorkingDirectory(new Path(methodTestDir,"mr_work"));
  job.setJarByClass(this.getClass());
  job.setMapperClass(MapWriteOldMapper.class);
  job.setInputFormat(org.apache.hadoop.mapred.TextInputFormat.class);
  org.apache.hadoop.mapred.TextInputFormat.setInputPaths(job,inputPath);
  job.setOutputFormat(HBaseBulkOutputFormat.class);
  org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputPath(job,interPath);
  job.setOutputCommitter(HBaseBulkOutputCommitter.class);
  RevisionManager rm=HBaseRevisionManagerUtil.getOpenedRevisionManager(conf);
  try {
    OutputJobInfo outputJobInfo=OutputJobInfo.create("default",tableName,null);
    Transaction txn=rm.beginWriteTransaction(tableName,Arrays.asList(familyName));
    outputJobInfo.getProperties().setProperty(HBaseConstants.PROPERTY_WRITE_TXN_KEY,HCatUtil.serialize(txn));
    job.set(HCatConstants.HCAT_KEY_OUTPUT_INFO,HCatUtil.serialize(outputJobInfo));
  }
  finally {
    rm.close();
  }
  job.setMapOutputKeyClass(ImmutableBytesWritable.class);
  job.setMapOutputValueClass(HCatRecord.class);
  job.setOutputKeyClass(ImmutableBytesWritable.class);
  job.setOutputValueClass(HCatRecord.class);
  job.setNumReduceTasks(0);
  RunningJob runJob=JobClient.runJob(job);
  runJob.waitForCompletion();
  assertTrue(runJob.isSuccessful());
  HTable table=new HTable(conf,tableName);
  Scan scan=new Scan();
  scan.addFamily(familyNameBytes);
  ResultScanner scanner=table.getScanner(scan);
  int index=0;
  for (  Result result : scanner) {
    String vals[]=data[index].toString().split(",");
    for (int i=1; i < vals.length; i++) {
      String pair[]=vals[i].split(":");
      assertTrue(result.containsColumn(familyNameBytes,Bytes.toBytes(pair[0])));
      assertEquals(pair[1],Bytes.toString(result.getValue(familyNameBytes,Bytes.toBytes(pair[0]))));
    }
    index++;
  }
  table.close();
  assertEquals(data.length,index);
  assertFalse(FileSystem.get(job).exists(interPath));
}
