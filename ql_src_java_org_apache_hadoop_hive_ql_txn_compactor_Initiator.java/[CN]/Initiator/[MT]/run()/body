{
  try {
    recoverFailedCompactions(false);
    int abortedThreshold=HiveConf.getIntVar(conf,HiveConf.ConfVars.HIVE_COMPACTOR_ABORTEDTXN_THRESHOLD);
    do {
      long startedAt=-1;
      TxnStore.MutexAPI.LockHandle handle=null;
      try {
        handle=txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Initiator.name());
        startedAt=System.currentTimeMillis();
        ShowCompactResponse currentCompactions=txnHandler.showCompact(new ShowCompactRequest());
        ValidTxnList txns=TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());
        Set<CompactionInfo> potentials=txnHandler.findPotentialCompactions(abortedThreshold);
        LOG.debug("Found " + potentials.size() + " potential compactions, "+ "checking to see if we should compact any of them");
        for (        CompactionInfo ci : potentials) {
          LOG.info("Checking to see if we should compact " + ci.getFullPartitionName());
          try {
            Table t=resolveTable(ci);
            if (t == null) {
              LOG.info("Can't find table " + ci.getFullTableName() + ", assuming it's a temp "+ "table or has been dropped and moving on.");
              continue;
            }
            if (noAutoCompactSet(t)) {
              LOG.info("Table " + tableName(t) + " marked "+ hive_metastoreConstants.TABLE_NO_AUTO_COMPACT+ "=true so we will not compact it.");
              continue;
            }
            if (t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0 && ci.partName == null) {
              LOG.debug("Skipping entry for " + ci.getFullTableName() + " as it is from dynamic"+ " partitioning");
              continue;
            }
            if (lookForCurrentCompactions(currentCompactions,ci)) {
              LOG.debug("Found currently initiated or working compaction for " + ci.getFullPartitionName() + " so we will not initiate another compaction");
              continue;
            }
            if (txnHandler.checkFailedCompactions(ci)) {
              LOG.warn("Will not initiate compaction for " + ci.getFullPartitionName() + " since last "+ HiveConf.ConfVars.COMPACTOR_INITIATOR_FAILED_THRESHOLD+ " attempts to compact it failed.");
              txnHandler.markFailed(ci);
              continue;
            }
            Partition p=resolvePartition(ci);
            if (p == null && ci.partName != null) {
              LOG.info("Can't find partition " + ci.getFullPartitionName() + ", assuming it has been dropped and moving on.");
              continue;
            }
            StorageDescriptor sd=resolveStorageDescriptor(t,p);
            String runAs=findUserToRunAs(sd.getLocation(),t);
            CompactionType compactionNeeded=checkForCompaction(ci,txns,sd,runAs);
            if (compactionNeeded != null)             requestCompaction(ci,runAs,compactionNeeded);
          }
 catch (          Throwable t) {
            LOG.error("Caught exception while trying to determine if we should compact " + ci + ".  Marking failed to avoid repeated failures, "+ ""+ StringUtils.stringifyException(t));
            txnHandler.markFailed(ci);
          }
        }
        recoverFailedCompactions(true);
        txnHandler.cleanEmptyAbortedTxns();
      }
 catch (      Throwable t) {
        LOG.error("Initiator loop caught unexpected exception this time through the loop: " + StringUtils.stringifyException(t));
      }
 finally {
        if (handle != null) {
          handle.releaseLocks();
        }
      }
      long elapsedTime=System.currentTimeMillis() - startedAt;
      if (elapsedTime >= checkInterval || stop.get())       continue;
 else       Thread.sleep(checkInterval - elapsedTime);
    }
 while (!stop.get());
  }
 catch (  Throwable t) {
    LOG.error("Caught an exception in the main loop of compactor initiator, exiting " + StringUtils.stringifyException(t));
  }
}
