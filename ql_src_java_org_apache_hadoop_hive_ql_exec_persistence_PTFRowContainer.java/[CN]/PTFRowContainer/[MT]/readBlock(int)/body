{
  currentReadBlockStartRow=getBlockSize() * blockNum;
  if (blockNum == numBlocks() - 1) {
    setWriteBlockAsReadBlock();
    return;
  }
  resetCurrentReadBlockToFirstReadBlock();
  BlockInfo bI=blockInfos.get(blockNum);
  int startSplit=bI.startingSplit;
  int endSplit=startSplit;
  if (blockNum != blockInfos.size() - 1) {
    endSplit=blockInfos.get(blockNum + 1).startingSplit;
  }
  try {
    int readIntoOffset=0;
    for (int i=startSplit; i <= endSplit; i++) {
      org.apache.hadoop.mapred.RecordReader rr=setReaderAtSplit(i);
      if (i == startSplit) {
        ((PTFSequenceFileRecordReader)rr).seek(bI.startOffset);
      }
      nextBlock(readIntoOffset);
      readIntoOffset=getCurrentReadBlockSize();
    }
  }
 catch (  Exception e) {
    clearRows();
    LOG.error(e.toString(),e);
    if (e instanceof HiveException) {
      throw (HiveException)e;
    }
    throw new HiveException(e);
  }
}
