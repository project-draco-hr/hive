{
  try {
    if (dynamicPartitioningUsed) {
      discoverPartitions(jobContext);
    }
    org.apache.hadoop.mapred.JobContext mapRedJobContext=HCatMapRedUtil.createJobContext(jobContext);
    if (getBaseOutputCommitter() != null && !dynamicPartitioningUsed) {
      getBaseOutputCommitter().abortJob(mapRedJobContext,state);
    }
 else     if (dynamicPartitioningUsed) {
      for (      JobContext currContext : contextDiscoveredByPath.values()) {
        try {
          new JobConf(currContext.getConfiguration()).getOutputCommitter().abortJob(currContext,state);
        }
 catch (        Exception e) {
          throw new IOException(e);
        }
      }
    }
    Path src;
    OutputJobInfo jobInfo=HCatOutputFormat.getJobInfo(jobContext.getConfiguration());
    Path tblPath=new Path(jobInfo.getTableInfo().getTableLocation());
    if (dynamicPartitioningUsed) {
      if (!customDynamicLocationUsed) {
        src=new Path(getPartitionRootLocation(jobInfo.getLocation(),jobInfo.getTableInfo().getTable().getPartitionKeysSize()));
      }
 else {
        src=new Path(getCustomPartitionRootLocation(jobInfo,jobContext.getConfiguration()));
      }
    }
 else {
      src=new Path(jobInfo.getLocation());
    }
    FileSystem fs=src.getFileSystem(jobContext.getConfiguration());
    LOG.info("Job failed. Try cleaning up temporary directory [{}].",src);
    if (!src.equals(tblPath)) {
      fs.delete(src,true);
    }
  }
  finally {
    cancelDelegationTokens(jobContext);
  }
}
