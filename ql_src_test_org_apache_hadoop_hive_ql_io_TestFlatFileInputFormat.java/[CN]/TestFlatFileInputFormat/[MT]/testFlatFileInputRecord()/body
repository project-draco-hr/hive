{
  Configuration conf;
  JobConf job;
  FileSystem fs;
  Path dir;
  Path file;
  Reporter reporter;
  FSDataOutputStream ds;
  try {
    conf=new Configuration();
    job=new JobConf(conf);
    fs=FileSystem.getLocal(conf);
    dir=new Path(System.getProperty("test.data.dir",".") + "/mapred");
    file=new Path(dir,"test.txt");
    reporter=Reporter.NULL;
    fs.delete(dir,true);
    job.setClass(FlatFileInputFormat.SerializationImplKey,org.apache.hadoop.io.serializer.WritableSerialization.class,org.apache.hadoop.io.serializer.Serialization.class);
    job.setClass(FlatFileInputFormat.SerializationContextFromConf.SerializationSubclassKey,RecordTestObj.class,Writable.class);
    FileInputFormat.setInputPaths(job,dir);
    ds=fs.create(file);
    Serializer serializer=new WritableSerialization().getSerializer(Writable.class);
    serializer.open(ds);
    for (int i=0; i < 10; i++) {
      serializer.serialize(new RecordTestObj("Hello World! " + String.valueOf(i),i));
    }
    serializer.close();
    FileInputFormat<Void,FlatFileInputFormat.RowContainer<Writable>> format=new FlatFileInputFormat<Writable>();
    InputSplit[] splits=format.getSplits(job,1);
    RecordReader<Void,FlatFileInputFormat.RowContainer<Writable>> reader=format.getRecordReader(splits[0],job,reporter);
    Void key=reader.createKey();
    FlatFileInputFormat.RowContainer<Writable> value=reader.createValue();
    int count=0;
    while (reader.next(key,value)) {
      assertTrue(key == null);
      assertTrue(((RecordTestObj)value.row).getS().equals("Hello World! " + String.valueOf(count)));
      assertTrue(((RecordTestObj)value.row).getNum() == count);
      count++;
    }
    reader.close();
  }
 catch (  Exception e) {
    System.err.println("caught: " + e);
    e.printStackTrace();
  }
 finally {
  }
}
