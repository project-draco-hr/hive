{
  try {
    serializer=(Serializer)conf.getTableInfo().getDeserializerClass().newInstance();
    serializer.initialize(null,conf.getTableInfo().getProperties());
    JobConf jc;
    if (hconf instanceof JobConf) {
      jc=(JobConf)hconf;
    }
 else {
      jc=new JobConf(hconf,ExecDriver.class);
    }
    int id=conf.getDestTableId();
    if ((id != 0) && (id <= TableIdEnum.values().length)) {
      String enumName="TABLE_ID_" + String.valueOf(id) + "_ROWCOUNT";
      tabIdEnum=TableIdEnum.valueOf(enumName);
      row_count=new LongWritable();
      statsMap.put(tabIdEnum,row_count);
    }
    String specPath=conf.getDirName();
    Path tmpPath=Utilities.toTempPath(specPath);
    String taskId=Utilities.getTaskId(hconf);
    fs=(new Path(specPath)).getFileSystem(hconf);
    finalPath=new Path(tmpPath,taskId);
    outPath=new Path(tmpPath,Utilities.toTempPath(taskId));
    LOG.info("Writing to temp file: FS " + outPath);
    HiveOutputFormat<?,?> hiveOutputFormat=conf.getTableInfo().getOutputFileFormatClass().newInstance();
    final Class<? extends Writable> outputClass=serializer.getSerializedClass();
    boolean isCompressed=conf.getCompressed();
    Path parent=Utilities.toTempPath(specPath);
    finalPath=HiveFileFormatUtils.getOutputFormatFinalPath(parent,jc,hiveOutputFormat,isCompressed,finalPath);
    tableDesc tableInfo=conf.getTableInfo();
    JobConf jc_output=jc;
    if (isCompressed) {
      jc_output=new JobConf(jc);
      String codecStr=conf.getCompressCodec();
      if (codecStr != null && !codecStr.trim().equals("")) {
        Class<? extends CompressionCodec> codec=(Class<? extends CompressionCodec>)Class.forName(codecStr);
        FileOutputFormat.setOutputCompressorClass(jc_output,codec);
      }
      String type=conf.getCompressType();
      if (type != null && !type.trim().equals("")) {
        CompressionType style=CompressionType.valueOf(type);
        SequenceFileOutputFormat.setOutputCompressionType(jc,style);
      }
    }
    outWriter=getRecordWriter(jc_output,hiveOutputFormat,outputClass,isCompressed,tableInfo.getProperties(),outPath);
    autoDelete=ShimLoader.getHadoopShims().fileSystemDeleteOnExit(fs,outPath);
    initializeChildren(hconf);
  }
 catch (  HiveException e) {
    throw e;
  }
catch (  Exception e) {
    e.printStackTrace();
    throw new HiveException(e);
  }
}
