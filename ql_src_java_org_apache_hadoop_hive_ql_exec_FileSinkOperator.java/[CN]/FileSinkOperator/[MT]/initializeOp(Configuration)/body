{
  try {
    serializer=(Serializer)conf.getTableInfo().getDeserializerClass().newInstance();
    serializer.initialize(null,conf.getTableInfo().getProperties());
    JobConf jc;
    if (hconf instanceof JobConf) {
      jc=(JobConf)hconf;
    }
 else {
      jc=new JobConf(hconf,ExecDriver.class);
    }
    int id=conf.getDestTableId();
    if ((id != 0) && (id <= TableIdEnum.values().length)) {
      String enumName="TABLE_ID_" + String.valueOf(id) + "_ROWCOUNT";
      tabIdEnum=TableIdEnum.valueOf(enumName);
      row_count=new LongWritable();
      statsMap.put(tabIdEnum,row_count);
    }
    String specPath=conf.getDirName();
    Path tmpPath=Utilities.toTempPath(specPath);
    String taskId=Utilities.getTaskId(hconf);
    fs=(new Path(specPath)).getFileSystem(hconf);
    finalPath=new Path(tmpPath,taskId);
    outPath=new Path(tmpPath,Utilities.toTempPath(taskId));
    LOG.info("Writing to temp file: FS " + outPath);
    HiveOutputFormat<?,?> hiveOutputFormat=conf.getTableInfo().getOutputFileFormatClass().newInstance();
    boolean isCompressed=conf.getCompressed();
    Path parent=Utilities.toTempPath(specPath);
    finalPath=HiveFileFormatUtils.getOutputFormatFinalPath(parent,jc,hiveOutputFormat,isCompressed,finalPath);
    final Class<? extends Writable> outputClass=serializer.getSerializedClass();
    outWriter=HiveFileFormatUtils.getHiveRecordWriter(jc,conf.getTableInfo(),outputClass,conf,outPath);
    autoDelete=ShimLoader.getHadoopShims().fileSystemDeleteOnExit(fs,outPath);
    initializeChildren(hconf);
  }
 catch (  HiveException e) {
    throw e;
  }
catch (  Exception e) {
    e.printStackTrace();
    throw new HiveException(e);
  }
}
