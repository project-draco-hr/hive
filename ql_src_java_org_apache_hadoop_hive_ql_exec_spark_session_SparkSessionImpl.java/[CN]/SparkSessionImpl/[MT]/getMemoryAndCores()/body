{
  SparkConf sparkConf=hiveSparkClient.getSparkConf();
  int cores=sparkConf.getInt("spark.executor.cores",1);
  double memoryFraction=sparkConf.getDouble("spark.shuffle.memoryFraction",0.2);
  int executorMemoryInMB=sparkConf.getInt("spark.executor.memory",512);
  long memoryPerTaskInBytes=(long)(executorMemoryInMB * memoryFraction * 1024* 1024 / cores);
  int executors=hiveSparkClient.getExecutorCount();
  int totalCores=executors * cores;
  LOG.info("Spark cluster current has executors: " + executors + ", cores per executor: "+ cores+ ", memory per executor: "+ executorMemoryInMB+ "M, shuffle memoryFraction: "+ memoryFraction);
  return new Tuple2<Long,Integer>(Long.valueOf(memoryPerTaskInBytes),Integer.valueOf(totalCores));
}
