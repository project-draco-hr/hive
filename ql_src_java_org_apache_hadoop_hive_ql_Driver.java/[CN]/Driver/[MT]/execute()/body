{
  PerfLogger perfLogger=PerfLogger.getPerfLogger();
  perfLogger.PerfLogBegin(LOG,PerfLogger.DRIVER_EXECUTE);
  boolean noName=StringUtils.isEmpty(conf.getVar(HiveConf.ConfVars.HADOOPJOBNAME));
  int maxlen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
  String queryId=plan.getQueryId();
  String queryStr=plan.getQueryStr();
  conf.setVar(HiveConf.ConfVars.HIVEQUERYID,queryId);
  conf.setVar(HiveConf.ConfVars.HIVEQUERYSTRING,queryStr);
  maxthreads=HiveConf.getIntVar(conf,HiveConf.ConfVars.EXECPARALLETHREADNUMBER);
  try {
    LOG.info("Starting command: " + queryStr);
    plan.setStarted();
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().startQuery(queryStr,conf.getVar(HiveConf.ConfVars.HIVEQUERYID));
      SessionState.get().getHiveHistory().logPlanProgress(plan);
    }
    resStream=null;
    HookContext hookContext=new HookContext(plan,conf,ctx.getPathToCS());
    hookContext.setHookType(HookContext.HookType.PRE_EXEC_HOOK);
    for (    Hook peh : getPreExecHooks()) {
      if (peh instanceof ExecuteWithHookContext) {
        perfLogger.PerfLogBegin(LOG,PerfLogger.PRE_HOOK + peh.getClass().getName());
        ((ExecuteWithHookContext)peh).run(hookContext);
        perfLogger.PerfLogEnd(LOG,PerfLogger.PRE_HOOK + peh.getClass().getName());
      }
 else       if (peh instanceof PreExecute) {
        perfLogger.PerfLogBegin(LOG,PerfLogger.PRE_HOOK + peh.getClass().getName());
        ((PreExecute)peh).run(SessionState.get(),plan.getInputs(),plan.getOutputs(),ShimLoader.getHadoopShims().getUGIForConf(conf));
        perfLogger.PerfLogEnd(LOG,PerfLogger.PRE_HOOK + peh.getClass().getName());
      }
    }
    int jobs=Utilities.getMRTasks(plan.getRootTasks()).size();
    if (jobs > 0) {
      console.printInfo("Total MapReduce jobs = " + jobs);
    }
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_NUM_TASKS,String.valueOf(jobs));
      SessionState.get().getHiveHistory().setIdToTableMap(plan.getIdToTableNameMap());
    }
    String jobname=Utilities.abbreviate(queryStr,maxlen - 6);
    Queue<Task<? extends Serializable>> runnable=new ConcurrentLinkedQueue<Task<? extends Serializable>>();
    Map<TaskResult,TaskRunner> running=new HashMap<TaskResult,TaskRunner>();
    DriverContext driverCxt=new DriverContext(runnable,ctx);
    SessionState.get().setLastMapRedStatsList(new ArrayList<MapRedStats>());
    for (    Task<? extends Serializable> tsk : plan.getRootTasks()) {
      driverCxt.addToRunnable(tsk);
    }
    while (running.size() != 0 || runnable.peek() != null) {
      while (runnable.peek() != null && running.size() < maxthreads) {
        Task<? extends Serializable> tsk=runnable.remove();
        launchTask(tsk,queryId,noName,running,jobname,jobs,driverCxt);
      }
      TaskResult tskRes=pollTasks(running.keySet());
      TaskRunner tskRun=running.remove(tskRes);
      Task<? extends Serializable> tsk=tskRun.getTask();
      hookContext.addCompleteTask(tskRun);
      int exitVal=tskRes.getExitVal();
      if (exitVal != 0) {
        if (tsk.ifRetryCmdWhenFail()) {
          if (running.size() != 0) {
            taskCleanup();
          }
          ctx.restoreOriginalTracker();
          throw new CommandNeedRetryException();
        }
        Task<? extends Serializable> backupTask=tsk.getAndInitBackupTask();
        if (backupTask != null) {
          errorMessage="FAILED: Execution Error, return code " + exitVal + " from "+ tsk.getClass().getName();
          console.printError(errorMessage);
          errorMessage="ATTEMPT: Execute BackupTask: " + backupTask.getClass().getName();
          console.printError(errorMessage);
          if (DriverContext.isLaunchable(backupTask)) {
            driverCxt.addToRunnable(backupTask);
          }
          continue;
        }
 else {
          hookContext.setHookType(HookContext.HookType.ON_FAILURE_HOOK);
          for (          Hook ofh : getOnFailureHooks()) {
            perfLogger.PerfLogBegin(LOG,PerfLogger.FAILURE_HOOK + ofh.getClass().getName());
            ((ExecuteWithHookContext)ofh).run(hookContext);
            perfLogger.PerfLogEnd(LOG,PerfLogger.FAILURE_HOOK + ofh.getClass().getName());
          }
          errorMessage="FAILED: Execution Error, return code " + exitVal + " from "+ tsk.getClass().getName();
          SQLState="08S01";
          console.printError(errorMessage);
          if (running.size() != 0) {
            taskCleanup();
          }
          ctx.restoreOriginalTracker();
          return 9;
        }
      }
      if (SessionState.get() != null) {
        SessionState.get().getHiveHistory().setTaskProperty(queryId,tsk.getId(),Keys.TASK_RET_CODE,String.valueOf(exitVal));
        SessionState.get().getHiveHistory().endTask(queryId,tsk);
      }
      if (tsk.getChildTasks() != null) {
        for (        Task<? extends Serializable> child : tsk.getChildTasks()) {
          if (DriverContext.isLaunchable(child)) {
            driverCxt.addToRunnable(child);
          }
        }
      }
    }
    ctx.restoreOriginalTracker();
    HashSet<WriteEntity> remOutputs=new HashSet<WriteEntity>();
    for (    WriteEntity output : plan.getOutputs()) {
      if (!output.isComplete()) {
        remOutputs.add(output);
      }
    }
    for (    WriteEntity output : remOutputs) {
      plan.getOutputs().remove(output);
    }
    hookContext.setHookType(HookContext.HookType.POST_EXEC_HOOK);
    for (    Hook peh : getPostExecHooks()) {
      if (peh instanceof ExecuteWithHookContext) {
        perfLogger.PerfLogBegin(LOG,PerfLogger.POST_HOOK + peh.getClass().getName());
        ((ExecuteWithHookContext)peh).run(hookContext);
        perfLogger.PerfLogEnd(LOG,PerfLogger.POST_HOOK + peh.getClass().getName());
      }
 else       if (peh instanceof PostExecute) {
        perfLogger.PerfLogBegin(LOG,PerfLogger.POST_HOOK + peh.getClass().getName());
        ((PostExecute)peh).run(SessionState.get(),plan.getInputs(),plan.getOutputs(),(SessionState.get() != null ? SessionState.get().getLineageState().getLineageInfo() : null),ShimLoader.getHadoopShims().getUGIForConf(conf));
        perfLogger.PerfLogEnd(LOG,PerfLogger.POST_HOOK + peh.getClass().getName());
      }
    }
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_RET_CODE,String.valueOf(0));
      SessionState.get().getHiveHistory().printRowCount(queryId);
    }
  }
 catch (  CommandNeedRetryException e) {
    throw e;
  }
catch (  Exception e) {
    ctx.restoreOriginalTracker();
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_RET_CODE,String.valueOf(12));
    }
    errorMessage="FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
    SQLState="08S01";
    console.printError(errorMessage + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return (12);
  }
 finally {
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().endQuery(queryId);
    }
    if (noName) {
      conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,"");
    }
    perfLogger.PerfLogEnd(LOG,PerfLogger.DRIVER_EXECUTE);
    if (SessionState.get().getLastMapRedStatsList() != null && SessionState.get().getLastMapRedStatsList().size() > 0) {
      long totalCpu=0;
      console.printInfo("MapReduce Jobs Launched: ");
      for (int i=0; i < SessionState.get().getLastMapRedStatsList().size(); i++) {
        console.printInfo("Job " + i + ": "+ SessionState.get().getLastMapRedStatsList().get(i));
        totalCpu+=SessionState.get().getLastMapRedStatsList().get(i).getCpuMSec();
      }
      console.printInfo("Total MapReduce CPU Time Spent: " + Utilities.formatMsecToStr(totalCpu));
    }
  }
  plan.setDone();
  if (SessionState.get() != null) {
    try {
      SessionState.get().getHiveHistory().logPlanProgress(plan);
    }
 catch (    Exception e) {
    }
  }
  console.printInfo("OK");
  return (0);
}
