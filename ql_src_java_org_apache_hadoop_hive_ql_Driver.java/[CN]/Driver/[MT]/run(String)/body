{
  boolean noName=StringUtils.isEmpty(conf.getVar(HiveConf.ConfVars.HADOOPJOBNAME));
  int maxlen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
  int jobs=0;
  conf.setVar(HiveConf.ConfVars.HIVEQUERYID,command);
  try {
    TaskFactory.resetId();
    BaseSemanticAnalyzer sem;
    LOG.info("Starting command: " + command);
    ctx.clear();
    resStream=null;
    pd=new ParseDriver();
    CommonTree tree=pd.parse(command);
    while ((tree.getToken() == null) && (tree.getChildCount() > 0)) {
      tree=(CommonTree)tree.getChild(0);
    }
    sem=SemanticAnalyzerFactory.get(conf,tree);
    sem.analyze(tree,ctx);
    LOG.info("Semantic Analysis Completed");
    jobs=countJobs(sem.getRootTasks());
    if (jobs > 0) {
      console.printInfo("Total MapReduce jobs = " + jobs);
    }
    String jobname=Utilities.abbreviate(command,maxlen - 6);
    int curJob=0;
    for (    Task<? extends Serializable> rootTask : sem.getRootTasks()) {
      if ((rootTask instanceof ExecDriver) || (rootTask instanceof MapRedTask)) {
        curJob++;
        if (noName) {
          conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,jobname + "(" + curJob+ "/"+ jobs+ ")");
        }
      }
      rootTask.initialize(conf);
    }
    Queue<Task<? extends Serializable>> runnable=new LinkedList<Task<? extends Serializable>>();
    for (    Task<? extends Serializable> rootTask : sem.getRootTasks()) {
      if (runnable.offer(rootTask) == false) {
        LOG.error("Could not insert the first task into the queue");
        return (1);
      }
    }
    while (runnable.peek() != null) {
      Task<? extends Serializable> tsk=runnable.remove();
      int exitVal=tsk.execute();
      if (exitVal != 0) {
        console.printError("FAILED: Execution Error, return code " + exitVal + " from "+ tsk.getClass().getName());
        return 9;
      }
      tsk.setDone();
      if (tsk.getChildTasks() == null) {
        continue;
      }
      for (      Task<? extends Serializable> child : tsk.getChildTasks()) {
        if (!child.isRunnable()) {
          continue;
        }
        if (runnable.offer(child) == false) {
          LOG.error("Could not add child task to queue");
        }
      }
    }
  }
 catch (  SemanticException e) {
    console.printError("FAILED: Error in semantic analysis: " + e.getMessage());
    return (10);
  }
catch (  ParseException e) {
    console.printError("FAILED: Parse Error: " + e.getMessage());
    return (11);
  }
catch (  Exception e) {
    console.printError("FAILED: Unknown exception : " + e.getMessage(),"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return (12);
  }
 finally {
    if (noName) {
      conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,"");
    }
  }
  console.printInfo("OK");
  return (0);
}
