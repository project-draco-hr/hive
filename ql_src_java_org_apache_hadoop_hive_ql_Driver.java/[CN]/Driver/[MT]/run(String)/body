{
  boolean noName=StringUtils.isEmpty(conf.getVar(HiveConf.ConfVars.HADOOPJOBNAME));
  int maxlen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
  int jobs=0;
  conf.setVar(HiveConf.ConfVars.HIVEQUERYSTRING,command);
  String queryId=makeQueryId();
  conf.setVar(HiveConf.ConfVars.HIVEQUERYID,queryId);
  try {
    TaskFactory.resetId();
    LOG.info("Starting command: " + command);
    ctx.clear();
    ctx.makeScratchDir();
    if (SessionState.get() != null)     SessionState.get().getHiveHistory().startQuery(command,conf.getVar(HiveConf.ConfVars.HIVEQUERYID));
    resStream=null;
    pd=new ParseDriver();
    ASTNode tree=pd.parse(command);
    while ((tree.getToken() == null) && (tree.getChildCount() > 0)) {
      tree=(ASTNode)tree.getChild(0);
    }
    sem=SemanticAnalyzerFactory.get(conf,tree);
    sem.analyze(tree,ctx);
    LOG.info("Semantic Analysis Completed");
    jobs=countJobs(sem.getRootTasks());
    if (jobs > 0) {
      console.printInfo("Total MapReduce jobs = " + jobs);
    }
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_NUM_TASKS,String.valueOf(jobs));
      SessionState.get().getHiveHistory().setIdToTableMap(sem.getIdToTableNameMap());
    }
    String jobname=Utilities.abbreviate(command,maxlen - 6);
    int curJob=0;
    for (    Task<? extends Serializable> rootTask : sem.getRootTasks()) {
      if (rootTask.isMapRedTask()) {
        curJob++;
        if (noName) {
          conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,jobname + "(" + curJob+ "/"+ jobs+ ")");
        }
      }
      rootTask.initialize(conf);
    }
    Queue<Task<? extends Serializable>> runnable=new LinkedList<Task<? extends Serializable>>();
    for (    Task<? extends Serializable> rootTask : sem.getRootTasks()) {
      if (runnable.offer(rootTask) == false) {
        LOG.error("Could not insert the first task into the queue");
        return (1);
      }
    }
    while (runnable.peek() != null) {
      Task<? extends Serializable> tsk=runnable.remove();
      if (SessionState.get() != null)       SessionState.get().getHiveHistory().startTask(queryId,tsk,tsk.getClass().getName());
      int exitVal=tsk.execute();
      if (SessionState.get() != null) {
        SessionState.get().getHiveHistory().setTaskProperty(queryId,tsk.getId(),Keys.TASK_RET_CODE,String.valueOf(exitVal));
        SessionState.get().getHiveHistory().endTask(queryId,tsk);
      }
      if (exitVal != 0) {
        console.printError("FAILED: Execution Error, return code " + exitVal + " from "+ tsk.getClass().getName());
        return 9;
      }
      tsk.setDone();
      if (tsk.getChildTasks() == null) {
        continue;
      }
      for (      Task<? extends Serializable> child : tsk.getChildTasks()) {
        if (!child.isRunnable()) {
          continue;
        }
        if (runnable.offer(child) == false) {
          LOG.error("Could not add child task to queue");
        }
      }
    }
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_RET_CODE,String.valueOf(0));
      SessionState.get().getHiveHistory().printRowCount();
    }
  }
 catch (  SemanticException e) {
    if (SessionState.get() != null)     SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_RET_CODE,String.valueOf(10));
    console.printError("FAILED: Error in semantic analysis: " + e.getMessage(),"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return (10);
  }
catch (  ParseException e) {
    if (SessionState.get() != null)     SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_RET_CODE,String.valueOf(11));
    console.printError("FAILED: Parse Error: " + e.getMessage(),"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return (11);
  }
catch (  Exception e) {
    if (SessionState.get() != null)     SessionState.get().getHiveHistory().setQueryProperty(queryId,Keys.QUERY_RET_CODE,String.valueOf(12));
    console.printError("FAILED: Unknown exception : " + e.getMessage(),"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return (12);
  }
 finally {
    if (SessionState.get() != null)     SessionState.get().getHiveHistory().endQuery(queryId);
    if (noName) {
      conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,"");
    }
  }
  console.printInfo("OK");
  return (0);
}
