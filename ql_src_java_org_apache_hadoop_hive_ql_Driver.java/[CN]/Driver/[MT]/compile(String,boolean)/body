{
  PerfLogger perfLogger=SessionState.getPerfLogger();
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.COMPILE);
  command=new VariableSubstitution(new HiveVariableSource(){
    @Override public Map<String,String> getHiveVariable(){
      return SessionState.get().getHiveVariables();
    }
  }
).substitute(conf,command);
  String queryStr=command;
  try {
    queryStr=HookUtils.redactLogString(conf,command);
  }
 catch (  Exception e) {
    LOG.warn("WARNING! Query command could not be redacted." + e);
  }
  this.savedQueryString=queryStr;
  QueryState queryState=new QueryState();
  if (ctx != null) {
    close();
  }
  if (resetTaskIds) {
    TaskFactory.resetId();
  }
  saveSession(queryState);
  String queryId=conf.getVar(HiveConf.ConfVars.HIVEQUERYID);
  if (queryId == null || queryId.isEmpty()) {
    queryId=QueryPlan.makeQueryId();
    conf.setVar(HiveConf.ConfVars.HIVEQUERYID,queryId);
  }
  LOG.info("Compiling command(queryId=" + queryId + "): "+ queryStr);
  SessionState.get().setupQueryCurrentTimestamp();
  try {
    final HiveTxnManager txnManager=SessionState.get().initTxnMgr(conf);
    ShutdownHookManager.removeShutdownHook(shutdownRunner);
    shutdownRunner=new Runnable(){
      @Override public void run(){
        try {
          releaseLocksAndCommitOrRollback(false,txnManager);
        }
 catch (        LockException e) {
          LOG.warn("Exception when releasing locks in ShutdownHook for Driver: " + e.getMessage());
        }
      }
    }
;
    ShutdownHookManager.addShutdownHook(shutdownRunner,SHUTDOWN_HOOK_PRIORITY);
    ctx=new Context(conf);
    ctx.setTryCount(getTryCount());
    ctx.setCmd(command);
    ctx.setHDFSCleanup(true);
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.PARSE);
    ParseDriver pd=new ParseDriver();
    ASTNode tree=pd.parse(command,ctx);
    tree=ParseUtils.findRootNonNullToken(tree);
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.PARSE);
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.ANALYZE);
    BaseSemanticAnalyzer sem=SemanticAnalyzerFactory.get(conf,tree);
    List<HiveSemanticAnalyzerHook> saHooks=getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,HiveSemanticAnalyzerHook.class);
    Hive.get().getMSC().flushCache();
    if (saHooks != null && !saHooks.isEmpty()) {
      HiveSemanticAnalyzerHookContext hookCtx=new HiveSemanticAnalyzerHookContextImpl();
      hookCtx.setConf(conf);
      hookCtx.setUserName(userName);
      hookCtx.setIpAddress(SessionState.get().getUserIpAddress());
      hookCtx.setCommand(command);
      for (      HiveSemanticAnalyzerHook hook : saHooks) {
        tree=hook.preAnalyze(hookCtx,tree);
      }
      sem.analyze(tree,ctx);
      hookCtx.update(sem);
      for (      HiveSemanticAnalyzerHook hook : saHooks) {
        hook.postAnalyze(hookCtx,sem.getRootTasks());
      }
    }
 else {
      sem.analyze(tree,ctx);
    }
    acidSinks=sem.getAcidFileSinks();
    LOG.info("Semantic Analysis Completed");
    sem.validate();
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.ANALYZE);
    schema=getSchema(sem,conf);
    plan=new QueryPlan(queryStr,sem,perfLogger.getStartTime(PerfLogger.DRIVER_RUN),queryId,SessionState.get().getHiveOperation(),schema);
    conf.setVar(HiveConf.ConfVars.HIVEQUERYSTRING,queryStr);
    conf.set("mapreduce.workflow.id","hive_" + queryId);
    conf.set("mapreduce.workflow.name",queryStr);
    if (plan.getFetchTask() != null) {
      plan.getFetchTask().initialize(conf,plan,null,ctx.getOpContext());
    }
    if (!sem.skipAuthorization() && HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {
      try {
        perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.DO_AUTHORIZATION);
        doAuthorization(sem,command);
      }
 catch (      AuthorizationException authExp) {
        console.printError("Authorization failed:" + authExp.getMessage() + ". Use SHOW GRANT to get more details.");
        errorMessage=authExp.getMessage();
        SQLState="42000";
        return 403;
      }
 finally {
        perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.DO_AUTHORIZATION);
      }
    }
    if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT)) {
      String explainOutput=getExplainOutput(sem,plan,tree);
      if (explainOutput != null) {
        LOG.info("EXPLAIN output for queryid " + queryId + " : "+ explainOutput);
      }
    }
    return 0;
  }
 catch (  Exception e) {
    ErrorMsg error=ErrorMsg.getErrorMsg(e.getMessage());
    errorMessage="FAILED: " + e.getClass().getSimpleName();
    if (error != ErrorMsg.GENERIC_ERROR) {
      errorMessage+=" [Error " + error.getErrorCode() + "]:";
    }
    if ((e instanceof IllegalArgumentException) && e.getMessage() == null && e.getCause() != null) {
      errorMessage+=" " + e.getCause().getMessage();
    }
 else {
      errorMessage+=" " + e.getMessage();
    }
    SQLState=error.getSQLState();
    downstreamError=e;
    console.printError(errorMessage,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return error.getErrorCode();
  }
 finally {
    double duration=perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.COMPILE) / 1000.00;
    dumpMetaCallTimingWithoutEx("compilation");
    restoreSession(queryState);
    LOG.info("Completed compiling command(queryId=" + queryId + "); Time taken: "+ duration+ " seconds");
  }
}
