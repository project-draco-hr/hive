{
  PerfLogger perfLogger=PerfLogger.getPerfLogger();
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.COMPILE);
  QueryState queryState=new QueryState();
  if (plan != null) {
    close();
    plan=null;
  }
  if (resetTaskIds) {
    TaskFactory.resetId();
  }
  saveSession(queryState);
  try {
    command=new VariableSubstitution().substitute(conf,command);
    ctx=new Context(conf);
    ctx.setTryCount(getTryCount());
    ctx.setCmd(command);
    ctx.setHDFSCleanup(true);
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.PARSE);
    ParseDriver pd=new ParseDriver();
    ASTNode tree=pd.parse(command,ctx);
    tree=ParseUtils.findRootNonNullToken(tree);
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.PARSE);
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.ANALYZE);
    BaseSemanticAnalyzer sem=SemanticAnalyzerFactory.get(conf,tree);
    List<HiveSemanticAnalyzerHook> saHooks=getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,HiveSemanticAnalyzerHook.class);
    if (saHooks != null) {
      HiveSemanticAnalyzerHookContext hookCtx=new HiveSemanticAnalyzerHookContextImpl();
      hookCtx.setConf(conf);
      for (      HiveSemanticAnalyzerHook hook : saHooks) {
        tree=hook.preAnalyze(hookCtx,tree);
      }
      sem.analyze(tree,ctx);
      hookCtx.update(sem);
      for (      HiveSemanticAnalyzerHook hook : saHooks) {
        hook.postAnalyze(hookCtx,sem.getRootTasks());
      }
    }
 else {
      sem.analyze(tree,ctx);
    }
    LOG.info("Semantic Analysis Completed");
    sem.validate();
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.ANALYZE);
    plan=new QueryPlan(command,sem,perfLogger.getStartTime(PerfLogger.DRIVER_RUN));
    if ("true".equalsIgnoreCase(System.getProperty("test.serialize.qplan"))) {
      String queryPlanFileName=ctx.getLocalScratchDir(true) + Path.SEPARATOR_CHAR + "queryplan.xml";
      LOG.info("query plan = " + queryPlanFileName);
      queryPlanFileName=new Path(queryPlanFileName).toUri().getPath();
      FileOutputStream fos=new FileOutputStream(queryPlanFileName);
      Utilities.serializePlan(plan,fos,conf);
      fos.close();
      FileInputStream fis=new FileInputStream(queryPlanFileName);
      QueryPlan newPlan=Utilities.deserializePlan(fis,QueryPlan.class,conf);
      fis.close();
      plan=newPlan;
    }
    if (plan.getFetchTask() != null) {
      plan.getFetchTask().initialize(conf,plan,null);
    }
    schema=getSchema(sem,conf);
    if (HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {
      try {
        perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.DO_AUTHORIZATION);
        doAuthorization(sem);
      }
 catch (      AuthorizationException authExp) {
        errorMessage="Authorization failed:" + authExp.getMessage() + ". Use show grant to get more details.";
        console.printError(errorMessage);
        return 403;
      }
 finally {
        perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.DO_AUTHORIZATION);
      }
    }
    return 0;
  }
 catch (  Exception e) {
    ErrorMsg error=ErrorMsg.getErrorMsg(e.getMessage());
    errorMessage="FAILED: " + e.getClass().getSimpleName();
    if (error != ErrorMsg.GENERIC_ERROR) {
      errorMessage+=" [Error " + error.getErrorCode() + "]:";
    }
    if ((e instanceof IllegalArgumentException) && e.getMessage() == null && e.getCause() != null) {
      errorMessage+=" " + e.getCause().getMessage();
    }
 else {
      errorMessage+=" " + e.getMessage();
    }
    SQLState=error.getSQLState();
    downstreamError=e;
    console.printError(errorMessage,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return error.getErrorCode();
  }
 finally {
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.COMPILE);
    restoreSession(queryState);
  }
}
