{
  PerfLogger perfLogger=PerfLogger.getPerfLogger();
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.COMPILE);
  QueryState queryState=new QueryState();
  if (plan != null) {
    close();
    plan=null;
  }
  if (resetTaskIds) {
    TaskFactory.resetId();
  }
  saveSession(queryState);
  String queryId=QueryPlan.makeQueryId();
  conf.setVar(HiveConf.ConfVars.HIVEQUERYID,queryId);
  try {
    command=new VariableSubstitution().substitute(conf,command);
    ctx=new Context(conf);
    ctx.setTryCount(getTryCount());
    ctx.setCmd(command);
    ctx.setHDFSCleanup(true);
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.PARSE);
    ParseDriver pd=new ParseDriver();
    ASTNode tree=pd.parse(command,ctx);
    tree=ParseUtils.findRootNonNullToken(tree);
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.PARSE);
    SessionState.get().initTxnMgr(conf);
    recordValidTxns();
    perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.ANALYZE);
    BaseSemanticAnalyzer sem=SemanticAnalyzerFactory.get(conf,tree);
    List<HiveSemanticAnalyzerHook> saHooks=getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,HiveSemanticAnalyzerHook.class);
    if (saHooks != null) {
      HiveSemanticAnalyzerHookContext hookCtx=new HiveSemanticAnalyzerHookContextImpl();
      hookCtx.setConf(conf);
      hookCtx.setUserName(userName);
      for (      HiveSemanticAnalyzerHook hook : saHooks) {
        tree=hook.preAnalyze(hookCtx,tree);
      }
      sem.analyze(tree,ctx);
      hookCtx.update(sem);
      for (      HiveSemanticAnalyzerHook hook : saHooks) {
        hook.postAnalyze(hookCtx,sem.getRootTasks());
      }
    }
 else {
      sem.analyze(tree,ctx);
    }
    acidSinks=sem.getAcidFileSinks();
    LOG.info("Semantic Analysis Completed");
    sem.validate();
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.ANALYZE);
    plan=new QueryPlan(command,sem,perfLogger.getStartTime(PerfLogger.DRIVER_RUN),queryId,SessionState.get().getCommandType());
    String queryStr=plan.getQueryStr();
    conf.setVar(HiveConf.ConfVars.HIVEQUERYSTRING,queryStr);
    conf.set("mapreduce.workflow.id","hive_" + queryId);
    conf.set("mapreduce.workflow.name",queryStr);
    if (plan.getFetchTask() != null) {
      plan.getFetchTask().initialize(conf,plan,null);
    }
    schema=getSchema(sem,conf);
    if (!sem.skipAuthorization() && HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {
      try {
        perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.DO_AUTHORIZATION);
        doAuthorization(sem,command);
      }
 catch (      AuthorizationException authExp) {
        console.printError("Authorization failed:" + authExp.getMessage() + ". Use SHOW GRANT to get more details.");
        errorMessage=authExp.getMessage();
        SQLState="42000";
        return 403;
      }
 finally {
        perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.DO_AUTHORIZATION);
      }
    }
    return 0;
  }
 catch (  Exception e) {
    ErrorMsg error=ErrorMsg.getErrorMsg(e.getMessage());
    errorMessage="FAILED: " + e.getClass().getSimpleName();
    if (error != ErrorMsg.GENERIC_ERROR) {
      errorMessage+=" [Error " + error.getErrorCode() + "]:";
    }
    if ((e instanceof IllegalArgumentException) && e.getMessage() == null && e.getCause() != null) {
      errorMessage+=" " + e.getCause().getMessage();
    }
 else {
      errorMessage+=" " + e.getMessage();
    }
    SQLState=error.getSQLState();
    downstreamError=e;
    console.printError(errorMessage,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return error.getErrorCode();
  }
 finally {
    perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.COMPILE);
    restoreSession(queryState);
  }
}
