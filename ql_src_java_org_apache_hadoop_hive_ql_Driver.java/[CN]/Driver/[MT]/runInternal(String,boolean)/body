{
  errorMessage=null;
  SQLState=null;
  downstreamError=null;
  if (!validateConfVariables()) {
    return createProcessorResponse(12);
  }
  HiveDriverRunHookContext hookContext=new HiveDriverRunHookContextImpl(conf,command);
  List<HiveDriverRunHook> driverRunHooks;
  try {
    driverRunHooks=getHooks(HiveConf.ConfVars.HIVE_DRIVER_RUN_HOOKS,HiveDriverRunHook.class);
    for (    HiveDriverRunHook driverRunHook : driverRunHooks) {
      driverRunHook.preDriverRun(hookContext);
    }
  }
 catch (  Exception e) {
    errorMessage="FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
    SQLState=ErrorMsg.findSQLState(e.getMessage());
    downstreamError=e;
    console.printError(errorMessage + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return createProcessorResponse(12);
  }
  PerfLogger perfLogger=PerfLogger.getPerfLogger(true);
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.DRIVER_RUN);
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.TIME_TO_SUBMIT);
  boolean requireLock=false;
  boolean ckLock=false;
  SessionState ss=SessionState.get();
  try {
    ckLock=checkConcurrency();
    try {
      ss.initTxnMgr(conf);
    }
 catch (    LockException e) {
      throw new SemanticException(e.getMessage(),e);
    }
  }
 catch (  SemanticException e) {
    errorMessage="FAILED: Error in semantic analysis: " + e.getMessage();
    SQLState=ErrorMsg.findSQLState(e.getMessage());
    downstreamError=e;
    console.printError(errorMessage,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return createProcessorResponse(10);
  }
  int ret=recordValidTxns();
  if (ret != 0) {
    return createProcessorResponse(ret);
  }
  if (!alreadyCompiled) {
    ret=compileInternal(command);
    if (ret != 0) {
      return createProcessorResponse(ret);
    }
  }
  ctx.setHiveTxnManager(ss.getTxnMgr());
  if (ckLock) {
    boolean lockOnlyMapred=HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_LOCK_MAPRED_ONLY);
    if (lockOnlyMapred) {
      Queue<Task<? extends Serializable>> taskQueue=new LinkedList<Task<? extends Serializable>>();
      taskQueue.addAll(plan.getRootTasks());
      while (taskQueue.peek() != null) {
        Task<? extends Serializable> tsk=taskQueue.remove();
        requireLock=requireLock || tsk.requireLock();
        if (requireLock) {
          break;
        }
        if (tsk instanceof ConditionalTask) {
          taskQueue.addAll(((ConditionalTask)tsk).getListTasks());
        }
        if (tsk.getChildTasks() != null) {
          taskQueue.addAll(tsk.getChildTasks());
        }
      }
    }
 else {
      requireLock=true;
    }
  }
  if (requireLock) {
    ret=acquireReadWriteLocks();
    if (ret != 0) {
      try {
        releaseLocks(ctx.getHiveLocks());
      }
 catch (      LockException e) {
      }
      return createProcessorResponse(ret);
    }
  }
  ret=execute();
  if (ret != 0) {
    try {
      releaseLocks(ctx.getHiveLocks());
    }
 catch (    LockException e) {
    }
    return createProcessorResponse(ret);
  }
  try {
    releaseLocks(ctx.getHiveLocks());
  }
 catch (  LockException e) {
    errorMessage="FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
    SQLState=ErrorMsg.findSQLState(e.getMessage());
    downstreamError=e;
    console.printError(errorMessage + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return createProcessorResponse(12);
  }
  perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.DRIVER_RUN);
  perfLogger.close(LOG,plan);
  try {
    for (    HiveDriverRunHook driverRunHook : driverRunHooks) {
      driverRunHook.postDriverRun(hookContext);
    }
  }
 catch (  Exception e) {
    errorMessage="FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
    SQLState=ErrorMsg.findSQLState(e.getMessage());
    downstreamError=e;
    console.printError(errorMessage + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return createProcessorResponse(12);
  }
  return createProcessorResponse(ret);
}
