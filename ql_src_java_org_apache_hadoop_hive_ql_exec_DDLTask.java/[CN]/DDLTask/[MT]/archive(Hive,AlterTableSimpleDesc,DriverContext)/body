{
  Table tbl=db.getTable(simpleDesc.getTableName());
  if (tbl.getTableType() != TableType.MANAGED_TABLE) {
    throw new HiveException("ARCHIVE can only be performed on managed tables");
  }
  Map<String,String> partSpec=simpleDesc.getPartSpec();
  PartSpecInfo partSpecInfo=PartSpecInfo.create(tbl,partSpec);
  List<Partition> partitions=db.getPartitions(tbl,partSpec);
  Path originalDir=null;
  if (partitions.isEmpty()) {
    throw new HiveException("No partition matches the specification");
  }
 else   if (partSpecInfo.values.size() != tbl.getPartCols().size()) {
    for (    Partition p : partitions) {
      if (partitionInCustomLocation(tbl,p)) {
        String message=String.format("ARCHIVE cannot run for partition " + "groups with custom locations like %s",p.getLocation());
        throw new HiveException(message);
      }
    }
    originalDir=partSpecInfo.createPath(tbl);
  }
 else {
    Partition p=partitions.get(0);
    if (ArchiveUtils.isArchived(p)) {
      originalDir=new Path(getOriginalLocation(p));
    }
 else {
      originalDir=p.getDataLocation();
    }
  }
  Path intermediateArchivedDir=new Path(originalDir.getParent(),originalDir.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);
  Path intermediateOriginalDir=new Path(originalDir.getParent(),originalDir.getName() + INTERMEDIATE_ORIGINAL_DIR_SUFFIX);
  console.printInfo("intermediate.archived is " + intermediateArchivedDir.toString());
  console.printInfo("intermediate.original is " + intermediateOriginalDir.toString());
  String archiveName="data.har";
  FileSystem fs=null;
  try {
    fs=originalDir.getFileSystem(conf);
  }
 catch (  IOException e) {
    throw new HiveException(e);
  }
  URI archiveUri=(new Path(originalDir,archiveName)).toUri();
  URI originalUri=ArchiveUtils.addSlash(originalDir.toUri());
  ArchiveUtils.HarPathHelper harHelper=new ArchiveUtils.HarPathHelper(conf,archiveUri,originalUri);
  for (  Partition p : partitions) {
    if (ArchiveUtils.isArchived(p)) {
      if (ArchiveUtils.getArchivingLevel(p) != partSpecInfo.values.size()) {
        String name=ArchiveUtils.getPartialName(p,ArchiveUtils.getArchivingLevel(p));
        String m=String.format("Conflict with existing archive %s",name);
        throw new HiveException(m);
      }
 else {
        throw new HiveException("Partition(s) already archived");
      }
    }
  }
  boolean recovery=false;
  if (pathExists(intermediateArchivedDir) || pathExists(intermediateOriginalDir)) {
    recovery=true;
    console.printInfo("Starting recovery after failed ARCHIVE");
  }
  if (!pathExists(intermediateArchivedDir) && !pathExists(intermediateOriginalDir)) {
    Path tmpPath=new Path(driverContext.getCtx().getExternalTmpPath(originalDir),"partlevel");
    console.printInfo("Creating " + archiveName + " for "+ originalDir.toString());
    console.printInfo("in " + tmpPath);
    console.printInfo("Please wait... (this may take a while)");
    int ret=0;
    try {
      int maxJobNameLen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
      String jobname=String.format("Archiving %s@%s",tbl.getTableName(),partSpecInfo.getName());
      jobname=Utilities.abbreviate(jobname,maxJobNameLen - 6);
      conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,jobname);
      HadoopArchives har=new HadoopArchives(conf);
      List<String> args=new ArrayList<String>();
      args.add("-archiveName");
      args.add(archiveName);
      args.add("-p");
      args.add(originalDir.toString());
      args.add(tmpPath.toString());
      ret=ToolRunner.run(har,args.toArray(new String[0]));
      ;
    }
 catch (    Exception e) {
      throw new HiveException(e);
    }
    if (ret != 0) {
      throw new HiveException("Error while creating HAR");
    }
    try {
      console.printInfo("Moving " + tmpPath + " to "+ intermediateArchivedDir);
      if (pathExists(intermediateArchivedDir)) {
        throw new HiveException("The intermediate archive directory already exists.");
      }
      fs.rename(tmpPath,intermediateArchivedDir);
    }
 catch (    IOException e) {
      throw new HiveException("Error while moving tmp directory");
    }
  }
 else {
    if (pathExists(intermediateArchivedDir)) {
      console.printInfo("Intermediate archive directory " + intermediateArchivedDir + " already exists. Assuming it contains an archived version of the partition");
    }
  }
  if (!pathExists(intermediateOriginalDir)) {
    console.printInfo("Moving " + originalDir + " to "+ intermediateOriginalDir);
    moveDir(fs,originalDir,intermediateOriginalDir);
  }
 else {
    console.printInfo(intermediateOriginalDir + " already exists. " + "Assuming it contains the original files in the partition");
  }
  if (!pathExists(originalDir)) {
    console.printInfo("Moving " + intermediateArchivedDir + " to "+ originalDir);
    moveDir(fs,intermediateArchivedDir,originalDir);
  }
 else {
    console.printInfo(originalDir + " already exists. " + "Assuming it contains the archived version of the partition");
  }
  try {
    for (    Partition p : partitions) {
      URI originalPartitionUri=ArchiveUtils.addSlash(p.getDataLocation().toUri());
      URI harPartitionDir=harHelper.getHarUri(originalPartitionUri);
      StringBuilder authority=new StringBuilder();
      if (harPartitionDir.getUserInfo() != null) {
        authority.append(harPartitionDir.getUserInfo()).append("@");
      }
      authority.append(harPartitionDir.getHost());
      if (harPartitionDir.getPort() != -1) {
        authority.append(":").append(harPartitionDir.getPort());
      }
      Path harPath=new Path(harPartitionDir.getScheme(),authority.toString(),harPartitionDir.getPath());
      setArchived(p,harPath,partSpecInfo.values.size());
      db.alterPartition(simpleDesc.getTableName(),p);
    }
  }
 catch (  Exception e) {
    throw new HiveException("Unable to change the partition info for HAR",e);
  }
  if (pathExists(intermediateOriginalDir)) {
    deleteDir(intermediateOriginalDir);
  }
  if (recovery) {
    console.printInfo("Recovery after ARCHIVE succeeded");
  }
  return 0;
}
