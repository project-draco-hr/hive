{
  String dbName=simpleDesc.getDbName();
  String tblName=simpleDesc.getTableName();
  Table tbl=db.getTable(dbName,tblName);
  validateAlterTableType(tbl,AlterTableDesc.AlterTableTypes.ARCHIVE);
  Map<String,String> partSpec=simpleDesc.getPartSpec();
  Partition p=db.getPartition(tbl,partSpec,false);
  if (tbl.getTableType() != TableType.MANAGED_TABLE) {
    throw new HiveException("ARCHIVE can only be performed on managed tables");
  }
  if (p == null) {
    throw new HiveException("Specified partition does not exist");
  }
  if (Utilities.isArchived(p)) {
    Path originalDir=new Path(getOriginalLocation(p));
    Path leftOverIntermediateOriginal=new Path(originalDir.getParent(),originalDir.getName() + INTERMEDIATE_ORIGINAL_DIR_SUFFIX);
    if (pathExists(leftOverIntermediateOriginal)) {
      console.printInfo("Deleting " + leftOverIntermediateOriginal + " left over from a previous archiving operation");
      deleteDir(leftOverIntermediateOriginal);
    }
    throw new HiveException("Specified partition is already archived");
  }
  Path originalDir=p.getPartitionPath();
  Path intermediateArchivedDir=new Path(originalDir.getParent(),originalDir.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);
  Path intermediateOriginalDir=new Path(originalDir.getParent(),originalDir.getName() + INTERMEDIATE_ORIGINAL_DIR_SUFFIX);
  String archiveName="data.har";
  FileSystem fs=null;
  try {
    fs=originalDir.getFileSystem(conf);
  }
 catch (  IOException e) {
    throw new HiveException(e);
  }
  if (!pathExists(intermediateArchivedDir) && !pathExists(intermediateOriginalDir)) {
    Path tmpDir=new Path(driverContext.getCtx().getExternalTmpFileURI(originalDir.toUri()),"partlevel");
    console.printInfo("Creating " + archiveName + " for "+ originalDir.toString());
    console.printInfo("in " + tmpDir);
    console.printInfo("Please wait... (this may take a while)");
    HadoopShims shim=ShimLoader.getHadoopShims();
    int ret=0;
    try {
      int maxJobNameLen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
      String jobname=String.format("Archiving %s@%s",tbl.getTableName(),p.getName());
      jobname=Utilities.abbreviate(jobname,maxJobNameLen - 6);
      conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME,jobname);
      ret=shim.createHadoopArchive(conf,originalDir,tmpDir,archiveName);
    }
 catch (    Exception e) {
      throw new HiveException(e);
    }
    if (ret != 0) {
      throw new HiveException("Error while creating HAR");
    }
    try {
      console.printInfo("Moving " + tmpDir + " to "+ intermediateArchivedDir);
      if (pathExists(intermediateArchivedDir)) {
        throw new HiveException("The intermediate archive directory already exists.");
      }
      fs.rename(tmpDir,intermediateArchivedDir);
    }
 catch (    IOException e) {
      throw new HiveException("Error while moving tmp directory");
    }
  }
 else {
    if (pathExists(intermediateArchivedDir)) {
      console.printInfo("Intermediate archive directory " + intermediateArchivedDir + " already exists. Assuming it contains an archived version of the partition");
    }
  }
  if (!pathExists(intermediateOriginalDir)) {
    console.printInfo("Moving " + originalDir + " to "+ intermediateOriginalDir);
    moveDir(fs,originalDir,intermediateOriginalDir);
  }
 else {
    console.printInfo(intermediateOriginalDir + " already exists. " + "Assuming it contains the original files in the partition");
  }
  if (!pathExists(originalDir)) {
    console.printInfo("Moving " + intermediateArchivedDir + " to "+ originalDir);
    moveDir(fs,intermediateArchivedDir,originalDir);
  }
 else {
    console.printInfo(originalDir + " already exists. " + "Assuming it contains the archived version of the partition");
  }
  try {
    boolean parentSettable=conf.getBoolVar(HiveConf.ConfVars.HIVEHARPARENTDIRSETTABLE);
    String dirInArchive="";
    if (!parentSettable) {
      dirInArchive=originalDir.toUri().getPath();
      if (dirInArchive.length() > 1 && dirInArchive.charAt(0) == '/') {
        dirInArchive=dirInArchive.substring(1);
      }
    }
    setArchived(p,originalDir,dirInArchive,archiveName);
    db.alterPartition(tblName,p);
  }
 catch (  Exception e) {
    throw new HiveException("Unable to change the partition info for HAR",e);
  }
  deleteDir(intermediateOriginalDir);
  return 0;
}
