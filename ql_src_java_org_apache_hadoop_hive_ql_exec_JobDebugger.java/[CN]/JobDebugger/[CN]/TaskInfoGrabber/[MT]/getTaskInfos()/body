{
  int startIndex=0;
  while (true) {
    TaskCompletionEvent[] taskCompletions=rj.getTaskCompletionEvents(startIndex);
    if (taskCompletions == null || taskCompletions.length == 0) {
      break;
    }
    boolean more=true;
    boolean firstError=true;
    for (    TaskCompletionEvent t : taskCompletions) {
      String[] taskJobIds=ShimLoader.getHadoopShims().getTaskJobIDs(t);
      if (taskJobIds == null) {
        console.printError("Task attempt info is unavailable in this Hadoop version");
        more=false;
        break;
      }
      String taskId=taskJobIds[0];
      String jobId=taskJobIds[1];
      if (firstError) {
        console.printError("Examining task ID: " + taskId + " (and more) from job "+ jobId);
        firstError=false;
      }
      TaskInfo ti=taskIdToInfo.get(taskId);
      if (ti == null) {
        ti=new TaskInfo(jobId);
        taskIdToInfo.put(taskId,ti);
      }
      assert(ti.getJobId() != null && ti.getJobId().equals(jobId));
      String taskAttemptLogUrl=ShimLoader.getHadoopShims().getTaskAttemptLogUrl(conf,t.getTaskTrackerHttp(),t.getTaskId());
      if (taskAttemptLogUrl != null) {
        ti.getLogUrls().add(taskAttemptLogUrl);
      }
      if (t.getTaskStatus() != TaskCompletionEvent.Status.SUCCEEDED) {
        String[] diags=rj.getTaskDiagnostics(t.getTaskAttemptId());
        ti.setDiagnosticMesgs(diags);
        if (ti.getErrorCode() == 0) {
          ti.setErrorCode(extractErrorCode(diags));
        }
        Integer failAttempts=failures.get(taskId);
        if (failAttempts == null) {
          failAttempts=Integer.valueOf(0);
        }
        failAttempts=Integer.valueOf(failAttempts.intValue() + 1);
        failures.put(taskId,failAttempts);
      }
 else {
        successes.add(taskId);
      }
    }
    if (!more) {
      break;
    }
    startIndex+=taskCompletions.length;
  }
}
