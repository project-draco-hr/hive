{
  int splitNum=88;
  byte[] planBytes="0123456789987654321".getBytes();
  byte[] fragmentBytes="abcdefghijklmnopqrstuvwxyz".getBytes();
  SplitLocationInfo[] locations={new SplitLocationInfo("location1",false),new SplitLocationInfo("location2",false)};
  ArrayList<FieldSchema> fields=new ArrayList<FieldSchema>();
  fields.add(new FieldSchema("col1","string","comment1"));
  fields.add(new FieldSchema("col2","int","comment2"));
  HashMap<String,String> properties=new HashMap<String,String>();
  properties.put("key1","val1");
  Schema schema=new Schema(fields,properties);
  org.apache.hadoop.hive.llap.LlapInputSplit split1=new org.apache.hadoop.hive.llap.LlapInputSplit(splitNum,planBytes,fragmentBytes,locations,schema,"hive");
  ByteArrayOutputStream byteOutStream=new ByteArrayOutputStream();
  DataOutputStream dataOut=new DataOutputStream(byteOutStream);
  split1.write(dataOut);
  ByteArrayInputStream byteInStream=new ByteArrayInputStream(byteOutStream.toByteArray());
  DataInputStream dataIn=new DataInputStream(byteInStream);
  org.apache.hadoop.hive.llap.LlapInputSplit split2=new org.apache.hadoop.hive.llap.LlapInputSplit();
  split2.readFields(dataIn);
  assertEquals(0,byteInStream.available());
  checkLlapSplits(split1,split2);
  org.apache.hive.jdbc.LlapInputSplit<Text> jdbcSplit1=new org.apache.hive.jdbc.LlapInputSplit<Text>(split1,"org.apache.hadoop.hive.llap.LlapInputFormat");
  byteOutStream.reset();
  jdbcSplit1.write(dataOut);
  byteInStream=new ByteArrayInputStream(byteOutStream.toByteArray());
  dataIn=new DataInputStream(byteInStream);
  org.apache.hive.jdbc.LlapInputSplit<Text> jdbcSplit2=new org.apache.hive.jdbc.LlapInputSplit<Text>();
  jdbcSplit2.readFields(dataIn);
  assertEquals(0,byteInStream.available());
  checkLlapSplits((org.apache.hadoop.hive.llap.LlapInputSplit)jdbcSplit1.getSplit(),(org.apache.hadoop.hive.llap.LlapInputSplit)jdbcSplit2.getSplit());
  assertEquals(jdbcSplit1.getInputFormat().getClass(),jdbcSplit2.getInputFormat().getClass());
}
