{
  String baseDir=null;
  String currentInputFile=context.getCurrentInputFile();
  LOG.info("******* Load from HashTable File: input : " + currentInputFile);
  String fileName=context.getLocalWork().getBucketFileName(currentInputFile);
  try {
    if (ShimLoader.getHadoopShims().isLocalMode(hconf)) {
      baseDir=context.getLocalWork().getTmpFileURI();
    }
 else {
      Path[] localArchives;
      String stageID=context.getLocalWork().getStageID();
      String suffix=Utilities.generateTarFileName(stageID);
      FileSystem localFs=FileSystem.getLocal(hconf);
      localArchives=DistributedCache.getLocalCacheArchives(hconf);
      Path archive;
      for (int j=0; j < localArchives.length; j++) {
        archive=localArchives[j];
        if (!archive.getName().endsWith(suffix)) {
          continue;
        }
        Path archiveLocalLink=archive.makeQualified(localFs);
        baseDir=archiveLocalLink.toUri().getPath();
      }
    }
    for (int pos=0; pos < mapJoinTables.length; pos++) {
      if (pos == posBigTable) {
        continue;
      }
      if (baseDir == null) {
        throw new IllegalStateException("baseDir cannot be null");
      }
      String filePath=Utilities.generatePath(baseDir,desc.getDumpFilePrefix(),(byte)pos,fileName);
      Path path=new Path(filePath);
      LOG.info("\tLoad back 1 hashtable file from tmp file uri:" + path);
      ObjectInputStream in=new ObjectInputStream(new BufferedInputStream(new FileInputStream(path.toUri().getPath()),4096));
      try {
        mapJoinTables[pos]=mapJoinTableSerdes[pos].load(in);
      }
  finally {
        in.close();
      }
    }
  }
 catch (  Exception e) {
    throw new HiveException(e);
  }
}
