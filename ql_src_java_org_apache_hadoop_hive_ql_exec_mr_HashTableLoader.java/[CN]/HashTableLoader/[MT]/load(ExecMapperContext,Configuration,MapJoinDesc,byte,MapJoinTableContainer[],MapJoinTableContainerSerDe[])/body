{
  Path baseDir=null;
  Path currentInputPath=context.getCurrentInputPath();
  LOG.info("******* Load from HashTable File: input : " + currentInputPath);
  String fileName=context.getLocalWork().getBucketFileName(currentInputPath.toString());
  try {
    if (ShimLoader.getHadoopShims().isLocalMode(hconf)) {
      baseDir=context.getLocalWork().getTmpPath();
    }
 else {
      Path[] localArchives;
      String stageID=context.getLocalWork().getStageID();
      String suffix=Utilities.generateTarFileName(stageID);
      FileSystem localFs=FileSystem.getLocal(hconf);
      localArchives=DistributedCache.getLocalCacheArchives(hconf);
      Path archive;
      for (int j=0; j < localArchives.length; j++) {
        archive=localArchives[j];
        if (!archive.getName().endsWith(suffix)) {
          continue;
        }
        baseDir=archive.makeQualified(localFs);
      }
    }
    for (int pos=0; pos < mapJoinTables.length; pos++) {
      if (pos == posBigTable) {
        continue;
      }
      if (baseDir == null) {
        throw new IllegalStateException("baseDir cannot be null");
      }
      Path path=Utilities.generatePath(baseDir,desc.getDumpFilePrefix(),(byte)pos,fileName);
      LOG.info("\tLoad back 1 hashtable file from tmp file uri:" + path);
      ObjectInputStream in=new ObjectInputStream(new BufferedInputStream(new FileInputStream(path.toUri().getPath()),4096));
      try {
        mapJoinTables[pos]=mapJoinTableSerdes[pos].load(in);
      }
  finally {
        in.close();
      }
    }
  }
 catch (  Exception e) {
    throw new HiveException(e);
  }
}
