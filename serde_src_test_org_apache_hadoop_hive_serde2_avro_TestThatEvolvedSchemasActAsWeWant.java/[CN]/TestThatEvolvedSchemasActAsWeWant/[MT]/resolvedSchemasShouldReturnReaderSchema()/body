{
  String v0="{\n" + "    \"namespace\": \"org.apache.hadoop.hive\",\n" + "    \"name\": \"SomeStuff\",\n"+ "    \"type\": \"record\",\n"+ "    \"fields\": [\n"+ "        {\n"+ "            \"name\":\"v0\",\n"+ "            \"type\":\"string\"\n"+ "        }\n"+ "    ]\n"+ "}";
  String v1="{\n" + "    \"namespace\": \"org.apache.hadoop.hive\",\n" + "    \"name\": \"SomeStuff\",\n"+ "    \"type\": \"record\",\n"+ "    \"fields\": [\n"+ "        {\n"+ "            \"name\":\"v0\",\n"+ "            \"type\":\"string\"\n"+ "        },\n"+ "        {\n"+ "            \"name\":\"v1\",\n"+ "            \"type\":\"string\",\n"+ "            \"default\":\"v1_default\""+ "        }\n"+ "    ]\n"+ "}";
  Schema[] schemas={AvroSerdeUtils.getSchemaFor(v0),AvroSerdeUtils.getSchemaFor(v1)};
  GenericRecord record=new GenericData.Record(schemas[0]);
  record.put("v0","v0 value");
  assertTrue(GenericData.get().validate(schemas[0],record));
  GenericDatumWriter<GenericRecord> gdw=new GenericDatumWriter<GenericRecord>(schemas[0]);
  DataFileWriter<GenericRecord> dfw=new DataFileWriter<GenericRecord>(gdw);
  ByteArrayOutputStream baos=new ByteArrayOutputStream();
  dfw.create(schemas[0],baos);
  dfw.append(record);
  dfw.close();
  ByteArrayInputStream bais=new ByteArrayInputStream(baos.toByteArray());
  GenericDatumReader<GenericRecord> gdr=new GenericDatumReader<GenericRecord>();
  gdr.setExpected(schemas[1]);
  DataFileStream<GenericRecord> dfs=new DataFileStream<GenericRecord>(bais,gdr);
  assertTrue(dfs.hasNext());
  GenericRecord next=dfs.next();
  assertEquals("v0 value",next.get("v0").toString());
  assertEquals("v1_default",next.get("v1").toString());
  assertEquals(schemas[1],next.getSchema());
}
