{
  moveWork mv=new moveWork(loadTableWork,loadFileWork);
  Task<? extends Serializable> mvTask=TaskFactory.get(mv,this.conf);
  HashMap<Operator<? extends Serializable>,Task<? extends Serializable>> opTaskMap=new HashMap<Operator<? extends Serializable>,Task<? extends Serializable>>();
  for (  String alias_id : this.topOps.keySet()) {
    Operator<? extends Serializable> topOp=this.topOps.get(alias_id);
    Operator<? extends Serializable> reducer=getReducer(topOp);
    Task<? extends Serializable> rootTask=opTaskMap.get(reducer);
    if (rootTask == null) {
      rootTask=TaskFactory.get(getMapRedWork(),this.conf);
      opTaskMap.put(reducer,rootTask);
      ((mapredWork)rootTask.getWork()).setReducer(reducer);
      this.rootTasks.add(rootTask);
    }
    genTaskPlan(topOp,rootTask,opTaskMap,mvTask);
    PartitionPruner pruner=this.aliasToPruner.get(alias_id);
    Set<Partition> parts=null;
    try {
      parts=pruner.prune();
    }
 catch (    HiveException e) {
      LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
      throw new SemanticException(e.getMessage(),e);
    }
    SamplePruner samplePruner=this.aliasToSamplePruner.get(alias_id);
    mapredWork plan=(mapredWork)rootTask.getWork();
    for (    Partition part : parts) {
      Path paths[];
      if (samplePruner != null) {
        paths=samplePruner.prune(part);
      }
 else {
        paths=part.getPath();
      }
      for (      Path p : paths) {
        String path=p.toString();
        LOG.debug("Adding " + path + " of table"+ alias_id);
        if (plan.getPathToAliases().get(path) == null) {
          plan.getPathToAliases().put(path,new ArrayList<String>());
        }
        plan.getPathToAliases().get(path).add(alias_id);
        plan.getPathToPartitionInfo().put(path,Utilities.getPartitionDesc(part));
        LOG.debug("Information added for path " + path);
      }
    }
    plan.getAliasToWork().put(alias_id,topOp);
    LOG.debug("Created Map Work for " + alias_id);
  }
}
