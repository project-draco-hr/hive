{
  fetchWork fetch=null;
  List<Task<? extends Serializable>> mvTask=new ArrayList<Task<? extends Serializable>>();
  Task<? extends Serializable> fetchTask=null;
  QBParseInfo qbParseInfo=qb.getParseInfo();
  if (qb.getIsQuery()) {
    if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1))     throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
    String cols=loadFileWork.get(0).getColumns();
    String colTypes=loadFileWork.get(0).getColumnTypes();
    fetch=new fetchWork(new Path(loadFileWork.get(0).getSourceDir()).toString(),new tableDesc(LazySimpleSerDe.class,TextInputFormat.class,IgnoreKeyTextOutputFormat.class,Utilities.makeProperties(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT,"" + Utilities.ctrlaCode,org.apache.hadoop.hive.serde.Constants.LIST_COLUMNS,cols,org.apache.hadoop.hive.serde.Constants.LIST_COLUMN_TYPES,colTypes)),qb.getParseInfo().getOuterQueryLimit());
    fetchTask=TaskFactory.get(fetch,this.conf);
    setFetchTask(fetchTask);
  }
 else {
    List<moveWork> mv=new ArrayList<moveWork>();
    for (    loadTableDesc ltd : loadTableWork)     mvTask.add(TaskFactory.get(new moveWork(ltd,null,false),this.conf));
    for (    loadFileDesc lfd : loadFileWork)     mvTask.add(TaskFactory.get(new moveWork(null,lfd,false),this.conf));
  }
  GenMRProcContext procCtx=new GenMRProcContext(conf,new HashMap<Operator<? extends Serializable>,Task<? extends Serializable>>(),new ArrayList<Operator<? extends Serializable>>(),getParseContext(),mvTask,this.rootTasks,new LinkedHashMap<Operator<? extends Serializable>,GenMapRedCtx>(),inputs,outputs);
  Map<Rule,NodeProcessor> opRules=new LinkedHashMap<Rule,NodeProcessor>();
  opRules.put(new RuleRegExp(new String("R1"),"TS%"),new GenMRTableScan1());
  opRules.put(new RuleRegExp(new String("R2"),"TS%.*RS%"),new GenMRRedSink1());
  opRules.put(new RuleRegExp(new String("R3"),"RS%.*RS%"),new GenMRRedSink2());
  opRules.put(new RuleRegExp(new String("R4"),"FS%"),new GenMRFileSink1());
  opRules.put(new RuleRegExp(new String("R5"),"UNION%"),new GenMRUnion1());
  opRules.put(new RuleRegExp(new String("R6"),"UNION%.*RS%"),new GenMRRedSink3());
  opRules.put(new RuleRegExp(new String("R6"),"MAPJOIN%.*RS%"),new GenMRRedSink4());
  opRules.put(new RuleRegExp(new String("R7"),"TS%.*MAPJOIN%"),MapJoinFactory.getTableScanMapJoin());
  opRules.put(new RuleRegExp(new String("R8"),"RS%.*MAPJOIN%"),MapJoinFactory.getReduceSinkMapJoin());
  opRules.put(new RuleRegExp(new String("R9"),"UNION%.*MAPJOIN%"),MapJoinFactory.getUnionMapJoin());
  opRules.put(new RuleRegExp(new String("R10"),"MAPJOIN%.*MAPJOIN%"),MapJoinFactory.getMapJoinMapJoin());
  opRules.put(new RuleRegExp(new String("R11"),"MAPJOIN%SEL%"),MapJoinFactory.getMapJoin());
  Dispatcher disp=new DefaultRuleDispatcher(new GenMROperator(),opRules,procCtx);
  GraphWalker ogw=new GenMapRedWalker(disp);
  ArrayList<Node> topNodes=new ArrayList<Node>();
  topNodes.addAll(this.topOps.values());
  ogw.startWalking(topNodes,null);
  if (qb.isSelectStarQuery() && qbParseInfo.getDestToClusterBy().isEmpty() && qbParseInfo.getDestToDistributeBy().isEmpty()&& qbParseInfo.getDestToOrderBy().isEmpty()&& qbParseInfo.getDestToSortBy().isEmpty()) {
    boolean noMapRed=false;
    Iterator<Map.Entry<String,Table>> iter=qb.getMetaData().getAliasToTable().entrySet().iterator();
    Table tab=((Map.Entry<String,Table>)iter.next()).getValue();
    if (!tab.isPartitioned()) {
      if (qbParseInfo.getDestToWhereExpr().isEmpty()) {
        fetch=new fetchWork(tab.getPath().toString(),Utilities.getTableDesc(tab),qb.getParseInfo().getOuterQueryLimit());
        noMapRed=true;
      }
    }
 else {
      if (topOps.size() == 1) {
        TableScanOperator ts=(TableScanOperator)topOps.values().toArray()[0];
        if (PartitionPruner.onlyContainsPartnCols(topToTable.get(ts),opToPartPruner.get(ts))) {
          PrunedPartitionList partsList=null;
          try {
            partsList=PartitionPruner.prune(topToTable.get(ts),opToPartPruner.get(ts),conf,null);
          }
 catch (          HiveException e) {
            LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
            throw new SemanticException(e.getMessage(),e);
          }
          if (partsList.getUnknownPartns().size() == 0) {
            List<String> listP=new ArrayList<String>();
            List<partitionDesc> partP=new ArrayList<partitionDesc>();
            Set<Partition> parts=partsList.getConfirmedPartns();
            Iterator<Partition> iterParts=parts.iterator();
            while (iterParts.hasNext()) {
              Partition part=iterParts.next();
              listP.add(part.getPartitionPath().toString());
              partP.add(Utilities.getPartitionDesc(part));
            }
            fetch=new fetchWork(listP,partP,qb.getParseInfo().getOuterQueryLimit());
            noMapRed=true;
          }
        }
      }
    }
    if (noMapRed) {
      fetchTask=TaskFactory.get(fetch,this.conf);
      setFetchTask(fetchTask);
      rootTasks.clear();
      return;
    }
  }
  for (  Task<? extends Serializable> rootTask : rootTasks)   breakTaskTree(rootTask);
  for (  Task<? extends Serializable> rootTask : rootTasks)   setKeyDescTaskTree(rootTask);
  if (HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVEJOBPROGRESS))   for (  Task<? extends Serializable> rootTask : rootTasks)   generateCountersTask(rootTask);
}
