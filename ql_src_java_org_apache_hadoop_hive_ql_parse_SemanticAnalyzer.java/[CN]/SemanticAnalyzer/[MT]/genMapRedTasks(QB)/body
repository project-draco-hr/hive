{
  fetchWork fetch=null;
  moveWork mv=null;
  Task<? extends Serializable> mvTask=null;
  Task<? extends Serializable> fetchTask=null;
  if (qb.getParseInfo().getCanOptTopQ()) {
    Iterator<Map.Entry<String,Table>> iter=qb.getMetaData().getAliasToTable().entrySet().iterator();
    Table tab=((Map.Entry<String,Table>)iter.next()).getValue();
    fetch=new fetchWork(tab.getPath(),tab.getDeserializer().getClass(),tab.getInputFormatClass(),tab.getSchema(),qb.getParseInfo().getOuterQueryLimit());
    fetchTask=TaskFactory.get(fetch,this.conf);
    setFetchTask(fetchTask);
    return;
  }
  if (qb.getIsQuery()) {
    if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1))     throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
    String cols=loadFileWork.get(0).getColumns();
    fetch=new fetchWork(new Path(loadFileWork.get(0).getSourceDir()),MetadataTypedColumnsetSerDe.class,TextInputFormat.class,Utilities.makeProperties("columns",cols),qb.getParseInfo().getOuterQueryLimit());
    fetchTask=TaskFactory.get(fetch,this.conf);
    setFetchTask(fetchTask);
  }
 else {
    mv=new moveWork(loadTableWork,loadFileWork);
    mvTask=TaskFactory.get(mv,this.conf);
  }
  HashMap<Operator<? extends Serializable>,Task<? extends Serializable>> opTaskMap=new HashMap<Operator<? extends Serializable>,Task<? extends Serializable>>();
  for (  String alias_id : this.topOps.keySet()) {
    Operator<? extends Serializable> topOp=this.topOps.get(alias_id);
    Operator<? extends Serializable> reduceSink=getReduceSink(topOp);
    Operator<? extends Serializable> reducer=null;
    if (reduceSink != null)     reducer=reduceSink.getChildOperators().get(0);
    Task<? extends Serializable> rootTask=opTaskMap.get(reducer);
    if (rootTask == null) {
      rootTask=TaskFactory.get(getMapRedWork(),this.conf);
      opTaskMap.put(reducer,rootTask);
      ((mapredWork)rootTask.getWork()).setReducer(reducer);
      reduceSinkDesc desc=(reduceSink == null) ? null : (reduceSinkDesc)reduceSink.getConf();
      if (desc != null) {
        if (desc.getNumReducers() != -1)         ((mapredWork)rootTask.getWork()).setNumReduceTasks(new Integer(desc.getNumReducers()));
 else         if (desc.getInferNumReducers() == true)         ((mapredWork)rootTask.getWork()).setInferNumReducers(true);
      }
      this.rootTasks.add(rootTask);
    }
    genTaskPlan(topOp,rootTask,opTaskMap,mvTask);
    PartitionPruner pruner=this.aliasToPruner.get(alias_id);
    Set<Partition> parts=null;
    try {
      parts=pruner.prune();
    }
 catch (    HiveException e) {
      LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
      throw new SemanticException(e.getMessage(),e);
    }
    SamplePruner samplePruner=this.aliasToSamplePruner.get(alias_id);
    mapredWork plan=(mapredWork)rootTask.getWork();
    for (    Partition part : parts) {
      Path paths[];
      if (samplePruner != null) {
        paths=samplePruner.prune(part);
      }
 else {
        paths=part.getPath();
      }
      for (      Path p : paths) {
        String path=p.toString();
        LOG.debug("Adding " + path + " of table"+ alias_id);
        if (plan.getPathToAliases().get(path) == null) {
          plan.getPathToAliases().put(path,new ArrayList<String>());
        }
        plan.getPathToAliases().get(path).add(alias_id);
        plan.getPathToPartitionInfo().put(path,Utilities.getPartitionDesc(part));
        LOG.debug("Information added for path " + path);
      }
    }
    plan.getAliasToWork().put(alias_id,topOp);
    LOG.debug("Created Map Work for " + alias_id);
  }
}
