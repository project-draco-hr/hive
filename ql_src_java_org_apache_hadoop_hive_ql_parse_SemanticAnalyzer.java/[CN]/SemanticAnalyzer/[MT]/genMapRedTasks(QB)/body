{
  fetchWork fetch=null;
  moveWork mv=null;
  Task<? extends Serializable> mvTask=null;
  Task<? extends Serializable> fetchTask=null;
  QBParseInfo qbParseInfo=qb.getParseInfo();
  if (qb.isSelectStarQuery() && qbParseInfo.getDestToClusterBy().isEmpty() && qbParseInfo.getDestToDistributeBy().isEmpty()&& qbParseInfo.getDestToSortBy().isEmpty()) {
    Iterator<Map.Entry<String,Table>> iter=qb.getMetaData().getAliasToTable().entrySet().iterator();
    Table tab=((Map.Entry<String,Table>)iter.next()).getValue();
    if (!tab.isPartitioned()) {
      if (qbParseInfo.getDestToWhereExpr().isEmpty())       fetch=new fetchWork(tab.getPath(),Utilities.getTableDesc(tab),qb.getParseInfo().getOuterQueryLimit());
    }
 else {
      if (aliasToPruner.size() == 1) {
        Iterator<Map.Entry<String,PartitionPruner>> iterP=aliasToPruner.entrySet().iterator();
        PartitionPruner pr=((Map.Entry<String,PartitionPruner>)iterP.next()).getValue();
        if (pr.containsPartitionCols()) {
          List<Path> listP=new ArrayList<Path>();
          List<partitionDesc> partP=new ArrayList<partitionDesc>();
          PartitionPruner.PrunedPartitionList partsList=null;
          Set<Partition> parts=null;
          try {
            partsList=pr.prune();
            if (partsList.getUnknownPartns().size() == 0) {
              parts=partsList.getConfirmedPartns();
              Iterator<Partition> iterParts=parts.iterator();
              while (iterParts.hasNext()) {
                Partition part=iterParts.next();
                listP.add(part.getPartitionPath());
                partP.add(Utilities.getPartitionDesc(part));
              }
              fetch=new fetchWork(listP,partP,qb.getParseInfo().getOuterQueryLimit());
            }
          }
 catch (          HiveException e) {
            LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
            throw new SemanticException(e.getMessage(),e);
          }
        }
      }
    }
    if (fetch != null) {
      fetchTask=TaskFactory.get(fetch,this.conf);
      setFetchTask(fetchTask);
      return;
    }
  }
  if (qb.getIsQuery()) {
    if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1))     throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
    String cols=loadFileWork.get(0).getColumns();
    fetch=new fetchWork(new Path(loadFileWork.get(0).getSourceDir()),new tableDesc(LazySimpleSerDe.class,TextInputFormat.class,IgnoreKeyTextOutputFormat.class,Utilities.makeProperties(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT,"" + Utilities.ctrlaCode,"columns",cols)),qb.getParseInfo().getOuterQueryLimit());
    fetchTask=TaskFactory.get(fetch,this.conf);
    setFetchTask(fetchTask);
  }
 else {
    mv=new moveWork(loadTableWork,loadFileWork);
    mvTask=TaskFactory.get(mv,this.conf);
  }
  GenMRProcContext procCtx=new GenMRProcContext(new HashMap<Operator<? extends Serializable>,Task<? extends Serializable>>(),new ArrayList<Operator<? extends Serializable>>(),getParseContext(),mvTask,this.rootTasks,this.scratchDir,this.randomid,this.pathid,new HashMap<Operator<? extends Serializable>,GenMapRedCtx>());
  Map<Rule,NodeProcessor> opRules=new LinkedHashMap<Rule,NodeProcessor>();
  opRules.put(new RuleRegExp(new String("R1"),"TS%"),new GenMRTableScan1());
  opRules.put(new RuleRegExp(new String("R2"),"TS%.*RS%"),new GenMRRedSink1());
  opRules.put(new RuleRegExp(new String("R3"),"RS%.*RS%"),new GenMRRedSink2());
  opRules.put(new RuleRegExp(new String("R4"),"FS%"),new GenMRFileSink1());
  Dispatcher disp=new DefaultRuleDispatcher(new GenMROperator(),opRules,procCtx);
  GraphWalker ogw=new GenMapRedWalker(disp);
  ArrayList<Node> topNodes=new ArrayList<Node>();
  topNodes.addAll(this.topOps.values());
  ogw.startWalking(topNodes,null);
  breakOperatorTree(procCtx.getRootOps());
}
