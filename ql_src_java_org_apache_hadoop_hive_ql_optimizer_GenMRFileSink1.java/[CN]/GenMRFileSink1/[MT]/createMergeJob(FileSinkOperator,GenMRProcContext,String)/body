{
  Task<? extends Serializable> currTask=ctx.getCurrTask();
  RowSchema fsRS=fsOp.getSchema();
  ArrayList<ExprNodeDesc> keyCols=new ArrayList<ExprNodeDesc>();
  keyCols.add(TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("rand"));
  ArrayList<ExprNodeDesc> valueCols=new ArrayList<ExprNodeDesc>();
  for (  ColumnInfo ci : fsRS.getSignature()) {
    valueCols.add(new ExprNodeColumnDesc(ci.getType(),ci.getInternalName(),ci.getTabAlias(),ci.getIsPartitionCol()));
  }
  Operator<? extends Serializable> ts_op=OperatorFactory.get(TableScanDesc.class,fsRS);
  ArrayList<String> outputColumns=new ArrayList<String>();
  for (int i=0; i < valueCols.size(); i++) {
    outputColumns.add(SemanticAnalyzer.getColumnInternalName(i));
  }
  ReduceSinkDesc rsDesc=PlanUtils.getReduceSinkDesc(new ArrayList<ExprNodeDesc>(),valueCols,outputColumns,false,-1,-1,-1);
  OperatorFactory.getAndMakeChild(rsDesc,fsRS,ts_op);
  ParseContext parseCtx=ctx.getParseCtx();
  MapredWork cplan=GenMapRedUtils.getMapRedWork(parseCtx.getConf());
  Task<? extends Serializable> mergeTask=TaskFactory.get(cplan,parseCtx.getConf());
  FileSinkDesc fsConf=fsOp.getConf();
  RowResolver out_rwsch=new RowResolver();
  RowResolver interim_rwsch=ctx.getParseCtx().getOpParseCtx().get(fsOp).getRR();
  Integer pos=Integer.valueOf(0);
  for (  ColumnInfo colInfo : interim_rwsch.getColumnInfos()) {
    String[] info=interim_rwsch.reverseLookup(colInfo.getInternalName());
    out_rwsch.put(info[0],info[1],new ColumnInfo(pos.toString(),colInfo.getType(),info[0],colInfo.getIsPartitionCol()));
    pos=Integer.valueOf(pos.intValue() + 1);
  }
  Operator extract=OperatorFactory.getAndMakeChild(new ExtractDesc(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo,Utilities.ReduceField.VALUE.toString(),"",false)),new RowSchema(out_rwsch.getColumnInfos()));
  TableDesc ts=(TableDesc)fsConf.getTableInfo().clone();
  fsConf.getTableInfo().getProperties().remove(org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_PARTITION_COLUMNS);
  FileSinkOperator newOutput=(FileSinkOperator)OperatorFactory.getAndMakeChild(new FileSinkDesc(finalName,ts,parseCtx.getConf().getBoolVar(HiveConf.ConfVars.COMPRESSRESULT)),fsRS,extract);
  cplan.setReducer(extract);
  ArrayList<String> aliases=new ArrayList<String>();
  aliases.add(fsConf.getDirName());
  cplan.getPathToAliases().put(fsConf.getDirName(),aliases);
  cplan.getAliasToWork().put(fsConf.getDirName(),ts_op);
  cplan.getPathToPartitionInfo().put(fsConf.getDirName(),new PartitionDesc(fsConf.getTableInfo(),null));
  cplan.setNumReduceTasks(-1);
  MoveWork dummyMv=new MoveWork(null,null,null,new LoadFileDesc(fsOp.getConf().getDirName(),finalName,true,null,null),false);
  Task<? extends Serializable> dummyMergeTask=TaskFactory.get(dummyMv,ctx.getConf());
  List<Serializable> listWorks=new ArrayList<Serializable>();
  listWorks.add(dummyMv);
  listWorks.add(mergeTask.getWork());
  ConditionalWork cndWork=new ConditionalWork(listWorks);
  ConditionalTask cndTsk=(ConditionalTask)TaskFactory.get(cndWork,ctx.getConf());
  List<Task<? extends Serializable>> listTasks=new ArrayList<Task<? extends Serializable>>();
  listTasks.add(dummyMergeTask);
  listTasks.add(mergeTask);
  cndTsk.setListTasks(listTasks);
  cndTsk.setResolver(new ConditionalResolverMergeFiles());
  cndTsk.setResolverCtx(new ConditionalResolverMergeFilesCtx(listTasks,fsOp.getConf().getDirName()));
  currTask.addDependentTask(cndTsk);
  List<Task<? extends Serializable>> mvTasks=ctx.getMvTask();
  Task<? extends Serializable> mvTask=findMoveTask(mvTasks,newOutput);
  if (mvTask != null) {
    for (    Task<? extends Serializable> tsk : cndTsk.getListTasks()) {
      tsk.addDependentTask(mvTask);
    }
  }
}
