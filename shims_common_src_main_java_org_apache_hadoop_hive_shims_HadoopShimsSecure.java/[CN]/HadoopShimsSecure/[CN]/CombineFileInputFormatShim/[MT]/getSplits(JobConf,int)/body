{
  long minSize=job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZE"),0);
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZEPERNODE"),0) == 0) {
    super.setMinSplitSizeNode(minSize);
  }
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZEPERRACK"),0) == 0) {
    super.setMinSplitSizeRack(minSize);
  }
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMAXSPLITSIZE"),0) == 0) {
    super.setMaxSplitSize(minSize);
  }
  InputSplit[] splits=super.getSplits(job,numSplits);
  ArrayList<InputSplitShim> inputSplitShims=new ArrayList<InputSplitShim>();
  for (int pos=0; pos < splits.length; pos++) {
    CombineFileSplit split=(CombineFileSplit)splits[pos];
    if (split.getPaths().length > 0) {
      inputSplitShims.add(new InputSplitShim(job,split.getPaths(),split.getStartOffsets(),split.getLengths(),split.getLocations()));
    }
  }
  return inputSplitShims.toArray(new InputSplitShim[inputSplitShims.size()]);
}
