{
  long minSize=job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE,0);
  if (job.getLong(org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.SPLIT_MINSIZE_PERNODE,0) == 0) {
    super.setMinSplitSizeNode(minSize);
  }
  if (job.getLong(org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.SPLIT_MINSIZE_PERRACK,0) == 0) {
    super.setMinSplitSizeRack(minSize);
  }
  if (job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MAXSIZE,0) == 0) {
    super.setMaxSplitSize(minSize);
  }
  InputSplit[] splits=super.getSplits(job,numSplits);
  ArrayList<InputSplitShim> inputSplitShims=new ArrayList<InputSplitShim>();
  for (int pos=0; pos < splits.length; pos++) {
    CombineFileSplit split=(CombineFileSplit)splits[pos];
    if (split.getPaths().length > 0) {
      inputSplitShims.add(new InputSplitShim(job,split.getPaths(),split.getStartOffsets(),split.getLengths(),split.getLocations()));
    }
  }
  return inputSplitShims.toArray(new InputSplitShim[inputSplitShims.size()]);
}
