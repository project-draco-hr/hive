{
  super.setUp();
  try {
    HiveConf hiveConf=new HiveConf(HiveMetaTool.class);
    client=new HiveMetaStoreClient(hiveConf);
    os=new ByteArrayOutputStream();
    ps=new PrintStream(os);
    Database db=new Database();
    db.setName(dbName);
    client.dropTable(dbName,tblName);
    client.dropTable(dbName,badTblName);
    dropDatabase(dbName);
    client.createDatabase(db);
    locationUri=db.getLocationUri();
    String avroUri="hdfs://nn.example.com/warehouse/hive/ab.avsc";
    String badAvroUri=new String("hdfs:/hive");
    client.dropType(typeName);
    Type typ1=new Type();
    typ1.setName(typeName);
    typ1.setFields(new ArrayList<FieldSchema>(2));
    typ1.getFields().add(new FieldSchema("name",serdeConstants.STRING_TYPE_NAME,""));
    typ1.getFields().add(new FieldSchema("income",serdeConstants.INT_TYPE_NAME,""));
    client.createType(typ1);
    Table tbl=new Table();
    tbl.setDbName(dbName);
    tbl.setTableName(tblName);
    Map<String,String> parameters=new HashMap<>();
    parameters.put(AvroSerdeUtils.SCHEMA_URL,avroUri);
    tbl.setParameters(parameters);
    StorageDescriptor sd=new StorageDescriptor();
    tbl.setSd(sd);
    sd.setCols(typ1.getFields());
    sd.setCompressed(false);
    sd.setNumBuckets(1);
    sd.setParameters(new HashMap<String,String>());
    sd.getParameters().put("test_param_1","Use this for comments etc");
    sd.setBucketCols(new ArrayList<String>(2));
    sd.getBucketCols().add("name");
    sd.setSerdeInfo(new SerDeInfo());
    sd.getSerdeInfo().setName(tbl.getTableName());
    sd.getSerdeInfo().setParameters(new HashMap<String,String>());
    sd.getSerdeInfo().getParameters().put(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT,"1");
    sd.getParameters().put(AvroSerdeUtils.SCHEMA_URL,avroUri);
    sd.getSerdeInfo().setSerializationLib(org.apache.hadoop.hive.serde2.avro.AvroSerDe.class.getName());
    sd.setInputFormat(AvroContainerInputFormat.class.getName());
    sd.setOutputFormat(AvroContainerOutputFormat.class.getName());
    tbl.setPartitionKeys(new ArrayList<FieldSchema>());
    client.createTable(tbl);
    tbl=new Table();
    tbl.setDbName(dbName);
    tbl.setTableName(badTblName);
    sd=new StorageDescriptor();
    tbl.setSd(sd);
    sd.setCols(typ1.getFields());
    sd.setCompressed(false);
    sd.setNumBuckets(1);
    sd.setParameters(new HashMap<String,String>());
    sd.getParameters().put("test_param_1","Use this for comments etc");
    sd.setBucketCols(new ArrayList<String>(2));
    sd.getBucketCols().add("name");
    sd.setSerdeInfo(new SerDeInfo());
    sd.getSerdeInfo().setName(tbl.getTableName());
    sd.getSerdeInfo().setParameters(new HashMap<String,String>());
    sd.getSerdeInfo().getParameters().put(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT,"1");
    sd.getParameters().put(AvroSerdeUtils.SCHEMA_URL,badAvroUri);
    sd.getSerdeInfo().setSerializationLib(org.apache.hadoop.hive.serde2.avro.AvroSerDe.class.getName());
    sd.setInputFormat(AvroContainerInputFormat.class.getName());
    sd.setOutputFormat(AvroContainerOutputFormat.class.getName());
    tbl.setPartitionKeys(new ArrayList<FieldSchema>());
    client.createTable(tbl);
    client.close();
  }
 catch (  Exception e) {
    System.err.println("Unable to setup the hive metatool test");
    System.err.println(StringUtils.stringifyException(e));
    throw new Exception(e);
  }
}
