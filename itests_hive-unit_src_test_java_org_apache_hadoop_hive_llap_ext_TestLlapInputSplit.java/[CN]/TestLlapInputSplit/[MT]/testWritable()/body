{
  int splitNum=88;
  byte[] planBytes="0123456789987654321".getBytes();
  byte[] fragmentBytes="abcdefghijklmnopqrstuvwxyz".getBytes();
  SplitLocationInfo[] locations={new SplitLocationInfo("location1",false),new SplitLocationInfo("location2",false)};
  ArrayList<FieldDesc> colDescs=new ArrayList<FieldDesc>();
  colDescs.add(new FieldDesc("col1",new TypeDesc(TypeDesc.Type.STRING)));
  colDescs.add(new FieldDesc("col2",new TypeDesc(TypeDesc.Type.INT)));
  Schema schema=new Schema(colDescs);
  org.apache.hadoop.hive.llap.LlapInputSplit split1=new org.apache.hadoop.hive.llap.LlapInputSplit(splitNum,planBytes,fragmentBytes,locations,schema,"hive");
  ByteArrayOutputStream byteOutStream=new ByteArrayOutputStream();
  DataOutputStream dataOut=new DataOutputStream(byteOutStream);
  split1.write(dataOut);
  ByteArrayInputStream byteInStream=new ByteArrayInputStream(byteOutStream.toByteArray());
  DataInputStream dataIn=new DataInputStream(byteInStream);
  org.apache.hadoop.hive.llap.LlapInputSplit split2=new org.apache.hadoop.hive.llap.LlapInputSplit();
  split2.readFields(dataIn);
  assertEquals(0,byteInStream.available());
  checkLlapSplits(split1,split2);
  org.apache.hive.llap.ext.LlapInputSplit<Text> jdbcSplit1=new org.apache.hive.llap.ext.LlapInputSplit<Text>(split1,"org.apache.hadoop.hive.llap.LlapInputFormat");
  byteOutStream.reset();
  jdbcSplit1.write(dataOut);
  byteInStream=new ByteArrayInputStream(byteOutStream.toByteArray());
  dataIn=new DataInputStream(byteInStream);
  org.apache.hive.llap.ext.LlapInputSplit<Text> jdbcSplit2=new org.apache.hive.llap.ext.LlapInputSplit<Text>();
  jdbcSplit2.readFields(dataIn);
  assertEquals(0,byteInStream.available());
  checkLlapSplits((org.apache.hadoop.hive.llap.LlapInputSplit)jdbcSplit1.getSplit(),(org.apache.hadoop.hive.llap.LlapInputSplit)jdbcSplit2.getSplit());
  assertEquals(jdbcSplit1.getInputFormat().getClass(),jdbcSplit2.getInputFormat().getClass());
}
