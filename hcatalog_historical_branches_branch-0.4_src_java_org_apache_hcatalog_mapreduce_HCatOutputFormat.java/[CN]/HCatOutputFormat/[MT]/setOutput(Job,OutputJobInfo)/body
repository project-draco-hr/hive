{
  HiveMetaStoreClient client=null;
  try {
    Configuration conf=job.getConfiguration();
    HiveConf hiveConf=HCatUtil.getHiveConf(conf);
    client=HCatUtil.getHiveClient(hiveConf);
    Table table=client.getTable(outputJobInfo.getDatabaseName(),outputJobInfo.getTableName());
    List<String> indexList=client.listIndexNames(outputJobInfo.getDatabaseName(),outputJobInfo.getTableName(),Short.MAX_VALUE);
    for (    String indexName : indexList) {
      Index index=client.getIndex(outputJobInfo.getDatabaseName(),outputJobInfo.getTableName(),indexName);
      if (!index.isDeferredRebuild()) {
        throw new HCatException(ErrorType.ERROR_NOT_SUPPORTED,"Store into a table with an automatic index from Pig/Mapreduce is not supported");
      }
    }
    StorageDescriptor sd=table.getSd();
    if (sd.isCompressed()) {
      throw new HCatException(ErrorType.ERROR_NOT_SUPPORTED,"Store into a compressed partition from Pig/Mapreduce is not supported");
    }
    if (sd.getBucketCols() != null && !sd.getBucketCols().isEmpty()) {
      throw new HCatException(ErrorType.ERROR_NOT_SUPPORTED,"Store into a partition with bucket definition from Pig/Mapreduce is not supported");
    }
    if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {
      throw new HCatException(ErrorType.ERROR_NOT_SUPPORTED,"Store into a partition with sorted column definition from Pig/Mapreduce is not supported");
    }
    if (table.getPartitionKeysSize() == 0) {
      if ((outputJobInfo.getPartitionValues() != null) && (!outputJobInfo.getPartitionValues().isEmpty())) {
        throw new HCatException(ErrorType.ERROR_INVALID_PARTITION_VALUES,"Partition values specified for non-partitioned table");
      }
      outputJobInfo.setPartitionValues(new HashMap<String,String>());
    }
 else {
      Map<String,String> valueMap=new HashMap<String,String>();
      if (outputJobInfo.getPartitionValues() != null) {
        for (        Map.Entry<String,String> entry : outputJobInfo.getPartitionValues().entrySet()) {
          valueMap.put(entry.getKey().toLowerCase(),entry.getValue());
        }
      }
      if ((outputJobInfo.getPartitionValues() == null) || (outputJobInfo.getPartitionValues().size() < table.getPartitionKeysSize())) {
        List<String> dynamicPartitioningKeys=new ArrayList<String>();
        boolean firstItem=true;
        for (        FieldSchema fs : table.getPartitionKeys()) {
          if (!valueMap.containsKey(fs.getName().toLowerCase())) {
            dynamicPartitioningKeys.add(fs.getName().toLowerCase());
          }
        }
        if (valueMap.size() + dynamicPartitioningKeys.size() != table.getPartitionKeysSize()) {
          throw new HCatException(ErrorType.ERROR_INVALID_PARTITION_VALUES,"Invalid partition keys specified");
        }
        outputJobInfo.setDynamicPartitioningKeys(dynamicPartitioningKeys);
        String dynHash;
        if ((dynHash=conf.get(HCatConstants.HCAT_DYNAMIC_PTN_JOBID)) == null) {
          dynHash=String.valueOf(Math.random());
        }
        conf.set(HCatConstants.HCAT_DYNAMIC_PTN_JOBID,dynHash);
      }
      outputJobInfo.setPartitionValues(valueMap);
    }
    StorageDescriptor tblSD=table.getSd();
    HCatSchema tableSchema=HCatUtil.extractSchemaFromStorageDescriptor(tblSD);
    StorerInfo storerInfo=InternalUtil.extractStorerInfo(tblSD,table.getParameters());
    List<String> partitionCols=new ArrayList<String>();
    for (    FieldSchema schema : table.getPartitionKeys()) {
      partitionCols.add(schema.getName());
    }
    HCatStorageHandler storageHandler=HCatUtil.getStorageHandler(job.getConfiguration(),storerInfo);
    outputJobInfo.setTableInfo(HCatTableInfo.valueOf(table));
    outputJobInfo.setOutputSchema(tableSchema);
    harRequested=getHarRequested(hiveConf);
    outputJobInfo.setHarRequested(harRequested);
    maxDynamicPartitions=getMaxDynamicPartitions(hiveConf);
    outputJobInfo.setMaximumDynamicPartitions(maxDynamicPartitions);
    HCatUtil.configureOutputStorageHandler(storageHandler,job,outputJobInfo);
    Path tblPath=new Path(table.getSd().getLocation());
    FsPermission.setUMask(conf,FsPermission.getDefault().applyUMask(tblPath.getFileSystem(conf).getFileStatus(tblPath).getPermission()));
    if (Security.getInstance().isSecurityEnabled()) {
      Security.getInstance().handleSecurity(job,outputJobInfo,client,conf,harRequested);
    }
  }
 catch (  Exception e) {
    if (e instanceof HCatException) {
      throw (HCatException)e;
    }
 else {
      throw new HCatException(ErrorType.ERROR_SET_OUTPUT,e);
    }
  }
 finally {
    HCatUtil.closeHiveClientQuietly(client);
  }
}
