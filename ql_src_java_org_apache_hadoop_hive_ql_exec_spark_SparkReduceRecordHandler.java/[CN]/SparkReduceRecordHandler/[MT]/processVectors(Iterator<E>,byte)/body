{
  VectorizedRowBatch batch=batches[tag];
  batch.reset();
  VectorizedBatchUtil.addRowToBatchFrom(keyObject,keyStructInspector,0,0,batch,buffer);
  for (int i=0; i < keysColumnOffset; i++) {
    VectorizedBatchUtil.setRepeatingColumn(batch,i);
  }
  int rowIdx=0;
  try {
    while (values.hasNext()) {
      BytesWritable valueWritable=(BytesWritable)values.next();
      Object valueObj=deserializeValue(valueWritable,tag);
      VectorizedBatchUtil.addRowToBatchFrom(valueObj,valueStructInspectors[tag],rowIdx,keysColumnOffset,batch,buffer);
      rowIdx++;
      if (rowIdx >= BATCH_SIZE) {
        VectorizedBatchUtil.setBatchSize(batch,rowIdx);
        reducer.processOp(batch,tag);
        rowIdx=0;
        if (isLogInfoEnabled) {
          logMemoryInfo();
        }
      }
    }
    if (rowIdx > 0) {
      VectorizedBatchUtil.setBatchSize(batch,rowIdx);
      reducer.processOp(batch,tag);
    }
    if (isLogInfoEnabled) {
      logMemoryInfo();
    }
  }
 catch (  Exception e) {
    String rowString=null;
    try {
      batch.setValueWriters(valueStringWriters[tag].toArray(new VectorExpressionWriter[0]));
      rowString=batch.toString();
    }
 catch (    Exception e2) {
      rowString="[Error getting row data with exception " + StringUtils.stringifyException(e2) + " ]";
    }
    throw new HiveException("Error while processing vector batch (tag=" + tag + ") "+ rowString,e);
  }
  return true;
}
