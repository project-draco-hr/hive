{
  HiveMetaStoreClient client=null;
  try {
    Configuration conf=job.getConfiguration();
    client=createHiveClient(outputJobInfo.getServerUri(),conf);
    Table table=client.getTable(outputJobInfo.getDatabaseName(),outputJobInfo.getTableName());
    if (table.getPartitionKeysSize() == 0) {
      if ((outputJobInfo.getPartitionValues() != null) && (!outputJobInfo.getPartitionValues().isEmpty())) {
        throw new HCatException(ErrorType.ERROR_INVALID_PARTITION_VALUES,"Partition values specified for non-partitioned table");
      }
      outputJobInfo.setPartitionValues(new HashMap<String,String>());
    }
 else {
      Map<String,String> valueMap=new HashMap<String,String>();
      if (outputJobInfo.getPartitionValues() != null) {
        for (        Map.Entry<String,String> entry : outputJobInfo.getPartitionValues().entrySet()) {
          valueMap.put(entry.getKey().toLowerCase(),entry.getValue());
        }
      }
      if ((outputJobInfo.getPartitionValues() == null) || (outputJobInfo.getPartitionValues().size() < table.getPartitionKeysSize())) {
        List<String> dynamicPartitioningKeys=new ArrayList<String>();
        boolean firstItem=true;
        for (        FieldSchema fs : table.getPartitionKeys()) {
          if (!valueMap.containsKey(fs.getName().toLowerCase())) {
            dynamicPartitioningKeys.add(fs.getName().toLowerCase());
          }
        }
        if (valueMap.size() + dynamicPartitioningKeys.size() != table.getPartitionKeysSize()) {
          throw new HCatException(ErrorType.ERROR_INVALID_PARTITION_VALUES,"Invalid partition keys specified");
        }
        outputJobInfo.setDynamicPartitioningKeys(dynamicPartitioningKeys);
        String dynHash;
        if ((dynHash=conf.get(HCatConstants.HCAT_DYNAMIC_PTN_JOBID)) == null) {
          dynHash=String.valueOf(Math.random());
        }
        conf.set(HCatConstants.HCAT_DYNAMIC_PTN_JOBID,dynHash);
      }
      outputJobInfo.setPartitionValues(valueMap);
    }
    StorageDescriptor tblSD=table.getSd();
    HCatSchema tableSchema=HCatUtil.extractSchemaFromStorageDescriptor(tblSD);
    StorerInfo storerInfo=InitializeInput.extractStorerInfo(tblSD,table.getParameters());
    List<String> partitionCols=new ArrayList<String>();
    for (    FieldSchema schema : table.getPartitionKeys()) {
      partitionCols.add(schema.getName());
    }
    Class<? extends HCatOutputStorageDriver> driverClass=(Class<? extends HCatOutputStorageDriver>)Class.forName(storerInfo.getOutputSDClass());
    HCatOutputStorageDriver driver=driverClass.newInstance();
    String tblLocation=tblSD.getLocation();
    String location=driver.getOutputLocation(job,tblLocation,partitionCols,outputJobInfo.getPartitionValues(),conf.get(HCatConstants.HCAT_DYNAMIC_PTN_JOBID));
    outputJobInfo.setTableInfo(HCatTableInfo.valueOf(table));
    outputJobInfo.setOutputSchema(tableSchema);
    outputJobInfo.setLocation(location);
    outputJobInfo.setHarRequested(harRequested);
    outputJobInfo.setMaximumDynamicPartitions(maxDynamicPartitions);
    conf.set(HCatConstants.HCAT_KEY_OUTPUT_INFO,HCatUtil.serialize(outputJobInfo));
    Path tblPath=new Path(tblLocation);
    FsPermission.setUMask(conf,FsPermission.getDefault().applyUMask(tblPath.getFileSystem(conf).getFileStatus(tblPath).getPermission()));
    try {
      UserGroupInformation.class.getMethod("isSecurityEnabled");
      Security.getInstance().handleSecurity(job,outputJobInfo,client,conf,harRequested);
    }
 catch (    NoSuchMethodException e) {
      LOG.info("Security is not supported by this version of hadoop.");
    }
  }
 catch (  Exception e) {
    if (e instanceof HCatException) {
      throw (HCatException)e;
    }
 else {
      throw new HCatException(ErrorType.ERROR_SET_OUTPUT,e);
    }
  }
 finally {
    if (client != null) {
      client.close();
    }
  }
}
