{
  this.splitLen=oldSplit.getLength();
  this.projectionPusher=pusher;
  jobConf=oldJobConf;
  final ParquetInputSplit split=getSplit(oldSplit,jobConf);
  TaskAttemptID taskAttemptID=TaskAttemptID.forName(jobConf.get(IOConstants.MAPRED_TASK_ID));
  if (taskAttemptID == null) {
    taskAttemptID=new TaskAttemptID();
  }
  setFilter(jobConf);
  Configuration conf=jobConf;
  if (skipTimestampConversion ^ HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)) {
    conf=new JobConf(oldJobConf);
    HiveConf.setBoolVar(conf,HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION,skipTimestampConversion);
  }
  final TaskAttemptContext taskContext=ContextUtil.newTaskAttemptContext(conf,taskAttemptID);
  if (split != null) {
    try {
      realReader=newInputFormat.createRecordReader(split,taskContext);
      realReader.initialize(split,taskContext);
      if (realReader.nextKeyValue()) {
        firstRecord=true;
        valueObj=realReader.getCurrentValue();
      }
 else {
        eof=true;
      }
    }
 catch (    final InterruptedException e) {
      throw new IOException(e);
    }
  }
 else {
    realReader=null;
    eof=true;
  }
  if (valueObj == null) {
    valueObj=new ArrayWritable(Writable.class,new Writable[schemaSize]);
  }
}
