{
  OptimizeSparkProcContext context=(OptimizeSparkProcContext)procContext;
  ReduceSinkOperator sink=(ReduceSinkOperator)nd;
  ReduceSinkDesc desc=sink.getConf();
  int maxReducers=context.getConf().getIntVar(HiveConf.ConfVars.MAXREDUCERS);
  int constantReducers=context.getConf().getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);
  if (context.getVisitedReduceSinks().contains(sink)) {
    LOG.debug("Already processed reduce sink: " + sink.getName());
    return true;
  }
  context.getVisitedReduceSinks().add(sink);
  if (desc.getNumReducers() <= 0) {
    if (constantReducers > 0) {
      LOG.info("Parallelism for reduce sink " + sink + " set by user to "+ constantReducers);
      desc.setNumReducers(constantReducers);
    }
 else {
      try {
        long numberOfBytes=0;
        for (        Operator<? extends OperatorDesc> sibling : sink.getChildOperators().get(0).getParentOperators()) {
          if (sibling.getStatistics() != null) {
            numberOfBytes+=sibling.getStatistics().getDataSize();
          }
 else {
            LOG.warn("No stats available from: " + sibling);
          }
        }
        if (sparkMemoryAndCores == null) {
          SparkSessionManager sparkSessionManager=null;
          SparkSession sparkSession=null;
          try {
            sparkSessionManager=SparkSessionManagerImpl.getInstance();
            sparkSession=SparkUtilities.getSparkSession(context.getConf(),sparkSessionManager);
            sparkMemoryAndCores=sparkSession.getMemoryAndCores();
          }
  finally {
            if (sparkSession != null && sparkSessionManager != null) {
              try {
                sparkSessionManager.returnSession(sparkSession);
              }
 catch (              HiveException ex) {
                LOG.error("Failed to return the session to SessionManager",ex);
              }
            }
          }
        }
        long bytesPerReducer=sparkMemoryAndCores._1.longValue() / 2;
        int numReducers=Utilities.estimateReducers(numberOfBytes,bytesPerReducer,maxReducers,false);
        int cores=sparkMemoryAndCores._2.intValue();
        if (numReducers < cores) {
          numReducers=cores;
        }
        LOG.info("Set parallelism for reduce sink " + sink + " to: "+ numReducers);
        desc.setNumReducers(numReducers);
      }
 catch (      Exception e) {
        LOG.warn("Failed to create spark client.",e);
      }
    }
  }
 else {
    LOG.info("Number of reducers determined to be: " + desc.getNumReducers());
  }
  return false;
}
