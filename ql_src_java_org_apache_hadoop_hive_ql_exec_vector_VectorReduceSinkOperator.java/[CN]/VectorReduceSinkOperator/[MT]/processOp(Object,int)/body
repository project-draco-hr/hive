{
  VectorizedRowBatch vrg=(VectorizedRowBatch)row;
  if (isDebugEnabled) {
    LOG.debug(String.format("sinking %d rows, %d values, %d keys, %d parts",vrg.size,valueEval.length,keyEval.length,partitionEval.length));
  }
  try {
    for (int i=0; i < keyEval.length; i++) {
      keyEval[i].evaluate(vrg);
    }
    int startResult=reducerHash.startVectorizedBatch(vrg.size);
    if (startResult == TopNHash.EXCLUDE) {
      return;
    }
    for (int i=0; i < partitionEval.length; i++) {
      partitionEval[i].evaluate(vrg);
    }
    if (bucketEval != null) {
      for (int i=0; i < bucketEval.length; i++) {
        bucketEval[i].evaluate(vrg);
      }
    }
    for (int i=0; i < valueEval.length; i++) {
      valueEval[i].evaluate(vrg);
    }
    boolean useTopN=startResult != TopNHash.FORWARD;
    for (int batchIndex=0; batchIndex < vrg.size; ++batchIndex) {
      int rowIndex=batchIndex;
      if (vrg.selectedInUse) {
        rowIndex=vrg.selected[batchIndex];
      }
      populatedCachedDistributionKeys(vrg,rowIndex,0);
      int buckNum=-1;
      if (bucketEval != null) {
        buckNum=computeBucketNumber(vrg,rowIndex,conf.getNumBuckets());
        cachedKeys[0][buckColIdxInKey]=new IntWritable(buckNum);
      }
      HiveKey firstKey=toHiveKey(cachedKeys[0],tag,null);
      int distKeyLength=firstKey.getDistKeyLength();
      if (numDistinctExprs > 0) {
        populateCachedDistinctKeys(vrg,rowIndex,0);
        firstKey=toHiveKey(cachedKeys[0],tag,distKeyLength);
      }
      final int hashCode;
      if (autoParallel && partitionEval.length > 0) {
        hashCode=computeMurmurHash(firstKey);
      }
 else {
        hashCode=computeHashCode(vrg,rowIndex,buckNum);
      }
      firstKey.setHashCode(hashCode);
      if (useTopN) {
        boolean partkeysNull=conf.isPTFReduceSink() && partitionKeysAreNull(vrg,rowIndex);
        reducerHash.tryStoreVectorizedKey(firstKey,partkeysNull,batchIndex);
      }
 else {
        BytesWritable value=makeValueWritable(vrg,rowIndex);
        collect(firstKey,value);
        forwardExtraDistinctRows(vrg,rowIndex,hashCode,value,distKeyLength,tag,0);
      }
    }
    if (!useTopN)     return;
    for (int batchIndex=0; batchIndex < vrg.size; ++batchIndex) {
      int result=reducerHash.getVectorizedBatchResult(batchIndex);
      if (result == TopNHash.EXCLUDE)       continue;
      int rowIndex=batchIndex;
      if (vrg.selectedInUse) {
        rowIndex=vrg.selected[batchIndex];
      }
      BytesWritable value=makeValueWritable(vrg,rowIndex);
      int distKeyLength=-1;
      int hashCode;
      if (result == TopNHash.FORWARD) {
        HiveKey firstKey=reducerHash.getVectorizedKeyToForward(batchIndex);
        distKeyLength=firstKey.getDistKeyLength();
        hashCode=firstKey.hashCode();
        collect(firstKey,value);
      }
 else {
        hashCode=reducerHash.getVectorizedKeyHashCode(batchIndex);
        reducerHash.storeValue(result,hashCode,value,true);
        distKeyLength=reducerHash.getVectorizedKeyDistLength(batchIndex);
      }
      if (numDistinctExprs > 1) {
        populatedCachedDistributionKeys(vrg,rowIndex,1);
        forwardExtraDistinctRows(vrg,rowIndex,hashCode,value,distKeyLength,tag,1);
      }
    }
  }
 catch (  SerDeException e) {
    throw new HiveException(e);
  }
catch (  IOException e) {
    throw new HiveException(e);
  }
}
