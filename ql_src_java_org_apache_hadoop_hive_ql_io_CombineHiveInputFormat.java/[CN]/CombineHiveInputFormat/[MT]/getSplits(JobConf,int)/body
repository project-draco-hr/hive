{
  PerfLogger perfLogger=SessionState.getPerfLogger();
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.GET_SPLITS);
  init(job);
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=getInputPaths(job);
  List<Path> nonCombinablePaths=new ArrayList<Path>(paths.length / 2);
  List<Path> combinablePaths=new ArrayList<Path>(paths.length / 2);
  int numThreads=Math.min(MAX_CHECK_NONCOMBINABLE_THREAD_NUM,(int)Math.ceil((double)paths.length / DEFAULT_NUM_PATH_PER_THREAD));
  int numPathPerThread=(int)Math.ceil((double)paths.length / numThreads);
  if (numThreads > 0) {
    LOG.info("Total number of paths: " + paths.length + ", launching "+ numThreads+ " threads to check non-combinable ones.");
    ExecutorService executor=Executors.newFixedThreadPool(numThreads);
    List<Future<Set<Integer>>> futureList=new ArrayList<Future<Set<Integer>>>(numThreads);
    try {
      for (int i=0; i < numThreads; i++) {
        int start=i * numPathPerThread;
        int length=i != numThreads - 1 ? numPathPerThread : paths.length - start;
        futureList.add(executor.submit(new CheckNonCombinablePathCallable(paths,start,length,job)));
      }
      Set<Integer> nonCombinablePathIndices=new HashSet<Integer>();
      for (      Future<Set<Integer>> future : futureList) {
        nonCombinablePathIndices.addAll(future.get());
      }
      for (int i=0; i < paths.length; i++) {
        if (nonCombinablePathIndices.contains(i)) {
          nonCombinablePaths.add(paths[i]);
        }
 else {
          combinablePaths.add(paths[i]);
        }
      }
    }
 catch (    Exception e) {
      LOG.error("Error checking non-combinable path",e);
      perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.GET_SPLITS);
      throw new IOException(e);
    }
 finally {
      executor.shutdownNow();
    }
  }
  String oldPaths=job.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);
  if (LOG.isDebugEnabled()) {
    LOG.debug("The received input paths are: [" + oldPaths + "] against the property "+ org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);
  }
  if (nonCombinablePaths.size() > 0) {
    FileInputFormat.setInputPaths(job,nonCombinablePaths.toArray(new Path[nonCombinablePaths.size()]));
    InputSplit[] splits=super.getSplits(job,numSplits);
    for (    InputSplit split : splits) {
      result.add(split);
    }
  }
  if (combinablePaths.size() > 0) {
    FileInputFormat.setInputPaths(job,combinablePaths.toArray(new Path[combinablePaths.size()]));
    Map<String,PartitionDesc> pathToPartitionInfo=this.pathToPartitionInfo != null ? this.pathToPartitionInfo : Utilities.getMapWork(job).getPathToPartitionInfo();
    InputSplit[] splits=getCombineSplits(job,numSplits,pathToPartitionInfo);
    for (    InputSplit split : splits) {
      result.add(split);
    }
  }
  if (oldPaths != null) {
    job.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,oldPaths);
  }
  Utilities.clearWorkMapForConf(job);
  LOG.info("Number of all splits " + result.size());
  perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.GET_SPLITS);
  return result.toArray(new InputSplit[result.size()]);
}
