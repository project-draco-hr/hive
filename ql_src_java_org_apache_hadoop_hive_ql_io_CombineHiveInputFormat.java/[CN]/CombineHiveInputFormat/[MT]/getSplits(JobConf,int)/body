{
  init(job);
  Map<String,ArrayList<String>> pathToAliases=mrwork.getPathToAliases();
  Map<String,Operator<? extends Serializable>> aliasToWork=mrwork.getAliasToWork();
  CombineFileInputFormatShim combine=ShimLoader.getHadoopShims().getCombineFileInputFormat();
  if (combine == null) {
    return super.getSplits(job,numSplits);
  }
  if (combine.getInputPathsShim(job).length == 0) {
    throw new IOException("No input paths specified in job");
  }
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=combine.getInputPathsShim(job);
  Map<CombinePathInputFormat,CombineFilter> poolMap=new HashMap<CombinePathInputFormat,CombineFilter>();
  Set<Path> poolSet=new HashSet<Path>();
  for (  Path path : paths) {
    PartitionDesc part=HiveFileFormatUtils.getPartitionDescFromPathRecursively(pathToPartitionInfo,path,IOPrepareCache.get().allocatePartitionDescMap());
    TableDesc tableDesc=part.getTableDesc();
    if ((tableDesc != null) && tableDesc.isNonNative()) {
      return super.getSplits(job,numSplits);
    }
    Class inputFormatClass=part.getInputFileFormatClass();
    String inputFormatClassName=inputFormatClass.getName();
    InputFormat inputFormat=getInputFormatFromCache(inputFormatClass,job);
    if (this.mrwork != null && !this.mrwork.getHadoopSupportsSplittable()) {
      FileSystem inpFs=path.getFileSystem(job);
      if (inputFormat instanceof TextInputFormat) {
        Queue<Path> dirs=new LinkedList<Path>();
        FileStatus fStats=inpFs.getFileStatus(path);
        if (fStats.isDir()) {
          dirs.offer(path);
        }
 else         if ((new CompressionCodecFactory(job)).getCodec(path) != null) {
          return super.getSplits(job,numSplits);
        }
        while (dirs.peek() != null) {
          Path tstPath=dirs.remove();
          FileStatus[] fStatus=inpFs.listStatus(tstPath);
          for (int idx=0; idx < fStatus.length; idx++) {
            if (fStatus[idx].isDir()) {
              dirs.offer(fStatus[idx].getPath());
            }
 else             if ((new CompressionCodecFactory(job)).getCodec(fStatus[idx].getPath()) != null) {
              return super.getSplits(job,numSplits);
            }
          }
        }
      }
    }
    if (inputFormat instanceof SymlinkTextInputFormat) {
      return super.getSplits(job,numSplits);
    }
    Path filterPath=path;
    if (mrwork.isMapperCannotSpanPartns() && !path.getFileSystem(job).getFileStatus(path).isDir()) {
      filterPath=path.getParent();
    }
    CombineFilter f=null;
    List<Operator<? extends Serializable>> opList=null;
    boolean done=false;
    if (!mrwork.isMapperCannotSpanPartns()) {
      opList=HiveFileFormatUtils.doGetWorksFromPath(pathToAliases,aliasToWork,filterPath);
      f=poolMap.get(new CombinePathInputFormat(opList,inputFormatClassName));
    }
 else {
      if (poolSet.contains(filterPath)) {
        LOG.info("CombineHiveInputSplit: pool is already created for " + path + "; using filter path "+ filterPath);
        done=true;
      }
      poolSet.add(filterPath);
    }
    if (!done) {
      if (f == null) {
        f=new CombineFilter(filterPath);
        LOG.info("CombineHiveInputSplit creating pool for " + path + "; using filter path "+ filterPath);
        combine.createPool(job,f);
        if (!mrwork.isMapperCannotSpanPartns()) {
          poolMap.put(new CombinePathInputFormat(opList,inputFormatClassName),f);
        }
      }
 else {
        LOG.info("CombineHiveInputSplit: pool is already created for " + path + "; using filter path "+ filterPath);
        f.addPath(filterPath);
      }
    }
  }
  InputSplitShim[] iss=combine.getSplits(job,1);
  if (mrwork.getNameToSplitSample() != null && !mrwork.getNameToSplitSample().isEmpty()) {
    iss=sampleSplits(iss);
  }
  for (  InputSplitShim is : iss) {
    CombineHiveInputSplit csplit=new CombineHiveInputSplit(job,is);
    result.add(csplit);
  }
  LOG.info("number of splits " + result.size());
  return result.toArray(new CombineHiveInputSplit[result.size()]);
}
