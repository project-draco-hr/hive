{
  PerfLogger perfLogger=SessionState.getPerfLogger();
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.GET_SPLITS);
  init(job);
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=getInputPaths(job);
  List<Path> nonCombinablePaths=new ArrayList<Path>(paths.length / 2);
  List<Path> combinablePaths=new ArrayList<Path>(paths.length / 2);
  int numThreads=Math.min(MAX_CHECK_NONCOMBINABLE_THREAD_NUM,(int)Math.ceil((double)paths.length / DEFAULT_NUM_PATH_PER_THREAD));
  if (numThreads > 0) {
    try {
      Set<Integer> nonCombinablePathIndices=getNonCombinablePathIndices(job,paths,numThreads);
      for (int i=0; i < paths.length; i++) {
        if (nonCombinablePathIndices.contains(i)) {
          nonCombinablePaths.add(paths[i]);
        }
 else {
          combinablePaths.add(paths[i]);
        }
      }
    }
 catch (    Exception e) {
      LOG.error("Error checking non-combinable path",e);
      perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.GET_SPLITS);
      throw new IOException(e);
    }
  }
  String oldPaths=job.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);
  if (LOG.isDebugEnabled()) {
    LOG.debug("The received input paths are: [" + oldPaths + "] against the property "+ org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);
  }
  if (nonCombinablePaths.size() > 0) {
    FileInputFormat.setInputPaths(job,nonCombinablePaths.toArray(new Path[nonCombinablePaths.size()]));
    InputSplit[] splits=super.getSplits(job,numSplits);
    for (    InputSplit split : splits) {
      result.add(split);
    }
  }
  if (combinablePaths.size() > 0) {
    FileInputFormat.setInputPaths(job,combinablePaths.toArray(new Path[combinablePaths.size()]));
    Map<Path,PartitionDesc> pathToPartitionInfo=this.pathToPartitionInfo != null ? this.pathToPartitionInfo : Utilities.getMapWork(job).getPathToPartitionInfo();
    InputSplit[] splits=getCombineSplits(job,numSplits,pathToPartitionInfo);
    for (    InputSplit split : splits) {
      result.add(split);
    }
  }
  if (oldPaths != null) {
    job.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,oldPaths);
  }
  Utilities.clearWorkMapForConf(job);
  LOG.info("Number of all splits " + result.size());
  perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.GET_SPLITS);
  return result.toArray(new InputSplit[result.size()]);
}
