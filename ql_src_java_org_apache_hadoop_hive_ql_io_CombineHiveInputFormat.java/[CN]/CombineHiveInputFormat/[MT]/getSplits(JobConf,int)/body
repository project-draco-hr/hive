{
  init(job);
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=getInputPaths(job);
  List<Path> nonCombinablePaths=new ArrayList<Path>(paths.length / 2);
  List<Path> combinablePaths=new ArrayList<Path>(paths.length / 2);
  for (  Path path : paths) {
    PartitionDesc part=HiveFileFormatUtils.getPartitionDescFromPathRecursively(pathToPartitionInfo,path,IOPrepareCache.get().allocatePartitionDescMap());
    Class<? extends InputFormat> inputFormatClass=part.getInputFileFormatClass();
    InputFormat<WritableComparable,Writable> inputFormat=getInputFormatFromCache(inputFormatClass,job);
    if (inputFormat instanceof AvoidSplitCombination && ((AvoidSplitCombination)inputFormat).shouldSkipCombine(path,job)) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("The split [" + path + "] is being parked for HiveInputFormat.getSplits");
      }
      nonCombinablePaths.add(path);
    }
 else {
      combinablePaths.add(path);
    }
  }
  String oldPaths=job.get(HiveConf.ConfVars.HADOOPMAPREDINPUTDIR.varname);
  if (LOG.isDebugEnabled()) {
    LOG.debug("The received input paths are: [" + oldPaths + "] against the property "+ HiveConf.ConfVars.HADOOPMAPREDINPUTDIR.varname);
  }
  if (nonCombinablePaths.size() > 0) {
    FileInputFormat.setInputPaths(job,nonCombinablePaths.toArray(new Path[nonCombinablePaths.size()]));
    InputSplit[] splits=super.getSplits(job,numSplits);
    for (    InputSplit split : splits) {
      result.add(split);
    }
  }
  if (combinablePaths.size() > 0) {
    FileInputFormat.setInputPaths(job,combinablePaths.toArray(new Path[combinablePaths.size()]));
    InputSplit[] splits=getCombineSplits(job,numSplits);
    for (    InputSplit split : splits) {
      result.add(split);
    }
  }
  if (oldPaths != null) {
    job.set(HiveConf.ConfVars.HADOOPMAPREDINPUTDIR.varname,oldPaths);
  }
  LOG.info("Number of all splits " + result.size());
  return result.toArray(new InputSplit[result.size()]);
}
