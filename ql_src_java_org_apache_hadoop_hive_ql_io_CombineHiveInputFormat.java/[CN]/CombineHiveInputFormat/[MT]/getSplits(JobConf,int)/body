{
  init(job);
  CombineFileInputFormatShim combine=ShimLoader.getHadoopShims().getCombineFileInputFormat();
  if (combine == null) {
    return super.getSplits(job,numSplits);
  }
  if (combine.getInputPathsShim(job).length == 0) {
    throw new IOException("No input paths specified in job");
  }
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=combine.getInputPathsShim(job);
  for (  Path path : paths) {
    LOG.info("CombineHiveInputSplit creating pool for " + path);
    FileSystem inpFs=path.getFileSystem(job);
    FileStatus fStats=inpFs.getFileStatus(path);
    Path tstPath=path;
    if (fStats.isDir()) {
      FileStatus[] fStatus=inpFs.listStatus(path);
      if (fStatus.length > 0) {
        tstPath=fStatus[0].getPath();
      }
    }
    PartitionDesc part=getPartitionDescFromPath(pathToPartitionInfo,path);
    Class inputFormatClass=part.getInputFileFormatClass();
    InputFormat inputFormat=getInputFormatFromCache(inputFormatClass,job);
    TableDesc tableDesc=part.getTableDesc();
    if ((tableDesc != null) && tableDesc.isNonNative()) {
      return super.getSplits(job,numSplits);
    }
    if ((inputFormat instanceof TextInputFormat) && ((new CompressionCodecFactory(job)).getCodec(tstPath) != null)) {
      return super.getSplits(job,numSplits);
    }
    if (inputFormat instanceof SymlinkTextInputFormat) {
      return super.getSplits(job,numSplits);
    }
    combine.createPool(job,new CombineFilter(path));
  }
  InputSplitShim[] iss=combine.getSplits(job,1);
  for (  InputSplitShim is : iss) {
    CombineHiveInputSplit csplit=new CombineHiveInputSplit(job,is);
    result.add(csplit);
  }
  LOG.info("number of splits " + result.size());
  return result.toArray(new CombineHiveInputSplit[result.size()]);
}
