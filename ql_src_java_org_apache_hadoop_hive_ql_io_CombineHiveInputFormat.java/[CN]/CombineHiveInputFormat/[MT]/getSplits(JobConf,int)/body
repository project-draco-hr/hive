{
  init(job);
  CombineFileInputFormatShim combine=ShimLoader.getHadoopShims().getCombineFileInputFormat();
  if (combine == null) {
    return super.getSplits(job,numSplits);
  }
  if (combine.getInputPathsShim(job).length == 0) {
    throw new IOException("No input paths specified in job");
  }
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=combine.getInputPathsShim(job);
  Set<Path> poolSet=new HashSet<Path>();
  for (  Path path : paths) {
    PartitionDesc part=HiveFileFormatUtils.getPartitionDescFromPathRecursively(pathToPartitionInfo,path,IOPrepareCache.get().allocatePartitionDescMap());
    TableDesc tableDesc=part.getTableDesc();
    if ((tableDesc != null) && tableDesc.isNonNative()) {
      return super.getSplits(job,numSplits);
    }
    Class inputFormatClass=part.getInputFileFormatClass();
    InputFormat inputFormat=getInputFormatFromCache(inputFormatClass,job);
    if (this.mrwork != null && !this.mrwork.getHadoopSupportsSplittable()) {
      FileSystem inpFs=path.getFileSystem(job);
      if (inputFormat instanceof TextInputFormat) {
        Queue<Path> dirs=new LinkedList<Path>();
        FileStatus fStats=inpFs.getFileStatus(path);
        if (fStats.isDir()) {
          dirs.offer(path);
        }
 else         if ((new CompressionCodecFactory(job)).getCodec(path) != null) {
          return super.getSplits(job,numSplits);
        }
        while (dirs.peek() != null) {
          Path tstPath=dirs.remove();
          FileStatus[] fStatus=inpFs.listStatus(tstPath);
          for (int idx=0; idx < fStatus.length; idx++) {
            if (fStatus[idx].isDir()) {
              dirs.offer(fStatus[idx].getPath());
            }
 else             if ((new CompressionCodecFactory(job)).getCodec(fStatus[idx].getPath()) != null) {
              return super.getSplits(job,numSplits);
            }
          }
        }
      }
    }
    if (inputFormat instanceof SymlinkTextInputFormat) {
      return super.getSplits(job,numSplits);
    }
    Path filterPath=path;
    if (!path.getFileSystem(job).getFileStatus(path).isDir()) {
      filterPath=path.getParent();
    }
    if (!poolSet.contains(filterPath)) {
      LOG.info("CombineHiveInputSplit creating pool for " + path + "; using filter path "+ filterPath);
      combine.createPool(job,new CombineFilter(filterPath));
      poolSet.add(filterPath);
    }
 else {
      LOG.info("CombineHiveInputSplit: pool is already created for " + path + "; using filter path "+ filterPath);
    }
  }
  InputSplitShim[] iss=combine.getSplits(job,1);
  for (  InputSplitShim is : iss) {
    CombineHiveInputSplit csplit=new CombineHiveInputSplit(job,is);
    result.add(csplit);
  }
  LOG.info("number of splits " + result.size());
  return result.toArray(new CombineHiveInputSplit[result.size()]);
}
