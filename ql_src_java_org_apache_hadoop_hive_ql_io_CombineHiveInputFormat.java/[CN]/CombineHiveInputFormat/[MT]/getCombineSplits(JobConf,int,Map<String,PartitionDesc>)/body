{
  init(job);
  Map<String,ArrayList<String>> pathToAliases=mrwork.getPathToAliases();
  Map<String,Operator<? extends OperatorDesc>> aliasToWork=mrwork.getAliasToWork();
  CombineFileInputFormatShim combine=ShimLoader.getHadoopShims().getCombineFileInputFormat();
  InputSplit[] splits=null;
  if (combine == null) {
    splits=super.getSplits(job,numSplits);
    return splits;
  }
  if (combine.getInputPathsShim(job).length == 0) {
    throw new IOException("No input paths specified in job");
  }
  ArrayList<InputSplit> result=new ArrayList<InputSplit>();
  Path[] paths=combine.getInputPathsShim(job);
  List<Path> inpDirs=new ArrayList<Path>();
  List<Path> inpFiles=new ArrayList<Path>();
  Map<CombinePathInputFormat,CombineFilter> poolMap=new HashMap<CombinePathInputFormat,CombineFilter>();
  Set<Path> poolSet=new HashSet<Path>();
  for (  Path path : paths) {
    PartitionDesc part=HiveFileFormatUtils.getPartitionDescFromPathRecursively(pathToPartitionInfo,path,IOPrepareCache.get().allocatePartitionDescMap());
    TableDesc tableDesc=part.getTableDesc();
    if ((tableDesc != null) && tableDesc.isNonNative()) {
      return super.getSplits(job,numSplits);
    }
    Class inputFormatClass=part.getInputFileFormatClass();
    String inputFormatClassName=inputFormatClass.getName();
    InputFormat inputFormat=getInputFormatFromCache(inputFormatClass,job);
    String deserializerClassName=null;
    try {
      deserializerClassName=part.getDeserializer(job).getClass().getName();
    }
 catch (    Exception e) {
    }
    FileSystem inpFs=path.getFileSystem(job);
    if (this.mrwork != null && !this.mrwork.getHadoopSupportsSplittable()) {
      if (inputFormat instanceof TextInputFormat) {
        Queue<Path> dirs=new LinkedList<Path>();
        FileStatus fStats=inpFs.getFileStatus(path);
        if (fStats.isDir()) {
          dirs.offer(path);
        }
 else         if ((new CompressionCodecFactory(job)).getCodec(path) != null) {
          splits=super.getSplits(job,numSplits);
          return splits;
        }
        while (dirs.peek() != null) {
          Path tstPath=dirs.remove();
          FileStatus[] fStatus=inpFs.listStatus(tstPath);
          for (int idx=0; idx < fStatus.length; idx++) {
            if (fStatus[idx].isDir()) {
              dirs.offer(fStatus[idx].getPath());
            }
 else             if ((new CompressionCodecFactory(job)).getCodec(fStatus[idx].getPath()) != null) {
              splits=super.getSplits(job,numSplits);
              return splits;
            }
          }
        }
      }
    }
    if (inputFormat instanceof SymlinkTextInputFormat) {
      splits=super.getSplits(job,numSplits);
      return splits;
    }
    Path filterPath=path;
    CombineFilter f=null;
    List<Operator<? extends OperatorDesc>> opList=null;
    if (!mrwork.isMapperCannotSpanPartns()) {
      opList=HiveFileFormatUtils.doGetWorksFromPath(pathToAliases,aliasToWork,filterPath);
      CombinePathInputFormat combinePathInputFormat=new CombinePathInputFormat(opList,inputFormatClassName,deserializerClassName);
      f=poolMap.get(combinePathInputFormat);
      if (f == null) {
        f=new CombineFilter(filterPath);
        LOG.info("CombineHiveInputSplit creating pool for " + path + "; using filter path "+ filterPath);
        combine.createPool(job,f);
        poolMap.put(combinePathInputFormat,f);
      }
 else {
        LOG.info("CombineHiveInputSplit: pool is already created for " + path + "; using filter path "+ filterPath);
        f.addPath(filterPath);
      }
    }
 else {
      if (!path.getFileSystem(job).getFileStatus(path).isDir()) {
        filterPath=path.getParent();
        inpFiles.add(path);
        poolSet.add(filterPath);
      }
 else {
        inpDirs.add(path);
      }
    }
  }
  List<CombineFileSplit> iss=new ArrayList<CombineFileSplit>();
  if (!mrwork.isMapperCannotSpanPartns()) {
    iss=Arrays.asList(combine.getSplits(job,1));
  }
 else {
    for (    Path path : inpDirs) {
      processPaths(job,combine,iss,path);
    }
    if (inpFiles.size() > 0) {
      for (      Path filterPath : poolSet) {
        combine.createPool(job,new CombineFilter(filterPath));
      }
      processPaths(job,combine,iss,inpFiles.toArray(new Path[0]));
    }
  }
  if (mrwork.getNameToSplitSample() != null && !mrwork.getNameToSplitSample().isEmpty()) {
    iss=sampleSplits(iss);
  }
  for (  CombineFileSplit is : iss) {
    CombineHiveInputSplit csplit=new CombineHiveInputSplit(job,is,pathToPartitionInfo);
    result.add(csplit);
  }
  LOG.info("number of splits " + result.size());
  return result.toArray(new CombineHiveInputSplit[result.size()]);
}
