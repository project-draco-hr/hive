{
  HiveMetaStoreClient client=null;
  try {
    client=createHiveMetaClient(job.getConfiguration(),inputJobInfo);
    Table table=client.getTable(inputJobInfo.getDatabaseName(),inputJobInfo.getTableName());
    List<PartInfo> partInfoList=new ArrayList<PartInfo>();
    if (table.getPartitionKeys().size() != 0) {
      List<Partition> parts=client.listPartitionsByFilter(inputJobInfo.getDatabaseName(),inputJobInfo.getTableName(),inputJobInfo.getFilter(),(short)-1);
      int maxPart=hiveConf.getInt("hcat.metastore.maxpartitions",100000);
      if (parts != null && parts.size() > maxPart) {
        throw new HCatException(ErrorType.ERROR_EXCEED_MAXPART,"total number of partitions is " + parts.size());
      }
      for (      Partition ptn : parts) {
        PartInfo partInfo=extractPartInfo(ptn.getSd(),ptn.getParameters());
        partInfo.setPartitionValues(createPtnKeyValueMap(table,ptn));
        partInfoList.add(partInfo);
      }
    }
 else {
      PartInfo partInfo=extractPartInfo(table.getSd(),table.getParameters());
      partInfo.setPartitionValues(new HashMap<String,String>());
      partInfoList.add(partInfo);
    }
    inputJobInfo.setPartitions(partInfoList);
    inputJobInfo.setTableInfo(HCatTableInfo.valueOf(table));
    job.getConfiguration().set(HCatConstants.HCAT_KEY_JOB_INFO,HCatUtil.serialize(inputJobInfo));
  }
  finally {
    if (client != null) {
      client.close();
    }
  }
}
