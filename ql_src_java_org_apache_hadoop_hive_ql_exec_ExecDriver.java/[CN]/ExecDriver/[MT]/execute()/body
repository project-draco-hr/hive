{
  success=true;
  try {
    setNumberOfReducers();
  }
 catch (  IOException e) {
    String statusMesg="IOException while accessing HDFS to estimate the number of reducers: " + e.getMessage();
    console.printError(statusMesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 1;
  }
  String invalidReason=work.isInvalid();
  if (invalidReason != null) {
    throw new RuntimeException("Plan invalid, Reason: " + invalidReason);
  }
  String hiveScratchDir=HiveConf.getVar(job,HiveConf.ConfVars.SCRATCHDIR);
  String jobScratchDirStr=hiveScratchDir + File.separator + Utilities.randGen.nextInt();
  Path jobScratchDir=new Path(jobScratchDirStr);
  String emptyScratchDirStr=null;
  Path emptyScratchDir=null;
  int numTries=3;
  while (numTries > 0) {
    emptyScratchDirStr=hiveScratchDir + File.separator + Utilities.randGen.nextInt();
    emptyScratchDir=new Path(emptyScratchDirStr);
    try {
      FileSystem fs=emptyScratchDir.getFileSystem(job);
      fs.mkdirs(emptyScratchDir);
      break;
    }
 catch (    Exception e) {
      if (numTries > 0) {
        numTries--;
      }
 else {
        throw new RuntimeException("Failed to make dir " + emptyScratchDir.toString() + " : "+ e.getMessage());
      }
    }
  }
  FileOutputFormat.setOutputPath(job,jobScratchDir);
  job.setMapperClass(ExecMapper.class);
  job.setMapOutputKeyClass(HiveKey.class);
  job.setMapOutputValueClass(BytesWritable.class);
  if (work.getNumMapTasks() != null) {
    job.setNumMapTasks(work.getNumMapTasks().intValue());
  }
  if (work.getMinSplitSize() != null) {
    job.setInt(HiveConf.ConfVars.MAPREDMINSPLITSIZE.varname,work.getMinSplitSize().intValue());
  }
  job.setNumReduceTasks(work.getNumReduceTasks().intValue());
  job.setReducerClass(ExecReducer.class);
  if (work.getInputformat() != null) {
    HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,work.getInputformat());
  }
  boolean useSpeculativeExecReducers=HiveConf.getBoolVar(job,HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);
  job.setBoolean(HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS.varname,useSpeculativeExecReducers);
  String inpFormat=HiveConf.getVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT);
  if ((inpFormat == null) || (!StringUtils.isNotBlank(inpFormat))) {
    inpFormat=ShimLoader.getHadoopShims().getInputFormatClassName();
  }
  LOG.info("Using " + inpFormat);
  try {
    job.setInputFormat((Class<? extends InputFormat>)(Class.forName(inpFormat)));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage());
  }
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  String addedJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDJARS);
  if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
    String allJars=StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars + "," + auxJars : auxJars) : addedJars;
    LOG.info("adding libjars: " + allJars);
    initializeFiles("tmpjars",allJars);
  }
  String addedFiles=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDFILES);
  if (StringUtils.isNotBlank(addedFiles)) {
    initializeFiles("tmpfiles",addedFiles);
  }
  String addedArchives=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDARCHIVES);
  if (StringUtils.isNotBlank(addedArchives)) {
    initializeFiles("tmparchives",addedArchives);
  }
  int returnVal=0;
  RunningJob rj=null, orig_rj=null;
  boolean noName=StringUtils.isEmpty(HiveConf.getVar(job,HiveConf.ConfVars.HADOOPJOBNAME));
  if (noName) {
    HiveConf.setVar(job,HiveConf.ConfVars.HADOOPJOBNAME,"JOB" + randGen.nextInt());
  }
  try {
    addInputPaths(job,work,emptyScratchDirStr);
    Utilities.setMapRedWork(job,work);
    String pwd=job.get(HiveConf.ConfVars.METASTOREPWD.varname);
    if (pwd != null) {
      job.set(HiveConf.ConfVars.METASTOREPWD.varname,"HIVE");
    }
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    orig_rj=rj=jc.submitJob(job);
    if (pwd != null) {
      job.set(HiveConf.ConfVars.METASTOREPWD.varname,pwd);
    }
    runningJobKillURIs.put(rj.getJobID(),rj.getTrackingURL() + "&action=kill");
    TaskHandle th=new ExecDriverTaskHandle(jc,rj);
    jobInfo(rj);
    progress(th);
    if (rj == null) {
      rj=orig_rj;
      success=false;
    }
    String statusMesg=getJobEndMsg(rj.getJobID());
    if (!success) {
      statusMesg+=" with errors";
      returnVal=2;
      console.printError(statusMesg);
      showJobFailDebugInfo(job,rj);
    }
 else {
      console.printInfo(statusMesg);
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    Utilities.clearMapRedWork(job);
    try {
      FileSystem fs=jobScratchDir.getFileSystem(job);
      fs.delete(jobScratchDir,true);
      fs.delete(emptyScratchDir,true);
      if (returnVal != 0 && rj != null) {
        rj.killJob();
      }
      runningJobKillURIs.remove(rj.getJobID());
    }
 catch (    Exception e) {
    }
  }
  try {
    if (rj != null) {
      if (work.getAliasToWork() != null) {
        for (        Operator<? extends Serializable> op : work.getAliasToWork().values()) {
          op.jobClose(job,success);
        }
      }
      if (work.getReducer() != null) {
        work.getReducer().jobClose(job,success);
      }
    }
  }
 catch (  Exception e) {
    if (success) {
      success=false;
      returnVal=3;
      String mesg="Job Commit failed with exception '" + Utilities.getNameMessage(e) + "'";
      console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    }
  }
  return (returnVal);
}
