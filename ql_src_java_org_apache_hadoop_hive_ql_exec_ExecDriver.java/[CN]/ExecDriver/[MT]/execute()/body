{
  try {
    setNumberOfReducers();
  }
 catch (  IOException e) {
    String statusMesg="IOException while accessing HDFS to estimate the number of reducers: " + e.getMessage();
    console.printError(statusMesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 1;
  }
  String invalidReason=work.isInvalid();
  if (invalidReason != null) {
    throw new RuntimeException("Plan invalid, Reason: " + invalidReason);
  }
  String hiveScratchDir=HiveConf.getVar(job,HiveConf.ConfVars.SCRATCHDIR);
  Path jobScratchDir=new Path(hiveScratchDir + Utilities.randGen.nextInt());
  FileOutputFormat.setOutputPath(job,jobScratchDir);
  job.setMapperClass(ExecMapper.class);
  job.setMapOutputKeyClass(HiveKey.class);
  job.setMapOutputValueClass(Text.class);
  job.setNumReduceTasks(work.getNumReduceTasks().intValue());
  job.setReducerClass(ExecReducer.class);
  job.setInputFormat(org.apache.hadoop.hive.ql.io.HiveInputFormat.class);
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  if (StringUtils.isNotBlank(auxJars)) {
    LOG.info("adding libjars: " + auxJars);
    job.set("tmpjars",auxJars);
  }
  int returnVal=0;
  RunningJob rj=null, orig_rj=null;
  boolean success=false;
  try {
    addInputPaths(job,work,hiveScratchDir);
    Utilities.setMapRedWork(job,work);
    String pwd=job.get(HiveConf.ConfVars.METASTOREPWD.varname);
    if (pwd != null)     job.set(HiveConf.ConfVars.METASTOREPWD.varname,"HIVE");
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    orig_rj=rj=jc.submitJob(job);
    if (pwd != null)     job.set(HiveConf.ConfVars.METASTOREPWD.varname,pwd);
    runningJobKillURIs.put(rj.getJobID(),rj.getTrackingURL() + "&action=kill");
    jobInfo(rj);
    rj=jobProgress(jc,rj);
    if (rj == null) {
      rj=orig_rj;
      success=false;
    }
 else {
      success=rj.isSuccessful();
    }
    String statusMesg="Ended Job = " + rj.getJobID();
    if (!success) {
      statusMesg+=" with errors";
      returnVal=2;
      console.printError(statusMesg);
    }
 else {
      console.printInfo(statusMesg);
    }
  }
 catch (  Exception e) {
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    Utilities.clearMapRedWork(job);
    try {
      FileSystem fs=jobScratchDir.getFileSystem(job);
      fs.delete(jobScratchDir,true);
      if (returnVal != 0 && rj != null) {
        rj.killJob();
      }
      runningJobKillURIs.remove(rj.getJobID());
    }
 catch (    Exception e) {
    }
  }
  try {
    if (rj != null) {
      if (work.getAliasToWork() != null) {
        for (        Operator<? extends Serializable> op : work.getAliasToWork().values()) {
          op.jobClose(job,success);
        }
      }
      if (work.getReducer() != null) {
        work.getReducer().jobClose(job,success);
      }
    }
  }
 catch (  Exception e) {
    if (success) {
      success=false;
      returnVal=3;
      String mesg="Job Commit failed with exception '" + Utilities.getNameMessage(e) + "'";
      console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    }
  }
  return (returnVal);
}
