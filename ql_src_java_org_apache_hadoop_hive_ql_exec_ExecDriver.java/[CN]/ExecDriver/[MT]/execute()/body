{
  fillInDefaults();
  String invalidReason=work.isInvalid();
  if (invalidReason != null) {
    throw new RuntimeException("Plan invalid, Reason: " + invalidReason);
  }
  Utilities.setMapRedWork(job,work);
  for (  String onefile : work.getPathToAliases().keySet()) {
    LOG.info("Adding input file " + onefile);
    FileInputFormat.addInputPaths(job,onefile);
  }
  String hiveScratchDir=HiveConf.getVar(job,HiveConf.ConfVars.SCRATCHDIR);
  String jobScratchDir=hiveScratchDir + Utilities.randGen.nextInt();
  FileOutputFormat.setOutputPath(job,new Path(jobScratchDir));
  job.setMapperClass(ExecMapper.class);
  if (!work.getNeedsTagging()) {
    job.setMapOutputValueClass(NoTagWritableHiveObject.class);
    job.setMapOutputKeyClass(NoTagWritableComparableHiveObject.class);
    job.setOutputKeyComparatorClass(NoTagHiveObjectComparator.class);
  }
 else {
    job.setMapOutputValueClass(WritableHiveObject.class);
    job.setMapOutputKeyClass(WritableComparableHiveObject.class);
    job.setOutputKeyComparatorClass(HiveObjectComparator.class);
  }
  job.setNumReduceTasks(work.getNumReduceTasks().intValue());
  job.setReducerClass(ExecReducer.class);
  job.setInputFormat(org.apache.hadoop.hive.ql.io.HiveInputFormat.class);
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  if (StringUtils.isNotBlank(auxJars)) {
    LOG.info("adding libjars: " + auxJars);
    job.set("tmpjars",auxJars);
  }
  int returnVal=0;
  FileSystem fs=null;
  RunningJob rj=null;
  try {
    fs=FileSystem.get(job);
    JobClient jc=new JobClient(job);
    rj=jc.submitJob(job);
    jobInfo(rj);
    rj=jobProgress(jc,rj);
    String statusMesg="Ended Job = " + rj.getJobID();
    if (!rj.isSuccessful()) {
      statusMesg+=" with errors";
      returnVal=2;
      console.printError(statusMesg);
    }
 else {
      console.printInfo(statusMesg);
    }
  }
 catch (  Exception e) {
    String mesg=" with exception '" + e.getMessage() + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    returnVal=1;
  }
 finally {
    Utilities.clearMapRedWork(job);
    try {
      fs.delete(new Path(jobScratchDir),true);
      if (returnVal != 0 && rj != null) {
        rj.killJob();
      }
    }
 catch (    Exception e) {
    }
  }
  return (returnVal);
}
