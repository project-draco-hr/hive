{
  try {
    setNumberOfReducers();
  }
 catch (  IOException e) {
    String statusMesg="IOException while accessing HDFS to estimate the number of reducers: " + e.getMessage();
    console.printError(statusMesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 1;
  }
  String invalidReason=work.isInvalid();
  if (invalidReason != null) {
    throw new RuntimeException("Plan invalid, Reason: " + invalidReason);
  }
  Utilities.setMapRedWork(job,work);
  for (  String onefile : work.getPathToAliases().keySet()) {
    LOG.info("Adding input file " + onefile);
    FileInputFormat.addInputPaths(job,onefile);
  }
  String hiveScratchDir=HiveConf.getVar(job,HiveConf.ConfVars.SCRATCHDIR);
  String jobScratchDir=hiveScratchDir + Utilities.randGen.nextInt();
  FileOutputFormat.setOutputPath(job,new Path(jobScratchDir));
  job.setMapperClass(ExecMapper.class);
  job.setMapOutputKeyClass(HiveKey.class);
  job.setMapOutputValueClass(Text.class);
  job.setNumReduceTasks(work.getNumReduceTasks().intValue());
  job.setReducerClass(ExecReducer.class);
  job.setInputFormat(org.apache.hadoop.hive.ql.io.HiveInputFormat.class);
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  if (StringUtils.isNotBlank(auxJars)) {
    LOG.info("adding libjars: " + auxJars);
    job.set("tmpjars",auxJars);
  }
  int returnVal=0;
  FileSystem fs=null;
  RunningJob rj=null, orig_rj=null;
  boolean success=false;
  try {
    fs=FileSystem.get(job);
    Path[] inputPaths=FileInputFormat.getInputPaths(job);
    boolean emptyInput=true;
    for (    Path inputP : inputPaths) {
      if (!fs.exists(inputP))       continue;
      FileStatus[] fStats=fs.listStatus(inputP);
      for (      FileStatus fStat : fStats) {
        if (fStat.getLen() > 0) {
          emptyInput=false;
          break;
        }
      }
    }
    if (emptyInput) {
      console.printInfo("Job need not be submitted: no output: Success");
      return 0;
    }
    String pwd=job.get(HiveConf.ConfVars.METASTOREPWD.varname);
    job.set(HiveConf.ConfVars.METASTOREPWD.varname,"HIVE");
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    orig_rj=rj=jc.submitJob(job);
    job.set(HiveConf.ConfVars.METASTOREPWD.varname,pwd);
    runningJobKillURIs.put(rj.getJobID(),rj.getTrackingURL() + "&action=kill");
    jobInfo(rj);
    rj=jobProgress(jc,rj);
    if (rj == null) {
      rj=orig_rj;
      success=false;
    }
 else {
      success=rj.isSuccessful();
    }
    String statusMesg="Ended Job = " + rj.getJobID();
    if (!success) {
      statusMesg+=" with errors";
      returnVal=2;
      console.printError(statusMesg);
    }
 else {
      console.printInfo(statusMesg);
    }
  }
 catch (  Exception e) {
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    Utilities.clearMapRedWork(job);
    try {
      fs.delete(new Path(jobScratchDir),true);
      if (returnVal != 0 && rj != null) {
        rj.killJob();
      }
      runningJobKillURIs.remove(rj.getJobID());
    }
 catch (    Exception e) {
    }
  }
  try {
    if (rj != null) {
      if (work.getAliasToWork() != null) {
        for (        Operator<? extends Serializable> op : work.getAliasToWork().values()) {
          op.jobClose(job,success);
        }
      }
      if (work.getReducer() != null) {
        work.getReducer().jobClose(job,success);
      }
    }
  }
 catch (  Exception e) {
    if (success) {
      success=false;
      returnVal=3;
      String mesg="Job Commit failed with exception '" + Utilities.getNameMessage(e) + "'";
      console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    }
  }
  return (returnVal);
}
