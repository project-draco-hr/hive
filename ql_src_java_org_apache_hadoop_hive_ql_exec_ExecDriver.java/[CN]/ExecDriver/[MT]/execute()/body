{
  fillInDefaults();
  String invalidReason=work.isInvalid();
  if (invalidReason != null) {
    throw new RuntimeException("Plan invalid, Reason: " + invalidReason);
  }
  Utilities.setMapRedWork(job,work);
  for (  String onefile : work.getPathToAliases().keySet()) {
    LOG.info("Adding input file " + onefile);
    FileInputFormat.addInputPaths(job,onefile);
  }
  String hiveScratchDir=HiveConf.getVar(job,HiveConf.ConfVars.SCRATCHDIR);
  String jobScratchDir=hiveScratchDir + Utilities.randGen.nextInt();
  FileOutputFormat.setOutputPath(job,new Path(jobScratchDir));
  job.setMapperClass(ExecMapper.class);
  job.setMapOutputKeyClass(HiveKey.class);
  job.setMapOutputValueClass(BytesWritable.class);
  job.setNumReduceTasks(work.getNumReduceTasks().intValue());
  job.setReducerClass(ExecReducer.class);
  job.setInputFormat(org.apache.hadoop.hive.ql.io.HiveInputFormat.class);
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  if (StringUtils.isNotBlank(auxJars)) {
    LOG.info("adding libjars: " + auxJars);
    job.set("tmpjars",auxJars);
  }
  int returnVal=0;
  FileSystem fs=null;
  RunningJob rj=null;
  try {
    fs=FileSystem.get(job);
    Path[] inputPaths=FileInputFormat.getInputPaths(job);
    boolean emptyInput=true;
    for (    Path inputP : inputPaths) {
      if (!fs.exists(inputP))       continue;
      FileStatus[] fStats=fs.listStatus(inputP);
      for (      FileStatus fStat : fStats) {
        if (fStat.getLen() > 0) {
          emptyInput=false;
          break;
        }
      }
    }
    if (emptyInput) {
      console.printInfo("Job need not be submitted: no output: Success");
      return 0;
    }
    inferNumReducers();
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    rj=jc.submitJob(job);
    runningJobKillURIs.put(rj.getJobID(),rj.getTrackingURL() + "&action=kill");
    jobInfo(rj);
    rj=jobProgress(jc,rj);
    String statusMesg="Ended Job = " + rj.getJobID();
    if (!rj.isSuccessful()) {
      statusMesg+=" with errors";
      returnVal=2;
      console.printError(statusMesg);
    }
 else {
      console.printInfo(statusMesg);
    }
  }
 catch (  Exception e) {
    String mesg=" with exception '" + e.getMessage() + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    returnVal=1;
  }
 finally {
    Utilities.clearMapRedWork(job);
    try {
      fs.delete(new Path(jobScratchDir),true);
      if (returnVal != 0 && rj != null) {
        rj.killJob();
      }
      runningJobKillURIs.remove(rj.getJobID());
    }
 catch (    Exception e) {
    }
  }
  return (returnVal);
}
