{
  success=true;
  String invalidReason=work.isInvalid();
  if (invalidReason != null) {
    throw new RuntimeException("Plan invalid, Reason: " + invalidReason);
  }
  Context ctx=driverContext.getCtx();
  boolean ctxCreated=false;
  String emptyScratchDirStr;
  Path emptyScratchDir;
  try {
    if (ctx == null) {
      ctx=new Context(job);
      ctxCreated=true;
    }
    emptyScratchDirStr=ctx.getMRTmpFileURI();
    emptyScratchDir=new Path(emptyScratchDirStr);
    FileSystem fs=emptyScratchDir.getFileSystem(job);
    fs.mkdirs(emptyScratchDir);
  }
 catch (  IOException e) {
    e.printStackTrace();
    console.printError("Error launching map-reduce job","\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  ShimLoader.getHadoopShims().setNullOutputFormat(job);
  job.setMapperClass(ExecMapper.class);
  job.setMapOutputKeyClass(HiveKey.class);
  job.setMapOutputValueClass(BytesWritable.class);
  try {
    job.setPartitionerClass((Class<? extends Partitioner>)(Class.forName(HiveConf.getVar(job,HiveConf.ConfVars.HIVEPARTITIONER))));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage());
  }
  if (work.getNumMapTasks() != null) {
    job.setNumMapTasks(work.getNumMapTasks().intValue());
  }
  if (work.getMinSplitSize() != null) {
    HiveConf.setIntVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZE,work.getMinSplitSize().intValue());
  }
  job.setNumReduceTasks(work.getNumReduceTasks().intValue());
  job.setReducerClass(ExecReducer.class);
  if (work.getInputformat() != null) {
    HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,work.getInputformat());
  }
  boolean useSpeculativeExecReducers=HiveConf.getBoolVar(job,HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);
  HiveConf.setBoolVar(job,HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,useSpeculativeExecReducers);
  String inpFormat=HiveConf.getVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT);
  if ((inpFormat == null) || (!StringUtils.isNotBlank(inpFormat))) {
    inpFormat=ShimLoader.getHadoopShims().getInputFormatClassName();
  }
  LOG.info("Using " + inpFormat);
  try {
    job.setInputFormat((Class<? extends InputFormat>)(Class.forName(inpFormat)));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage());
  }
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  String addedJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDJARS);
  if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
    String allJars=StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars + "," + auxJars : auxJars) : addedJars;
    LOG.info("adding libjars: " + allJars);
    initializeFiles("tmpjars",allJars);
  }
  String addedFiles=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDFILES);
  if (StringUtils.isNotBlank(addedFiles)) {
    initializeFiles("tmpfiles",addedFiles);
  }
  String addedArchives=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDARCHIVES);
  if (StringUtils.isNotBlank(addedArchives)) {
    initializeFiles("tmparchives",addedArchives);
  }
  int returnVal=0;
  RunningJob rj=null, orig_rj=null;
  boolean noName=StringUtils.isEmpty(HiveConf.getVar(job,HiveConf.ConfVars.HADOOPJOBNAME));
  if (noName) {
    HiveConf.setVar(job,HiveConf.ConfVars.HADOOPJOBNAME,"JOB" + Utilities.randGen.nextInt());
  }
  try {
    addInputPaths(job,work,emptyScratchDirStr);
    Utilities.setMapRedWork(job,work,ctx.getMRTmpFileURI());
    String pwd=HiveConf.getVar(job,HiveConf.ConfVars.METASTOREPWD);
    if (pwd != null) {
      HiveConf.setVar(job,HiveConf.ConfVars.METASTOREPWD,"HIVE");
    }
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    orig_rj=rj=jc.submitJob(job);
    if (pwd != null) {
      HiveConf.setVar(job,HiveConf.ConfVars.METASTOREPWD,pwd);
    }
    runningJobKillURIs.put(rj.getJobID(),rj.getTrackingURL() + "&action=kill");
    ExecDriverTaskHandle th=new ExecDriverTaskHandle(jc,rj);
    jobInfo(rj);
    progress(th);
    if (rj == null) {
      rj=orig_rj;
      success=false;
    }
    String statusMesg=getJobEndMsg(rj.getJobID());
    if (!success) {
      statusMesg+=" with errors";
      returnVal=2;
      console.printError(statusMesg);
      showJobFailDebugInfo(job,rj);
    }
 else {
      console.printInfo(statusMesg);
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    Utilities.clearMapRedWork(job);
    try {
      if (ctxCreated)       ctx.clear();
      if (returnVal != 0 && rj != null) {
        rj.killJob();
      }
      runningJobKillURIs.remove(rj.getJobID());
    }
 catch (    Exception e) {
    }
  }
  ArrayList<String> dpPaths=new ArrayList<String>();
  try {
    if (rj != null) {
      JobCloseFeedBack feedBack=new JobCloseFeedBack();
      if (work.getAliasToWork() != null) {
        for (        Operator<? extends Serializable> op : work.getAliasToWork().values()) {
          op.jobClose(job,success,feedBack);
          ArrayList<Object> dirs=feedBack.get(JobCloseFeedBack.FeedBackType.DYNAMIC_PARTITIONS);
          if (dirs != null) {
            for (            Object o : dirs) {
              if (o instanceof String) {
                dpPaths.add((String)o);
              }
            }
          }
        }
      }
      if (work.getReducer() != null) {
        work.getReducer().jobClose(job,success,feedBack);
      }
    }
  }
 catch (  Exception e) {
    if (success) {
      success=false;
      returnVal=3;
      String mesg="Job Commit failed with exception '" + Utilities.getNameMessage(e) + "'";
      console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    }
  }
  return (returnVal);
}
