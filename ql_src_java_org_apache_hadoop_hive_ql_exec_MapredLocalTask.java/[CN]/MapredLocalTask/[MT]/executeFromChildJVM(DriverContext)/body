{
  if (work == null) {
    return -1;
  }
  memoryMXBean=ManagementFactory.getMemoryMXBean();
  long startTime=System.currentTimeMillis();
  console.printInfo(Utilities.now() + "\tStarting to launch local task to process map join;\tmaximum memory = " + memoryMXBean.getHeapMemoryUsage().getMax());
  fetchOperators=new HashMap<String,FetchOperator>();
  Map<FetchOperator,JobConf> fetchOpJobConfMap=new HashMap<FetchOperator,JobConf>();
  execContext.setJc(job);
  execContext.setLocalWork(work);
  boolean inputFileChangeSenstive=work.getInputFileChangeSensitive();
  try {
    initializeOperators(fetchOpJobConfMap);
    if (inputFileChangeSenstive) {
      for (      Map<String,List<String>> bigTableBucketFiles : work.getBucketMapjoinContext().getAliasBucketFileNameMapping().values()) {
        for (        String bigTableBucket : bigTableBucketFiles.keySet()) {
          startForward(inputFileChangeSenstive,bigTableBucket);
        }
      }
    }
 else {
      startForward(inputFileChangeSenstive,null);
    }
    long currentTime=System.currentTimeMillis();
    long elapsed=currentTime - startTime;
    console.printInfo(Utilities.now() + "\tEnd of local task; Time Taken: " + Utilities.showTime(elapsed)+ " sec.");
  }
 catch (  Throwable e) {
    if (e instanceof OutOfMemoryError || (e instanceof HiveException && e.getMessage().equals("RunOutOfMeomoryUsage"))) {
      return 3;
    }
 else {
      l4j.error("Hive Runtime Error: Map local work failed");
      e.printStackTrace();
      return 2;
    }
  }
  return 0;
}
