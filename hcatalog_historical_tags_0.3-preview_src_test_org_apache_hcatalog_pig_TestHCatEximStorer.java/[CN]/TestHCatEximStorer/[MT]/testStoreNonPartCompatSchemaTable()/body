{
  populateDataFile();
  PigServer server=new PigServer(ExecType.LOCAL,props);
  UDFContext.getUDFContext().setClientSystemProps();
  server.setBatchOn();
  server.registerQuery("A = load '" + fqdataLocation + "' using PigStorage(',') as (emp_id:int, emp_name:chararray, emp_dob:chararray, emp_sex:chararray);");
  server.registerQuery("store A into '" + NONPART_TABLE + "' using org.apache.hcatalog.pig.HCatEximStorer('"+ fqexportLocation+ "', '', 'id:int, name:chararray, dob:chararray, sex:chararray');");
  server.executeBatch();
  FileSystem fs=cluster.getFileSystem();
  System.out.println("Filesystem class : " + cluster.getFileSystem().getClass().getName() + ", fs.default.name : "+ props.getProperty("fs.default.name"));
  Map.Entry<Table,List<Partition>> metadata=EximUtil.readMetaData(fs,new Path(exportLocation,"_metadata"));
  Table table=metadata.getKey();
  List<Partition> partitions=metadata.getValue();
  List<HCatFieldSchema> columns=new ArrayList<HCatFieldSchema>();
  columns.add(HCatSchemaUtils.getHCatFieldSchema(new FieldSchema("id",Constants.INT_TYPE_NAME,"")));
  columns.add(HCatSchemaUtils.getHCatFieldSchema(new FieldSchema("name",Constants.STRING_TYPE_NAME,"")));
  columns.add(HCatSchemaUtils.getHCatFieldSchema(new FieldSchema("dob",Constants.STRING_TYPE_NAME,"")));
  columns.add(HCatSchemaUtils.getHCatFieldSchema(new FieldSchema("sex",Constants.STRING_TYPE_NAME,"")));
  assertEquals("default",table.getDbName());
  assertEquals(NONPART_TABLE,table.getTableName());
  assertTrue(EximUtil.schemaCompare(table.getSd().getCols(),HCatUtil.getFieldSchemaList(columns)));
  assertEquals("org.apache.hcatalog.rcfile.RCFileInputDriver",table.getParameters().get(HCatConstants.HCAT_ISD_CLASS));
  assertEquals("org.apache.hcatalog.rcfile.RCFileOutputDriver",table.getParameters().get(HCatConstants.HCAT_OSD_CLASS));
  assertEquals("org.apache.hadoop.hive.ql.io.RCFileInputFormat",table.getSd().getInputFormat());
  assertEquals("org.apache.hadoop.hive.ql.io.RCFileOutputFormat",table.getSd().getOutputFormat());
  assertEquals("org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe",table.getSd().getSerdeInfo().getSerializationLib());
  assertEquals(0,table.getPartitionKeys().size());
  assertEquals(0,partitions.size());
}
