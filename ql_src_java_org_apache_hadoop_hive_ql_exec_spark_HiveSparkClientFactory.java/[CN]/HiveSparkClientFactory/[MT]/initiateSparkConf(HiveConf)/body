{
  Map<String,String> sparkConf=new HashMap<String,String>();
  HBaseConfiguration.addHbaseResources(hiveConf);
  sparkConf.put("spark.master",SPARK_DEFAULT_MASTER);
  final String appNameKey="spark.app.name";
  String appName=hiveConf.get(appNameKey);
  if (appName == null) {
    appName=SPARK_DEFAULT_APP_NAME;
  }
  sparkConf.put(appNameKey,appName);
  sparkConf.put("spark.serializer",SPARK_DEFAULT_SERIALIZER);
  sparkConf.put("spark.kryo.referenceTracking",SPARK_DEFAULT_REFERENCE_TRACKING);
  InputStream inputStream=null;
  try {
    inputStream=HiveSparkClientFactory.class.getClassLoader().getResourceAsStream(SPARK_DEFAULT_CONF_FILE);
    if (inputStream != null) {
      LOG.info("loading spark properties from:" + SPARK_DEFAULT_CONF_FILE);
      Properties properties=new Properties();
      properties.load(new InputStreamReader(inputStream,CharsetNames.UTF_8));
      for (      String propertyName : properties.stringPropertyNames()) {
        if (propertyName.startsWith("spark")) {
          String value=properties.getProperty(propertyName);
          sparkConf.put(propertyName,properties.getProperty(propertyName));
          LOG.info(String.format("load spark property from %s (%s -> %s).",SPARK_DEFAULT_CONF_FILE,propertyName,value));
        }
      }
    }
  }
 catch (  IOException e) {
    LOG.info("Failed to open spark configuration file:" + SPARK_DEFAULT_CONF_FILE,e);
  }
 finally {
    if (inputStream != null) {
      try {
        inputStream.close();
      }
 catch (      IOException e) {
        LOG.debug("Failed to close inputstream.",e);
      }
    }
  }
  String sparkMaster=hiveConf.get("spark.master");
  if (sparkMaster == null) {
    sparkMaster=sparkConf.get("spark.master");
    hiveConf.set("spark.master",sparkMaster);
  }
  if (sparkMaster.equals("yarn-cluster")) {
    sparkConf.put("spark.yarn.maxAppAttempts","1");
  }
  for (  Map.Entry<String,String> entry : hiveConf) {
    String propertyName=entry.getKey();
    if (propertyName.startsWith("spark")) {
      String value=hiveConf.get(propertyName);
      sparkConf.put(propertyName,value);
      LOG.info(String.format("load spark property from hive configuration (%s -> %s).",propertyName,value));
    }
 else     if (propertyName.startsWith("yarn") && (sparkMaster.equals("yarn-client") || sparkMaster.equals("yarn-cluster"))) {
      String value=hiveConf.get(propertyName);
      sparkConf.put("spark.hadoop." + propertyName,value);
      LOG.info(String.format("load yarn property from hive configuration in %s mode (%s -> %s).",sparkMaster,propertyName,value));
    }
 else     if (propertyName.equals(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY)) {
      String value=hiveConf.get(propertyName);
      if (value != null && !value.isEmpty()) {
        sparkConf.put("spark.hadoop." + propertyName,value);
      }
    }
 else     if (propertyName.startsWith("hbase")) {
      String value=hiveConf.get(propertyName);
      sparkConf.put("spark.hadoop." + propertyName,value);
      LOG.info(String.format("load HBase configuration (%s -> %s).",propertyName,value));
    }
    if (RpcConfiguration.HIVE_SPARK_RSC_CONFIGS.contains(propertyName)) {
      String value=RpcConfiguration.getValue(hiveConf,propertyName);
      sparkConf.put(propertyName,value);
      LOG.info(String.format("load RPC property from hive configuration (%s -> %s).",propertyName,value));
    }
  }
  Set<String> classes=Sets.newHashSet(Splitter.on(",").trimResults().omitEmptyStrings().split(Strings.nullToEmpty(sparkConf.get("spark.kryo.classesToRegister"))));
  classes.add(Writable.class.getName());
  classes.add(VectorizedRowBatch.class.getName());
  classes.add(BytesWritable.class.getName());
  classes.add(HiveKey.class.getName());
  sparkConf.put("spark.kryo.classesToRegister",Joiner.on(",").join(classes));
  return sparkConf;
}
