{
  try {
    Context ctx=driverContext.getCtx();
    String hiveJar=conf.getJar();
    String hadoopExec=conf.getVar(HiveConf.ConfVars.HADOOPBIN);
    String libJarsOption;
    Path planPath=new Path(ctx.getLocalTmpFileURI(),"plan.xml");
    OutputStream out=FileSystem.getLocal(conf).create(planPath);
    MapredLocalWork plan=getWork();
    LOG.info("Generating plan file " + planPath.toString());
    Utilities.serializeMapRedLocalWork(plan,out);
    String isSilent="true".equalsIgnoreCase(System.getProperty("test.silent")) ? "-nolog" : "";
    String jarCmd;
    jarCmd=hiveJar + " " + ExecDriver.class.getName();
    String hiveConfArgs=ExecDriver.generateCmdLine(conf,ctx);
    String cmdLine=hadoopExec + " jar " + jarCmd+ " -localtask -plan "+ planPath.toString()+ " "+ isSilent+ " "+ hiveConfArgs;
    String workDir=(new File(".")).getCanonicalPath();
    String files=Utilities.getResourceFiles(conf,SessionState.ResourceType.FILE);
    if (!files.isEmpty()) {
      cmdLine=cmdLine + " -files " + files;
      workDir=(new Path(ctx.getLocalTmpFileURI())).toUri().getPath();
      if (!(new File(workDir)).mkdir()) {
        throw new IOException("Cannot create tmp working dir: " + workDir);
      }
      for (      String f : StringUtils.split(files,',')) {
        Path p=new Path(f);
        String target=p.toUri().getPath();
        String link=workDir + Path.SEPARATOR + p.getName();
        if (FileUtil.symLink(target,link) != 0) {
          throw new IOException("Cannot link to added file: " + target + " from: "+ link);
        }
      }
    }
    LOG.info("Executing: " + cmdLine);
    String hadoopOpts;
    StringBuilder sb=new StringBuilder();
    Properties p=System.getProperties();
    for (    String element : HIVE_SYS_PROP) {
      if (p.containsKey(element)) {
        sb.append(" -D" + element + "="+ p.getProperty(element));
      }
    }
    hadoopOpts=sb.toString();
    String[] env;
    Map<String,String> variables=new HashMap(System.getenv());
    int hadoopMem=conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);
    if (hadoopMem == 0) {
      variables.remove(HADOOP_MEM_KEY);
    }
 else {
      console.printInfo(" set heap size\t" + hadoopMem + "MB");
      variables.put(HADOOP_MEM_KEY,String.valueOf(hadoopMem));
    }
    HadoopShims shim=ShimLoader.getHadoopShims();
    String endUserName=shim.getShortUserName(shim.getUGIForConf(job));
    LOG.debug("setting HADOOP_USER_NAME\t" + endUserName);
    variables.put("HADOOP_USER_NAME",endUserName);
    if (variables.containsKey(HADOOP_OPTS_KEY)) {
      variables.put(HADOOP_OPTS_KEY,variables.get(HADOOP_OPTS_KEY) + hadoopOpts);
    }
 else {
      variables.put(HADOOP_OPTS_KEY,hadoopOpts);
    }
    if (variables.containsKey(MapRedTask.HIVE_DEBUG_RECURSIVE)) {
      MapRedTask.configureDebugVariablesForChildJVM(variables);
    }
    env=new String[variables.size()];
    int pos=0;
    for (    Map.Entry<String,String> entry : variables.entrySet()) {
      String name=entry.getKey();
      String value=entry.getValue();
      env[pos++]=name + "=" + value;
    }
    executor=Runtime.getRuntime().exec(cmdLine,env,new File(workDir));
    CachingPrintStream errPrintStream=new CachingPrintStream(System.err);
    StreamPrinter outPrinter=new StreamPrinter(executor.getInputStream(),null,System.out);
    StreamPrinter errPrinter=new StreamPrinter(executor.getErrorStream(),null,errPrintStream);
    outPrinter.start();
    errPrinter.start();
    int exitVal=jobExecHelper.progressLocal(executor,getId());
    if (exitVal != 0) {
      LOG.error("Execution failed with exit status: " + exitVal);
      if (SessionState.get() != null) {
        SessionState.get().addLocalMapRedErrors(getId(),errPrintStream.getOutput());
      }
    }
 else {
      LOG.info("Execution completed successfully");
      console.printInfo("Mapred Local Task Succeeded . Convert the Join into MapJoin");
    }
    return exitVal;
  }
 catch (  Exception e) {
    e.printStackTrace();
    LOG.error("Exception: " + e.getMessage());
    return (1);
  }
}
