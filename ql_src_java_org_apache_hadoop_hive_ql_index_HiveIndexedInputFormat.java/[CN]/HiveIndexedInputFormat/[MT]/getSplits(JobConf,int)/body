{
  String indexFileStr=job.get(indexFile);
  l4j.info("index_file is " + indexFileStr);
  List<String> indexFiles=getIndexFiles(indexFileStr);
  HiveIndexResult hiveIndexResult=null;
  if (indexFiles != null) {
    boolean first=true;
    StringBuilder newInputPaths=new StringBuilder();
    try {
      hiveIndexResult=new HiveIndexResult(indexFiles,job);
    }
 catch (    HiveException e) {
      l4j.error("Unable to read index..");
      throw new IOException(e);
    }
    Set<String> inputFiles=hiveIndexResult.buckets.keySet();
    if (inputFiles == null || inputFiles.size() <= 0) {
      return new InputSplit[0];
    }
    Iterator<String> iter=inputFiles.iterator();
    while (iter.hasNext()) {
      String path=iter.next();
      if (path.trim().equalsIgnoreCase("")) {
        continue;
      }
      if (!first) {
        newInputPaths.append(",");
      }
 else {
        first=false;
      }
      newInputPaths.append(path);
    }
    FileInputFormat.setInputPaths(job,newInputPaths.toString());
  }
 else {
    return super.getSplits(job,numSplits);
  }
  HiveInputSplit[] splits=(HiveInputSplit[])this.doGetSplits(job,numSplits);
  long maxInputSize=HiveConf.getLongVar(job,ConfVars.HIVE_INDEX_COMPACT_QUERY_MAX_SIZE);
  if (maxInputSize < 0) {
    maxInputSize=Long.MAX_VALUE;
  }
  SplitFilter filter=new SplitFilter(hiveIndexResult,maxInputSize);
  Collection<HiveInputSplit> newSplits=filter.filter(splits);
  return newSplits.toArray(new FileSplit[newSplits.size()]);
}
