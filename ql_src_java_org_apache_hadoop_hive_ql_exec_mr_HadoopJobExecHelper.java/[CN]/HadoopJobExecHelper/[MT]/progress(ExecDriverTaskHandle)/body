{
  JobClient jc=th.getJobClient();
  RunningJob rj=th.getRunningJob();
  String lastReport="";
  SimpleDateFormat dateFormat=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss,SSS");
  long reportTime=System.currentTimeMillis();
  long maxReportInterval=HiveConf.getLongVar(job,HiveConf.ConfVars.HIVE_LOG_INCREMENTAL_PLAN_PROGRESS_INTERVAL);
  boolean fatal=false;
  StringBuilder errMsg=new StringBuilder();
  long pullInterval=HiveConf.getLongVar(job,HiveConf.ConfVars.HIVECOUNTERSPULLINTERVAL);
  boolean initializing=true;
  boolean initOutputPrinted=false;
  long cpuMsec=-1;
  int numMap=-1;
  int numReduce=-1;
  List<ClientStatsPublisher> clientStatPublishers=getClientStatPublishers();
  Heartbeater heartbeater=new Heartbeater(th.getTxnManager(),job);
  while (!rj.isComplete()) {
    try {
      Thread.sleep(pullInterval);
    }
 catch (    InterruptedException e) {
    }
    heartbeater.heartbeat();
    if (initializing && rj.getJobState() == JobStatus.PREP) {
      continue;
    }
 else {
      initializing=false;
    }
    if (!initOutputPrinted) {
      SessionState ss=SessionState.get();
      String logMapper;
      String logReducer;
      TaskReport[] mappers=jc.getMapTaskReports(rj.getID());
      if (mappers == null) {
        logMapper="no information for number of mappers; ";
      }
 else {
        numMap=mappers.length;
        if (ss != null) {
          ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_NUM_MAPPERS,Integer.toString(numMap));
        }
        logMapper="number of mappers: " + numMap + "; ";
      }
      TaskReport[] reducers=jc.getReduceTaskReports(rj.getID());
      if (reducers == null) {
        logReducer="no information for number of reducers. ";
      }
 else {
        numReduce=reducers.length;
        if (ss != null) {
          ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_NUM_REDUCERS,Integer.toString(numReduce));
        }
        logReducer="number of reducers: " + numReduce;
      }
      console.printInfo("Hadoop job information for " + getId() + ": "+ logMapper+ logReducer);
      initOutputPrinted=true;
    }
    RunningJob newRj=jc.getJob(rj.getID());
    if (newRj == null) {
      throw new IOException("Could not find status of job:" + rj.getID());
    }
 else {
      th.setRunningJob(newRj);
      rj=newRj;
    }
    if (fatal) {
      continue;
    }
    Counters ctrs=th.getCounters();
    if (fatal=checkFatalErrors(ctrs,errMsg)) {
      console.printError("[Fatal Error] " + errMsg.toString() + ". Killing the job.");
      rj.killJob();
      continue;
    }
    errMsg.setLength(0);
    updateCounters(ctrs,rj);
    if (clientStatPublishers.size() > 0 && ctrs != null) {
      Map<String,Double> exctractedCounters=extractAllCounterValues(ctrs);
      for (      ClientStatsPublisher clientStatPublisher : clientStatPublishers) {
        try {
          clientStatPublisher.run(exctractedCounters,rj.getID().toString());
        }
 catch (        RuntimeException runtimeException) {
          LOG.error("Exception " + runtimeException.getClass().getCanonicalName() + " thrown when running clientStatsPublishers. The stack trace is: ",runtimeException);
        }
      }
    }
    String report=" " + getId() + " map = "+ mapProgress+ "%,  reduce = "+ reduceProgress+ "%";
    if (!report.equals(lastReport) || System.currentTimeMillis() >= reportTime + maxReportInterval) {
      if (ctrs != null) {
        Counter counterCpuMsec=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","CPU_MILLISECONDS");
        if (counterCpuMsec != null) {
          long newCpuMSec=counterCpuMsec.getValue();
          if (newCpuMSec > 0) {
            cpuMsec=newCpuMSec;
            report+=", Cumulative CPU " + (cpuMsec / 1000D) + " sec";
          }
        }
      }
      String output=dateFormat.format(Calendar.getInstance().getTime()) + report;
      SessionState ss=SessionState.get();
      if (ss != null) {
        ss.getHiveHistory().setTaskCounters(SessionState.get().getQueryId(),getId(),ctrs);
        ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_HADOOP_PROGRESS,output);
        if (ss.getConf().getBoolVar(HiveConf.ConfVars.HIVE_LOG_INCREMENTAL_PLAN_PROGRESS)) {
          ss.getHiveHistory().progressTask(SessionState.get().getQueryId(),this.task);
          this.callBackObj.logPlanProgress(ss);
        }
      }
      console.printInfo(output);
      lastReport=report;
      reportTime=System.currentTimeMillis();
    }
  }
  if (cpuMsec > 0) {
    console.printInfo("MapReduce Total cumulative CPU time: " + Utilities.formatMsecToStr(cpuMsec));
  }
  boolean success;
  Counters ctrs=th.getCounters();
  if (fatal) {
    success=false;
  }
 else {
    if (checkFatalErrors(ctrs,errMsg)) {
      console.printError("[Fatal Error] " + errMsg.toString());
      success=false;
    }
 else {
      SessionState ss=SessionState.get();
      if (ss != null) {
        ss.getHiveHistory().setTaskCounters(SessionState.get().getQueryId(),getId(),ctrs);
      }
      success=rj.isSuccessful();
    }
  }
  if (ctrs != null) {
    Counter counterCpuMsec=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","CPU_MILLISECONDS");
    if (counterCpuMsec != null) {
      long newCpuMSec=counterCpuMsec.getValue();
      if (newCpuMSec > cpuMsec) {
        cpuMsec=newCpuMSec;
      }
    }
  }
  MapRedStats mapRedStats=new MapRedStats(numMap,numReduce,cpuMsec,success,rj.getID().toString());
  mapRedStats.setCounters(ctrs);
  updateCounters(ctrs,rj);
  SessionState ss=SessionState.get();
  if (ss != null) {
    this.callBackObj.logPlanProgress(ss);
  }
  return mapRedStats;
}
