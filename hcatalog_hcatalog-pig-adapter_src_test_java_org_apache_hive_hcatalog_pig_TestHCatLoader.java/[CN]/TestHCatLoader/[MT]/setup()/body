{
  File f=new File(TEST_WAREHOUSE_DIR);
  if (f.exists()) {
    FileUtil.fullyDelete(f);
  }
  if (!(new File(TEST_WAREHOUSE_DIR).mkdirs())) {
    throw new RuntimeException("Could not create " + TEST_WAREHOUSE_DIR);
  }
  HiveConf hiveConf=new HiveConf(this.getClass());
  hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname,"");
  hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname,"");
  hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname,"false");
  hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname,TEST_WAREHOUSE_DIR);
  driver=new Driver(hiveConf);
  SessionState.start(new CliSessionState(hiveConf));
  createTable(BASIC_TABLE,"a int, b string");
  createTable(COMPLEX_TABLE,"name string, studentid int, " + "contact struct<phno:string,email:string>, " + "currently_registered_courses array<string>, "+ "current_grades map<string,string>, "+ "phnos array<struct<phno:string,type:string>>");
  createTable(PARTITIONED_TABLE,"a int, b string","bkt string");
  createTable(SPECIFIC_SIZE_TABLE,"a int, b string");
  AllTypesTable.setupAllTypesTable(driver);
  int LOOP_SIZE=3;
  String[] input=new String[LOOP_SIZE * LOOP_SIZE];
  basicInputData=new HashMap<Integer,Pair<Integer,String>>();
  int k=0;
  for (int i=1; i <= LOOP_SIZE; i++) {
    String si=i + "";
    for (int j=1; j <= LOOP_SIZE; j++) {
      String sj="S" + j + "S";
      input[k]=si + "\t" + sj;
      basicInputData.put(k,new Pair<Integer,String>(i,sj));
      k++;
    }
  }
  HcatTestUtils.createTestDataFile(BASIC_FILE_NAME,input);
  HcatTestUtils.createTestDataFile(COMPLEX_FILE_NAME,new String[]{});
  PigServer server=new PigServer(ExecType.LOCAL);
  server.setBatchOn();
  int i=0;
  server.registerQuery("A = load '" + BASIC_FILE_NAME + "' as (a:int, b:chararray);",++i);
  server.registerQuery("store A into '" + BASIC_TABLE + "' using org.apache.hive.hcatalog.pig.HCatStorer();",++i);
  server.registerQuery("store A into '" + SPECIFIC_SIZE_TABLE + "' using org.apache.hive.hcatalog.pig.HCatStorer();",++i);
  server.registerQuery("B = foreach A generate a,b;",++i);
  server.registerQuery("B2 = filter B by a < 2;",++i);
  server.registerQuery("store B2 into '" + PARTITIONED_TABLE + "' using org.apache.hive.hcatalog.pig.HCatStorer('bkt=0');",++i);
  server.registerQuery("C = foreach A generate a,b;",++i);
  server.registerQuery("C2 = filter C by a >= 2;",++i);
  server.registerQuery("store C2 into '" + PARTITIONED_TABLE + "' using org.apache.hive.hcatalog.pig.HCatStorer('bkt=1');",++i);
  server.registerQuery("D = load '" + COMPLEX_FILE_NAME + "' as (name:chararray, studentid:int, contact:tuple(phno:chararray,email:chararray), currently_registered_courses:bag{innertup:tuple(course:chararray)}, current_grades:map[ ] , phnos :bag{innertup:tuple(phno:chararray,type:chararray)});",++i);
  server.registerQuery("store D into '" + COMPLEX_TABLE + "' using org.apache.hive.hcatalog.pig.HCatStorer();",++i);
  server.executeBatch();
}
