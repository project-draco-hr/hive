{
  StructObjectInspector inspector;
synchronized (TestOrcFile.class) {
    inspector=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
  }
  JobConf conf=createMockExecutionEnvironment(workDir,new Path("mock:///"),"combinationAcid",inspector,false);
  Path partDir=new Path(conf.get("mapred.input.dir"));
  OrcRecordUpdater writer=new OrcRecordUpdater(partDir,new AcidOutputFormat.Options(conf).maximumTransactionId(10).writingBase(true).bucket(0).inspector(inspector));
  for (int i=0; i < 10; ++i) {
    writer.insert(10,new MyRow(i,2 * i));
  }
  WriterImpl baseWriter=(WriterImpl)writer.getWriter();
  writer.close(false);
  MockOutputStream outputStream=(MockOutputStream)baseWriter.getStream();
  int length0=outputStream.file.length;
  writer=new OrcRecordUpdater(partDir,new AcidOutputFormat.Options(conf).maximumTransactionId(10).writingBase(true).bucket(1).inspector(inspector));
  for (int i=10; i < 20; ++i) {
    writer.insert(10,new MyRow(i,2 * i));
  }
  baseWriter=(WriterImpl)writer.getWriter();
  writer.close(false);
  outputStream=(MockOutputStream)baseWriter.getStream();
  outputStream.setBlocks(new MockBlock("host1","host2"));
  HiveInputFormat<?,?> inputFormat=new CombineHiveInputFormat<WritableComparable,Writable>();
  try {
    InputSplit[] splits=inputFormat.getSplits(conf,1);
    assertTrue("shouldn't reach here",false);
  }
 catch (  IOException ioe) {
    assertEquals("CombineHiveInputFormat is incompatible" + "  with ACID tables. Please set hive.input.format=org.apache.hadoop" + ".hive.ql.io.HiveInputFormat",ioe.getMessage());
  }
}
