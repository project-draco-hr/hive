{
  Properties properties=new Properties();
  properties.setProperty("columns","z,r");
  properties.setProperty("columns.types","int:struct<x:int,y:int>");
  StructObjectInspector inspector;
synchronized (TestOrcFile.class) {
    inspector=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(NestedRow.class,ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
  }
  SerDe serde=new OrcSerde();
  OutputFormat<?,?> outFormat=new OrcOutputFormat();
  conf.setInt("mapred.max.split.size",50);
  RecordWriter writer=outFormat.getRecordWriter(fs,conf,testFilePath.toString(),Reporter.NULL);
  writer.write(NullWritable.get(),serde.serialize(new NestedRow(1,2,3),inspector));
  writer.write(NullWritable.get(),serde.serialize(new NestedRow(4,5,6),inspector));
  writer.write(NullWritable.get(),serde.serialize(new NestedRow(7,8,9),inspector));
  writer.close(Reporter.NULL);
  serde=new OrcSerde();
  SearchArgument sarg=SearchArgumentFactory.newBuilder().startAnd().lessThan("z",PredicateLeaf.Type.LONG,new Long(0)).end().build();
  conf.set("sarg.pushdown",toKryo(sarg));
  conf.set("hive.io.file.readcolumn.names","z,r");
  SerDeUtils.initializeSerDe(serde,conf,properties,null);
  inspector=(StructObjectInspector)serde.getObjectInspector();
  InputFormat<?,?> in=new OrcInputFormat();
  FileInputFormat.setInputPaths(conf,testFilePath.toString());
  InputSplit[] splits=in.getSplits(conf,1);
  assertEquals(0,splits.length);
}
