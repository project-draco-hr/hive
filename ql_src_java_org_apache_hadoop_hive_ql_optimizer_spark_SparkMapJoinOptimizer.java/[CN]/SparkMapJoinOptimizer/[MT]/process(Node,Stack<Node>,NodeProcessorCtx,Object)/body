{
  OptimizeSparkProcContext context=(OptimizeSparkProcContext)procCtx;
  HiveConf conf=context.getConf();
  ParseContext parseContext=context.getParseContext();
  JoinOperator joinOp=(JoinOperator)nd;
  int numBuckets=1;
  LOG.info("Estimated number of buckets " + numBuckets);
  int mapJoinConversionPos=getMapJoinConversionPos(joinOp,context,numBuckets);
  LOG.info("Convert to non-bucketed map join");
  mapJoinConversionPos=getMapJoinConversionPos(joinOp,context,1);
  MapJoinOperator mapJoinOp=convertJoinMapJoin(joinOp,context,mapJoinConversionPos);
  mapJoinOp.setOpTraits(new OpTraits(null,-1,null));
  mapJoinOp.setStatistics(joinOp.getStatistics());
  for (  Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {
    setAllChildrenTraitsToNull(childOp);
  }
  return null;
}
