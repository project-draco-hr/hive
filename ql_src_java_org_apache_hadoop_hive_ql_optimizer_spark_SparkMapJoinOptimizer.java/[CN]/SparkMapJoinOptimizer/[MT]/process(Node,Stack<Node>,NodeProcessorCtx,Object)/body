{
  OptimizeSparkProcContext context=(OptimizeSparkProcContext)procCtx;
  HiveConf conf=context.getConf();
  JoinOperator joinOp=(JoinOperator)nd;
  if (!conf.getBoolVar(HiveConf.ConfVars.HIVECONVERTJOIN)) {
    return null;
  }
  LOG.info("Check if it can be converted to map join");
  long[] mapJoinInfo=getMapJoinConversionInfo(joinOp,context);
  int mapJoinConversionPos=(int)mapJoinInfo[0];
  if (mapJoinConversionPos < 0) {
    return null;
  }
  int numBuckets=-1;
  List<List<String>> bucketColNames=null;
  LOG.info("Convert to non-bucketed map join");
  MapJoinOperator mapJoinOp=convertJoinMapJoin(joinOp,context,mapJoinConversionPos);
  if (conf.getBoolVar(HiveConf.ConfVars.HIVEOPTBUCKETMAPJOIN)) {
    LOG.info("Check if it can be converted to bucketed map join");
    numBuckets=convertJoinBucketMapJoin(joinOp,mapJoinOp,context,mapJoinConversionPos);
    if (numBuckets > 1) {
      LOG.info("Converted to map join with " + numBuckets + " buckets");
      bucketColNames=joinOp.getOpTraits().getBucketColNames();
      mapJoinInfo[2]/=numBuckets;
    }
 else {
      LOG.info("Can not convert to bucketed map join");
    }
  }
  OpTraits opTraits=new OpTraits(bucketColNames,numBuckets,null,joinOp.getOpTraits().getNumReduceSinks());
  mapJoinOp.setOpTraits(opTraits);
  mapJoinOp.setStatistics(joinOp.getStatistics());
  setNumberOfBucketsOnChildren(mapJoinOp);
  context.getMjOpSizes().put(mapJoinOp,mapJoinInfo[1] + mapJoinInfo[2]);
  return mapJoinOp;
}
