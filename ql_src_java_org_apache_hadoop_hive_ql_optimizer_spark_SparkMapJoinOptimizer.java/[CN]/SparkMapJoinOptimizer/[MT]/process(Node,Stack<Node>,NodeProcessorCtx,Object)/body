{
  OptimizeSparkProcContext context=(OptimizeSparkProcContext)procCtx;
  HiveConf conf=context.getConf();
  ParseContext parseContext=context.getParseContext();
  JoinOperator joinOp=(JoinOperator)nd;
  if (!conf.getBoolVar(HiveConf.ConfVars.HIVECONVERTJOIN)) {
    return null;
  }
  int numBuckets=1;
  LOG.info("Estimated number of buckets " + numBuckets);
  LOG.info("Convert to non-bucketed map join");
  ObjectPair<Integer,Long> mapJoinInfo=getMapJoinConversionInfo(joinOp,context,1);
  int mapJoinConversionPos=mapJoinInfo.getFirst();
  if (mapJoinConversionPos < 0) {
    return null;
  }
  MapJoinOperator mapJoinOp=convertJoinMapJoin(joinOp,context,mapJoinConversionPos);
  mapJoinOp.setOpTraits(new OpTraits(null,-1,null));
  mapJoinOp.setStatistics(joinOp.getStatistics());
  for (  Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {
    setAllChildrenTraitsToNull(childOp);
  }
  context.getMjOpSizes().put(mapJoinOp,mapJoinInfo.getSecond());
  return null;
}
