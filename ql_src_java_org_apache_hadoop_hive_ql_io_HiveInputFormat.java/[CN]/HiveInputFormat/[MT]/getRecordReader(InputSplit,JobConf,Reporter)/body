{
  HiveInputSplit hsplit=(HiveInputSplit)split;
  InputSplit inputSplit=hsplit.getInputSplit();
  String inputFormatClassName=null;
  Class inputFormatClass=null;
  try {
    inputFormatClassName=hsplit.inputFormatClassName();
    inputFormatClass=job.getClassByName(inputFormatClassName);
  }
 catch (  Exception e) {
    throw new IOException("cannot find class " + inputFormatClassName);
  }
  InputFormat inputFormat=getInputFormatFromCache(inputFormatClass,job);
  if (this.mrwork == null)   init(job);
  JobConf jobConf=new JobConf(job);
  ArrayList<String> aliases=new ArrayList<String>();
  Iterator<Entry<String,ArrayList<String>>> iterator=this.mrwork.getPathToAliases().entrySet().iterator();
  String splitPath=hsplit.getPath().toString();
  String splitPathWithNoSchema=hsplit.getPath().toUri().getPath();
  while (iterator.hasNext()) {
    Entry<String,ArrayList<String>> entry=iterator.next();
    String key=entry.getKey();
    if (splitPath.startsWith(key) || splitPathWithNoSchema.startsWith(key)) {
      ArrayList<String> list=entry.getValue();
      for (      String val : list)       aliases.add(val);
    }
  }
  for (  String alias : aliases) {
    Operator<? extends Serializable> op=this.mrwork.getAliasToWork().get(alias);
    if (op instanceof TableScanOperator) {
      TableScanOperator tableScan=(TableScanOperator)op;
      ArrayList<Integer> list=tableScan.getNeededColumnIDs();
      if (list != null)       HiveFileFormatUtils.setReadColumnIDs(jobConf,list);
 else       HiveFileFormatUtils.setFullyReadColumns(jobConf);
    }
  }
  return new HiveRecordReader(inputFormat.getRecordReader(inputSplit,jobConf,reporter));
}
