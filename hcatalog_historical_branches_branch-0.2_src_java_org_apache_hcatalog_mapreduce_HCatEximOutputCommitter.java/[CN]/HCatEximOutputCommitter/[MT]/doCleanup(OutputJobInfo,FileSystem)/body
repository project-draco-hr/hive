{
  try {
    Table ttable=jobInfo.getTable();
    org.apache.hadoop.hive.ql.metadata.Table table=new org.apache.hadoop.hive.ql.metadata.Table(ttable);
    StorageDescriptor tblSD=ttable.getSd();
    Path tblPath=new Path(tblSD.getLocation());
    Path path=new Path(tblPath,"_metadata");
    List<Partition> tpartitions=null;
    try {
      Map.Entry<org.apache.hadoop.hive.metastore.api.Table,List<Partition>> rv=EximUtil.readMetaData(fs,path);
      tpartitions=rv.getValue();
    }
 catch (    IOException e) {
    }
    List<org.apache.hadoop.hive.ql.metadata.Partition> partitions=new ArrayList<org.apache.hadoop.hive.ql.metadata.Partition>();
    if (tpartitions != null) {
      for (      Partition tpartition : tpartitions) {
        partitions.add(new org.apache.hadoop.hive.ql.metadata.Partition(table,tpartition));
      }
    }
    if (!table.getPartitionKeys().isEmpty()) {
      Map<String,String> partitionValues=jobInfo.getTableInfo().getPartitionValues();
      org.apache.hadoop.hive.ql.metadata.Partition partition=new org.apache.hadoop.hive.ql.metadata.Partition(table,partitionValues,new Path(tblPath,Warehouse.makePartPath(partitionValues)));
      partition.getTPartition().setParameters(table.getParameters());
      partitions.add(partition);
    }
    EximUtil.createExportDump(fs,path,(table),partitions);
  }
 catch (  SemanticException e) {
    throw new HCatException(ErrorType.ERROR_PUBLISHING_PARTITION,e);
  }
catch (  HiveException e) {
    throw new HCatException(ErrorType.ERROR_PUBLISHING_PARTITION,e);
  }
catch (  MetaException e) {
    throw new HCatException(ErrorType.ERROR_PUBLISHING_PARTITION,e);
  }
}
