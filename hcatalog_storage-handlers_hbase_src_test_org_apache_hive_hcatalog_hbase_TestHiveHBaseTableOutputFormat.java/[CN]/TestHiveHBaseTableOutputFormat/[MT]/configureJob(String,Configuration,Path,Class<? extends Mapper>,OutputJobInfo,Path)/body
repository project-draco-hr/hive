{
  try {
    HiveConf hiveConf=HCatUtil.getHiveConf(conf);
    HiveMetaStoreClient client=HCatUtil.getHiveClient(hiveConf);
    Table table=client.getTable(outputJobInfo.getDatabaseName(),outputJobInfo.getTableName());
    StorageDescriptor tblSD=table.getSd();
    if (tblSD == null) {
      throw new HCatException("Cannot construct partition info from an empty storage descriptor.");
    }
    HCatSchema tableSchema=new HCatSchema(HCatUtil.getHCatFieldSchemaList(tblSD.getCols()));
    outputJobInfo.setOutputSchema(tableSchema);
  }
 catch (  Exception e) {
    if (e instanceof HCatException) {
      throw (HCatException)e;
    }
 else {
      throw new HCatException(ErrorType.ERROR_SET_OUTPUT,e);
    }
  }
  conf.set(HBaseSerDe.HBASE_TABLE_NAME,outputJobInfo.getDatabaseName() + "." + outputJobInfo.getTableName());
  conf.set(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,outputJobInfo.getDatabaseName() + "." + outputJobInfo.getTableName());
  conf.set(TableOutputFormat.OUTPUT_TABLE,outputJobInfo.getDatabaseName() + "." + outputJobInfo.getTableName());
  conf.set(HCatConstants.HCAT_DEFAULT_TOPIC_PREFIX + ".hbase.mapreduce.outputTableName",outputJobInfo.getDatabaseName() + "." + outputJobInfo.getTableName());
  conf.set(HCatConstants.HCAT_KEY_OUTPUT_INFO,HCatUtil.serialize(outputJobInfo));
  Job job=new Job(conf,jobName);
  job.setWorkingDirectory(workingDir);
  job.setJarByClass(this.getClass());
  job.setMapperClass(mapperClass);
  job.setInputFormatClass(TextInputFormat.class);
  TextInputFormat.setInputPaths(job,inputPath);
  job.setOutputFormatClass(HCatOutputFormat.class);
  HCatOutputFormat.setOutput(job,outputJobInfo);
  job.setMapOutputKeyClass(BytesWritable.class);
  job.setMapOutputValueClass(HCatRecord.class);
  job.setOutputKeyClass(BytesWritable.class);
  job.setOutputValueClass(HCatRecord.class);
  job.setNumReduceTasks(0);
  return job;
}
