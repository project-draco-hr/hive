{
  PerfLogger perfLogger=PerfLogger.getPerfLogger();
  perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.INPUT_SUMMARY);
  long[] summary={0,0,0};
  final List<String> pathNeedProcess=new ArrayList<String>();
synchronized (INPUT_SUMMARY_LOCK) {
    for (    String path : work.getPathToAliases().keySet()) {
      Path p=new Path(path);
      if (filter != null && !filter.accept(p)) {
        continue;
      }
      ContentSummary cs=ctx.getCS(path);
      if (cs == null) {
        if (path == null) {
          continue;
        }
        pathNeedProcess.add(path);
      }
 else {
        summary[0]+=cs.getLength();
        summary[1]+=cs.getFileCount();
        summary[2]+=cs.getDirectoryCount();
      }
    }
    final Map<String,ContentSummary> resultMap=new ConcurrentHashMap<String,ContentSummary>();
    ArrayList<Future<?>> results=new ArrayList<Future<?>>();
    final ThreadPoolExecutor executor;
    int maxThreads=ctx.getConf().getInt("mapred.dfsclient.parallelism.max",0);
    if (pathNeedProcess.size() > 1 && maxThreads > 1) {
      int numExecutors=Math.min(pathNeedProcess.size(),maxThreads);
      LOG.info("Using " + numExecutors + " threads for getContentSummary");
      executor=new ThreadPoolExecutor(numExecutors,numExecutors,60,TimeUnit.SECONDS,new LinkedBlockingQueue<Runnable>());
    }
 else {
      executor=null;
    }
    HiveInterruptCallback interrup=HiveInterruptUtils.add(new HiveInterruptCallback(){
      @Override public void interrupt(){
        for (        String path : pathNeedProcess) {
          try {
            new Path(path).getFileSystem(ctx.getConf()).close();
          }
 catch (          IOException ignore) {
            LOG.debug(ignore);
          }
        }
        if (executor != null) {
          executor.shutdownNow();
        }
      }
    }
);
    try {
      Configuration conf=ctx.getConf();
      JobConf jobConf=new JobConf(conf);
      for (      String path : pathNeedProcess) {
        final Path p=new Path(path);
        final String pathStr=path;
        final Configuration myConf=conf;
        final JobConf myJobConf=jobConf;
        final Map<String,Operator<?>> aliasToWork=work.getAliasToWork();
        final Map<String,ArrayList<String>> pathToAlias=work.getPathToAliases();
        final PartitionDesc partDesc=work.getPathToPartitionInfo().get(p.toString());
        Runnable r=new Runnable(){
          @Override public void run(){
            try {
              Class<? extends InputFormat> inputFormatCls=partDesc.getInputFileFormatClass();
              InputFormat inputFormatObj=HiveInputFormat.getInputFormatFromCache(inputFormatCls,myJobConf);
              if (inputFormatObj instanceof ContentSummaryInputFormat) {
                ContentSummaryInputFormat cs=(ContentSummaryInputFormat)inputFormatObj;
                resultMap.put(pathStr,cs.getContentSummary(p,myJobConf));
                return;
              }
              HiveStorageHandler handler=HiveUtils.getStorageHandler(myConf,partDesc.getOverlayedProperties().getProperty(hive_metastoreConstants.META_TABLE_STORAGE));
              if (handler instanceof InputEstimator) {
                long total=0;
                TableDesc tableDesc=partDesc.getTableDesc();
                InputEstimator estimator=(InputEstimator)handler;
                for (                String alias : HiveFileFormatUtils.doGetAliasesFromPath(pathToAlias,p)) {
                  JobConf jobConf=new JobConf(myJobConf);
                  TableScanOperator scanOp=(TableScanOperator)aliasToWork.get(alias);
                  Utilities.setColumnNameList(jobConf,scanOp,true);
                  Utilities.setColumnTypeList(jobConf,scanOp,true);
                  PlanUtils.configureInputJobPropertiesForStorageHandler(tableDesc);
                  Utilities.copyTableJobPropertiesToConf(tableDesc,jobConf);
                  total+=estimator.estimate(myJobConf,scanOp,-1).getTotalLength();
                }
                resultMap.put(pathStr,new ContentSummary(total,-1,-1));
              }
              FileSystem fs=p.getFileSystem(myConf);
              resultMap.put(pathStr,fs.getContentSummary(p));
            }
 catch (            Exception e) {
              LOG.info("Cannot get size of " + pathStr + ". Safely ignored.");
            }
          }
        }
;
        if (executor == null) {
          r.run();
        }
 else {
          Future<?> result=executor.submit(r);
          results.add(result);
        }
      }
      if (executor != null) {
        for (        Future<?> result : results) {
          boolean executorDone=false;
          do {
            try {
              result.get();
              executorDone=true;
            }
 catch (            InterruptedException e) {
              LOG.info("Interrupted when waiting threads: ",e);
              Thread.currentThread().interrupt();
              break;
            }
catch (            ExecutionException e) {
              throw new IOException(e);
            }
          }
 while (!executorDone);
        }
        executor.shutdown();
      }
      HiveInterruptUtils.checkInterrupted();
      for (      Map.Entry<String,ContentSummary> entry : resultMap.entrySet()) {
        ContentSummary cs=entry.getValue();
        summary[0]+=cs.getLength();
        summary[1]+=cs.getFileCount();
        summary[2]+=cs.getDirectoryCount();
        ctx.addCS(entry.getKey(),cs);
        LOG.info("Cache Content Summary for " + entry.getKey() + " length: "+ cs.getLength()+ " file count: "+ cs.getFileCount()+ " directory count: "+ cs.getDirectoryCount());
      }
      perfLogger.PerfLogEnd(CLASS_NAME,PerfLogger.INPUT_SUMMARY);
      return new ContentSummary(summary[0],summary[1],summary[2]);
    }
  finally {
      HiveInterruptUtils.remove(interrup);
    }
  }
}
