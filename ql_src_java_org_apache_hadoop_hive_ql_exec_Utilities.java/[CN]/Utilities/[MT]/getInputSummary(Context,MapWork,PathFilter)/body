{
  long[] summary={0,0,0};
  List<String> pathNeedProcess=new ArrayList<String>();
synchronized (INPUT_SUMMARY_LOCK) {
    for (    String path : work.getPathToAliases().keySet()) {
      Path p=new Path(path);
      if (filter != null && !filter.accept(p)) {
        continue;
      }
      ContentSummary cs=ctx.getCS(path);
      if (cs == null) {
        if (path == null) {
          continue;
        }
        pathNeedProcess.add(path);
      }
 else {
        summary[0]+=cs.getLength();
        summary[1]+=cs.getFileCount();
        summary[2]+=cs.getDirectoryCount();
      }
    }
    final Map<String,ContentSummary> resultMap=new ConcurrentHashMap<String,ContentSummary>();
    ArrayList<Future<?>> results=new ArrayList<Future<?>>();
    final ThreadPoolExecutor executor;
    int maxThreads=ctx.getConf().getInt("mapred.dfsclient.parallelism.max",0);
    if (pathNeedProcess.size() > 1 && maxThreads > 1) {
      int numExecutors=Math.min(pathNeedProcess.size(),maxThreads);
      LOG.info("Using " + numExecutors + " threads for getContentSummary");
      executor=new ThreadPoolExecutor(numExecutors,numExecutors,60,TimeUnit.SECONDS,new LinkedBlockingQueue<Runnable>());
    }
 else {
      executor=null;
    }
    HiveInterruptCallback interrup=HiveInterruptUtils.add(new HiveInterruptCallback(){
      @Override public void interrupt(){
        if (executor != null) {
          executor.shutdownNow();
        }
      }
    }
);
    try {
      Configuration conf=ctx.getConf();
      JobConf jobConf=new JobConf(conf);
      for (      String path : pathNeedProcess) {
        final Path p=new Path(path);
        final String pathStr=path;
        final Configuration myConf=conf;
        final JobConf myJobConf=jobConf;
        final PartitionDesc partDesc=work.getPathToPartitionInfo().get(p.toString());
        Runnable r=new Runnable(){
          public void run(){
            try {
              ContentSummary resultCs;
              Class<? extends InputFormat> inputFormatCls=partDesc.getInputFileFormatClass();
              InputFormat inputFormatObj=HiveInputFormat.getInputFormatFromCache(inputFormatCls,myJobConf);
              if (inputFormatObj instanceof ContentSummaryInputFormat) {
                resultCs=((ContentSummaryInputFormat)inputFormatObj).getContentSummary(p,myJobConf);
              }
 else {
                FileSystem fs=p.getFileSystem(myConf);
                resultCs=fs.getContentSummary(p);
              }
              resultMap.put(pathStr,resultCs);
            }
 catch (            IOException e) {
              LOG.info("Cannot get size of " + pathStr + ". Safely ignored.");
            }
          }
        }
;
        if (executor == null) {
          r.run();
        }
 else {
          Future<?> result=executor.submit(r);
          results.add(result);
        }
      }
      if (executor != null) {
        for (        Future<?> result : results) {
          boolean executorDone=false;
          do {
            try {
              result.get();
              executorDone=true;
            }
 catch (            InterruptedException e) {
              LOG.info("Interrupted when waiting threads: ",e);
              Thread.currentThread().interrupt();
              break;
            }
catch (            ExecutionException e) {
              throw new IOException(e);
            }
          }
 while (!executorDone);
        }
        executor.shutdown();
      }
      HiveInterruptUtils.checkInterrupted();
      for (      Map.Entry<String,ContentSummary> entry : resultMap.entrySet()) {
        ContentSummary cs=entry.getValue();
        summary[0]+=cs.getLength();
        summary[1]+=cs.getFileCount();
        summary[2]+=cs.getDirectoryCount();
        ctx.addCS(entry.getKey(),cs);
        LOG.info("Cache Content Summary for " + entry.getKey() + " length: "+ cs.getLength()+ " file count: "+ cs.getFileCount()+ " directory count: "+ cs.getDirectoryCount());
      }
      return new ContentSummary(summary[0],summary[1],summary[2]);
    }
  finally {
      HiveInterruptUtils.remove(interrup);
    }
  }
}
