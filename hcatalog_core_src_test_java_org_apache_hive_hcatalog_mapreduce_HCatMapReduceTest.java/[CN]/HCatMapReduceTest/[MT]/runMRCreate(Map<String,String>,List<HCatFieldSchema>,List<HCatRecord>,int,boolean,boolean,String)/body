{
  writeRecords=records;
  MapCreate.writeCount=0;
  Configuration conf=new Configuration();
  Job job=new Job(conf,"hcat mapreduce write test");
  job.setJarByClass(this.getClass());
  job.setMapperClass(HCatMapReduceTest.MapCreate.class);
  job.setInputFormatClass(TextInputFormat.class);
  if (asSingleMapTask) {
    Path path=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceInput");
    createInputFile(path,writeCount);
    TextInputFormat.setInputPaths(job,path);
  }
 else {
    Path path=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceInput");
    createInputFile(path,writeCount / 2);
    Path path2=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceInput2");
    createInputFile(path2,(writeCount - writeCount / 2));
    TextInputFormat.setInputPaths(job,path,path2);
  }
  job.setOutputFormatClass(HCatOutputFormat.class);
  OutputJobInfo outputJobInfo=OutputJobInfo.create(dbName,tableName,partitionValues);
  if (customDynamicPathPattern != null) {
    job.getConfiguration().set(HCatConstants.HCAT_DYNAMIC_CUSTOM_PATTERN,customDynamicPathPattern);
  }
  HCatOutputFormat.setOutput(job,outputJobInfo);
  job.setMapOutputKeyClass(BytesWritable.class);
  job.setMapOutputValueClass(DefaultHCatRecord.class);
  job.setNumReduceTasks(0);
  HCatOutputFormat.setSchema(job,new HCatSchema(partitionColumns));
  boolean success=job.waitForCompletion(true);
  if (partitionValues != null) {
    assertTrue(job.getCounters().getGroup("FileSystemCounters").findCounter("FILE_BYTES_READ").getValue() > 0);
  }
  if (!HCatUtil.isHadoop23()) {
    if (success) {
      new FileOutputCommitterContainer(job,null).commitJob(job);
    }
 else {
      new FileOutputCommitterContainer(job,null).abortJob(job,JobStatus.State.FAILED);
    }
  }
  if (assertWrite) {
    Assert.assertEquals(writeCount,MapCreate.writeCount);
  }
  if (isTableExternal()) {
    externalTableLocation=outputJobInfo.getTableInfo().getTableLocation();
  }
  return job;
}
