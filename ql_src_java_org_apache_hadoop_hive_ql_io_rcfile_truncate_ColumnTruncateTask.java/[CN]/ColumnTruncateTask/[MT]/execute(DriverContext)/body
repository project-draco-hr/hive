{
  HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,BucketizedHiveInputFormat.class.getName());
  success=true;
  HiveFileFormatUtils.prepareJobOutput(job);
  job.setOutputFormat(HiveOutputFormatImpl.class);
  job.setMapperClass(work.getMapperClass());
  Context ctx=driverContext.getCtx();
  boolean ctxCreated=false;
  try {
    if (ctx == null) {
      ctx=new Context(job);
      ctxCreated=true;
    }
  }
 catch (  IOException e) {
    e.printStackTrace();
    console.printError("Error launching map-reduce job","\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  job.setMapOutputKeyClass(NullWritable.class);
  job.setMapOutputValueClass(NullWritable.class);
  if (work.getNumMapTasks() != null) {
    job.setNumMapTasks(work.getNumMapTasks());
  }
  job.setNumReduceTasks(0);
  if (work.getMinSplitSize() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZE,work.getMinSplitSize().longValue());
  }
  if (work.getInputformat() != null) {
    HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,work.getInputformat());
  }
  String inpFormat=HiveConf.getVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT);
  LOG.info("Using " + inpFormat);
  try {
    job.setInputFormat(JavaUtils.loadClass(inpFormat));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage(),e);
  }
  Path outputPath=this.work.getOutputDir();
  Path tempOutPath=Utilities.toTempPath(outputPath);
  try {
    FileSystem fs=tempOutPath.getFileSystem(job);
    if (!fs.exists(tempOutPath)) {
      fs.mkdirs(tempOutPath);
    }
  }
 catch (  IOException e) {
    console.printError("Can't make path " + outputPath + " : "+ e.getMessage());
    return 6;
  }
  job.setOutputKeyClass(NullWritable.class);
  job.setOutputValueClass(NullWritable.class);
  int returnVal=0;
  RunningJob rj=null;
  boolean noName=StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));
  String jobName=null;
  if (noName && this.getQueryPlan() != null) {
    int maxlen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
    jobName=Utilities.abbreviate(this.getQueryPlan().getQueryStr(),maxlen - 6);
  }
  if (noName) {
    job.set(MRJobConfig.JOB_NAME,jobName != null ? jobName : "JOB" + Utilities.randGen.nextInt());
  }
  try {
    addInputPaths(job,work);
    MapredWork mrWork=new MapredWork();
    mrWork.setMapWork(work);
    Utilities.setMapRedWork(job,mrWork,ctx.getMRTmpPath());
    String pwd=HiveConf.getVar(job,HiveConf.ConfVars.METASTOREPWD);
    if (pwd != null) {
      HiveConf.setVar(job,HiveConf.ConfVars.METASTOREPWD,"HIVE");
    }
    JobClient jc=new JobClient(job);
    String addedJars=Utilities.getResourceFiles(job,SessionState.ResourceType.JAR);
    if (!addedJars.isEmpty()) {
      job.set("tmpjars",addedJars);
    }
    Throttle.checkJobTracker(job,LOG);
    rj=jc.submitJob(job);
    returnVal=jobExecHelper.progress(rj,jc,null);
    success=(returnVal == 0);
  }
 catch (  Exception e) {
    e.printStackTrace();
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    try {
      if (ctxCreated) {
        ctx.clear();
      }
      if (rj != null) {
        if (returnVal != 0) {
          rj.killJob();
        }
        jobID=rj.getID().toString();
      }
      ColumnTruncateMapper.jobClose(outputPath,success,job,console,work.getDynPartCtx(),null);
    }
 catch (    Exception e) {
      LOG.warn("Failed while cleaning up ",e);
    }
 finally {
      HadoopJobExecHelper.runningJobs.remove(rj);
    }
  }
  return (returnVal);
}
