{
  HCatSplit hcatSplit=(HCatSplit)split;
  PartInfo partitionInfo=hcatSplit.getPartitionInfo();
  JobContext jobContext=taskContext;
  HCatStorageHandler storageHandler=HCatUtil.getStorageHandler(jobContext.getConfiguration(),partitionInfo);
  JobConf jobConf=HCatUtil.getJobConfFromContext(jobContext);
  Class inputFormatClass=storageHandler.getInputFormatClass();
  org.apache.hadoop.mapred.InputFormat inputFormat=getMapRedInputFormat(jobConf,inputFormatClass);
  Map<String,String> jobProperties=partitionInfo.getJobProperties();
  HCatUtil.copyJobPropertiesToJobConf(jobProperties,jobConf);
  Reporter reporter=InternalUtil.createReporter(taskContext);
  org.apache.hadoop.mapred.RecordReader recordReader=inputFormat.getRecordReader(hcatSplit.getBaseSplit(),jobConf,reporter);
  SerDe serde;
  try {
    serde=ReflectionUtils.newInstance(storageHandler.getSerDeClass(),jobContext.getConfiguration());
    Configuration conf=storageHandler.getConf();
    InternalUtil.initializeInputSerDe(serde,conf,partitionInfo.getTableInfo(),partitionInfo.getPartitionSchema());
  }
 catch (  Exception e) {
    throw new IOException("Unable to create objectInspector " + "for serde class " + storageHandler.getSerDeClass().getName() + e);
  }
  Map<String,String> valuesNotInDataCols=getColValsNotInDataColumns(getOutputSchema(jobContext),partitionInfo);
  HCatRecordReader hcatRecordReader=new HCatRecordReader(storageHandler,recordReader,serde,valuesNotInDataCols);
  return hcatRecordReader;
}
