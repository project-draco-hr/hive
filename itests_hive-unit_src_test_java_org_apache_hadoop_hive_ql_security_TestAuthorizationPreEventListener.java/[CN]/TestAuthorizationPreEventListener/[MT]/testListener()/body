{
  String dbName="hive3705";
  String tblName="tmptbl";
  String renamed="tmptbl2";
  int listSize=0;
  List<AuthCallContext> authCalls=DummyHiveMetastoreAuthorizationProvider.authCalls;
  assertEquals(authCalls.size(),listSize);
  driver.run("create database " + dbName);
  listSize++;
  Database dbFromEvent=(Database)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.DB);
  Database db=msc.getDatabase(dbName);
  validateCreateDb(db,dbFromEvent);
  driver.run("use " + dbName);
  driver.run(String.format("create table %s (a string) partitioned by (b string)",tblName));
  listSize=authCalls.size();
  Table tblFromEvent=((org.apache.hadoop.hive.ql.metadata.Table)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE)).getTTable();
  Table tbl=msc.getTable(dbName,tblName);
  validateCreateTable(tbl,tblFromEvent);
  driver.run("alter table tmptbl add partition (b='2011')");
  listSize=authCalls.size();
  Partition ptnFromEvent=((org.apache.hadoop.hive.ql.metadata.Partition)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION)).getTPartition();
  Partition part=msc.getPartition("hive3705","tmptbl","b=2011");
  validateAddPartition(part,ptnFromEvent);
  driver.run(String.format("alter table %s touch partition (%s)",tblName,"b='2011'"));
  listSize=authCalls.size();
  Partition ptnFromEventAfterAlter=((org.apache.hadoop.hive.ql.metadata.Partition)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION)).getTPartition();
  Partition modifiedP=msc.getPartition(dbName,tblName,"b=2011");
  validateAlterPartition(part,modifiedP,ptnFromEventAfterAlter.getDbName(),ptnFromEventAfterAlter.getTableName(),ptnFromEventAfterAlter.getValues(),ptnFromEventAfterAlter);
  List<String> part_vals=new ArrayList<String>();
  part_vals.add("c=2012");
  listSize=authCalls.size();
  Partition newPart=msc.appendPartition(dbName,tblName,part_vals);
  listSize++;
  Partition newPtnFromEvent=((org.apache.hadoop.hive.ql.metadata.Partition)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION)).getTPartition();
  validateAddPartition(newPart,newPtnFromEvent);
  driver.run(String.format("alter table %s rename to %s",tblName,renamed));
  listSize=authCalls.size();
  Table renamedTableFromEvent=((org.apache.hadoop.hive.ql.metadata.Table)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE)).getTTable();
  Table renamedTable=msc.getTable(dbName,renamed);
  validateAlterTable(tbl,renamedTable,renamedTableFromEvent,renamedTable);
  assertFalse(tbl.getTableName().equals(renamedTable.getTableName()));
  driver.run(String.format("alter table %s rename to %s",renamed,tblName));
  driver.run(String.format("alter table %s drop partition (b='2011')",tblName));
  listSize=authCalls.size();
  Partition ptnFromDropPartition=((org.apache.hadoop.hive.ql.metadata.Partition)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION)).getTPartition();
  validateDropPartition(modifiedP,ptnFromDropPartition);
  driver.run("drop table " + tblName);
  listSize=authCalls.size();
  Table tableFromDropTableEvent=((org.apache.hadoop.hive.ql.metadata.Table)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE)).getTTable();
  validateDropTable(tbl,tableFromDropTableEvent);
  Table tCustom=tbl.deepCopy();
  tCustom.getSd().setInputFormat("org.apache.hive.dummy.DoesNotExistInputFormat");
  tCustom.getSd().setOutputFormat("org.apache.hive.dummy.DoesNotExistOutputFormat");
  if (tCustom.getSd().getSerdeInfo() == null) {
    tCustom.getSd().setSerdeInfo(new SerDeInfo("dummy","org.apache.hive.dummy.DoesNotExistSerDe",new HashMap<String,String>()));
  }
 else {
    tCustom.getSd().getSerdeInfo().setSerializationLib("org.apache.hive.dummy.DoesNotExistSerDe");
  }
  tCustom.setTableName(tbl.getTableName() + "_custom");
  listSize=authCalls.size();
  msc.createTable(tCustom);
  listSize++;
  Table customCreatedTableFromEvent=((org.apache.hadoop.hive.ql.metadata.Table)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE)).getTTable();
  Table customCreatedTable=msc.getTable(tCustom.getDbName(),tCustom.getTableName());
  validateCreateTable(tCustom,customCreatedTable);
  validateCreateTable(tCustom,customCreatedTableFromEvent);
  assertEquals(tCustom.getSd().getInputFormat(),customCreatedTable.getSd().getInputFormat());
  assertEquals(tCustom.getSd().getOutputFormat(),customCreatedTable.getSd().getOutputFormat());
  assertEquals(tCustom.getSd().getSerdeInfo().getSerializationLib(),customCreatedTable.getSd().getSerdeInfo().getSerializationLib());
  assertEquals(tCustom.getSd().getInputFormat(),customCreatedTableFromEvent.getSd().getInputFormat());
  assertEquals(tCustom.getSd().getOutputFormat(),customCreatedTableFromEvent.getSd().getOutputFormat());
  assertEquals(tCustom.getSd().getSerdeInfo().getSerializationLib(),customCreatedTableFromEvent.getSd().getSerdeInfo().getSerializationLib());
  listSize=authCalls.size();
  msc.dropTable(tCustom.getDbName(),tCustom.getTableName());
  listSize+=2;
  Table table2FromDropTableEvent=((org.apache.hadoop.hive.ql.metadata.Table)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE)).getTTable();
  validateDropTable(tCustom,table2FromDropTableEvent);
  driver.run("drop database " + dbName);
  listSize=authCalls.size();
  Database dbFromDropDatabaseEvent=(Database)assertAndExtractSingleObjectFromEvent(listSize,authCalls,DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.DB);
  validateDropDb(db,dbFromDropDatabaseEvent);
}
