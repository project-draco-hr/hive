{
  Table tbl=getTable(addPartitionDesc.getDbName(),addPartitionDesc.getTableName());
  int size=addPartitionDesc.getPartitionCount();
  List<org.apache.hadoop.hive.metastore.api.Partition> in=new ArrayList<org.apache.hadoop.hive.metastore.api.Partition>(size);
  for (int i=0; i < size; ++i) {
    in.add(convertAddSpecToMetaPartition(tbl,addPartitionDesc.getPartition(i)));
  }
  List<Partition> out=new ArrayList<Partition>();
  try {
    if (!addPartitionDesc.getReplaceMode()) {
      for (      org.apache.hadoop.hive.metastore.api.Partition outPart : getMSC().add_partitions(in,addPartitionDesc.isIfNotExists(),true)) {
        out.add(new Partition(tbl,outPart));
      }
    }
 else {
      getMSC().alter_partitions(addPartitionDesc.getDbName(),addPartitionDesc.getTableName(),in);
      List<String> part_names=new ArrayList<String>();
      for (      org.apache.hadoop.hive.metastore.api.Partition p : in) {
        part_names.add(Warehouse.makePartName(tbl.getPartitionKeys(),p.getValues()));
      }
      for (      org.apache.hadoop.hive.metastore.api.Partition outPart : getMSC().getPartitionsByNames(addPartitionDesc.getDbName(),addPartitionDesc.getTableName(),part_names)) {
        out.add(new Partition(tbl,outPart));
      }
    }
  }
 catch (  Exception e) {
    LOG.error(StringUtils.stringifyException(e));
    throw new HiveException(e);
  }
  return out;
}
