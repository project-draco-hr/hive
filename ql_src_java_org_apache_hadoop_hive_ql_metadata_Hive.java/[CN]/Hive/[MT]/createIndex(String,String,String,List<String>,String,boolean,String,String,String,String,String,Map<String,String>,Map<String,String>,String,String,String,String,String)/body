{
  try {
    String dbName=MetaStoreUtils.DEFAULT_DATABASE_NAME;
    Index old_index=null;
    try {
      old_index=getIndex(dbName,tableName,indexName);
    }
 catch (    Exception e) {
    }
    if (old_index != null) {
      throw new HiveException("Index " + indexName + " already exists on table "+ tableName+ ", db="+ dbName);
    }
    org.apache.hadoop.hive.metastore.api.Table baseTbl=getMSC().getTable(dbName,tableName);
    if (baseTbl.getTableType() == TableType.VIRTUAL_VIEW.toString()) {
      throw new HiveException("tableName=" + tableName + " is a VIRTUAL VIEW. Index on VIRTUAL VIEW is not supported.");
    }
    if (indexTblName == null) {
      indexTblName=MetaStoreUtils.getIndexTableName(dbName,tableName,indexName);
    }
 else {
      org.apache.hadoop.hive.metastore.api.Table temp=null;
      try {
        temp=getMSC().getTable(dbName,indexTblName);
      }
 catch (      Exception e) {
      }
      if (temp != null) {
        throw new HiveException("Table name " + indexTblName + " already exists. Choose another name.");
      }
    }
    org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor=baseTbl.getSd().clone();
    SerDeInfo serdeInfo=storageDescriptor.getSerdeInfo();
    if (serde != null) {
      serdeInfo.setSerializationLib(serde);
    }
 else {
      if (storageHandler == null) {
        serdeInfo.setSerializationLib(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
      }
 else {
        HiveStorageHandler sh=HiveUtils.getStorageHandler(getConf(),storageHandler);
        String serDeClassName=sh.getSerDeClass().getName();
        serdeInfo.setSerializationLib(serDeClassName);
      }
    }
    if (fieldDelim != null) {
      serdeInfo.getParameters().put(Constants.FIELD_DELIM,fieldDelim);
      serdeInfo.getParameters().put(Constants.SERIALIZATION_FORMAT,fieldDelim);
    }
    if (fieldEscape != null) {
      serdeInfo.getParameters().put(Constants.ESCAPE_CHAR,fieldEscape);
    }
    if (collItemDelim != null) {
      serdeInfo.getParameters().put(Constants.COLLECTION_DELIM,collItemDelim);
    }
    if (mapKeyDelim != null) {
      serdeInfo.getParameters().put(Constants.MAPKEY_DELIM,mapKeyDelim);
    }
    if (lineDelim != null) {
      serdeInfo.getParameters().put(Constants.LINE_DELIM,lineDelim);
    }
    if (serdeProps != null) {
      Iterator<Entry<String,String>> iter=serdeProps.entrySet().iterator();
      while (iter.hasNext()) {
        Entry<String,String> m=iter.next();
        serdeInfo.getParameters().put(m.getKey(),m.getValue());
      }
    }
    storageDescriptor.setLocation(null);
    if (location != null) {
      storageDescriptor.setLocation(location);
    }
    storageDescriptor.setInputFormat(inputFormat);
    storageDescriptor.setOutputFormat(outputFormat);
    Map<String,String> params=new HashMap<String,String>();
    List<FieldSchema> indexTblCols=new ArrayList<FieldSchema>();
    List<Order> sortCols=new ArrayList<Order>();
    storageDescriptor.setBucketCols(null);
    int k=0;
    for (int i=0; i < storageDescriptor.getCols().size(); i++) {
      FieldSchema col=storageDescriptor.getCols().get(i);
      if (indexedCols.contains(col.getName())) {
        indexTblCols.add(col);
        sortCols.add(new Order(col.getName(),1));
        k++;
      }
    }
    if (k != indexedCols.size())     throw new RuntimeException("Check the index columns, they should appear in the table being indexed.");
    storageDescriptor.setCols(indexTblCols);
    storageDescriptor.setSortCols(sortCols);
    int time=(int)(System.currentTimeMillis() / 1000);
    org.apache.hadoop.hive.metastore.api.Table tt=null;
    HiveIndexHandler indexHandler=HiveUtils.getIndexHandler(this.getConf(),indexHandlerClass);
    if (indexHandler.usesIndexTable()) {
      tt=new org.apache.hadoop.hive.ql.metadata.Table(indexTblName).getTTable();
      List<FieldSchema> partKeys=baseTbl.getPartitionKeys();
      tt.setPartitionKeys(partKeys);
      tt.setTableType(TableType.INDEX_TABLE.toString());
    }
    if (!deferredRebuild) {
      throw new RuntimeException("Please specify deferred rebuild using \" WITH DEFERRED REBUILD \".");
    }
    Index indexDesc=new Index(indexName,indexHandlerClass,dbName,tableName,time,time,indexTblName,storageDescriptor,params,deferredRebuild);
    indexHandler.analyzeIndexDefinition(baseTbl,indexDesc,tt);
    this.getMSC().createIndex(indexDesc,tt);
  }
 catch (  Exception e) {
    throw new HiveException(e);
  }
}
