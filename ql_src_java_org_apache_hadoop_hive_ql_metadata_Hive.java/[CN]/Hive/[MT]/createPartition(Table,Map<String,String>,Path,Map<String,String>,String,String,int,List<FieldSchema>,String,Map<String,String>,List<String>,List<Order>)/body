{
  org.apache.hadoop.hive.metastore.api.Partition partition=null;
  for (  FieldSchema field : tbl.getPartCols()) {
    String val=partSpec.get(field.getName());
    if (val == null || val.length() == 0) {
      throw new HiveException("add partition: Value for key " + field.getName() + " is null or empty");
    }
  }
  try {
    Partition tmpPart=new Partition(tbl,partSpec,location);
    org.apache.hadoop.hive.metastore.api.Partition inPart=tmpPart.getTPartition();
    if (partParams != null) {
      inPart.setParameters(partParams);
    }
    if (inputFormat != null) {
      inPart.getSd().setInputFormat(inputFormat);
    }
    if (outputFormat != null) {
      inPart.getSd().setOutputFormat(outputFormat);
    }
    if (numBuckets != -1) {
      inPart.getSd().setNumBuckets(numBuckets);
    }
    if (cols != null) {
      inPart.getSd().setCols(cols);
    }
    if (serializationLib != null) {
      inPart.getSd().getSerdeInfo().setSerializationLib(serializationLib);
    }
    if (serdeParams != null) {
      inPart.getSd().getSerdeInfo().setParameters(serdeParams);
    }
    if (bucketCols != null) {
      inPart.getSd().setBucketCols(bucketCols);
    }
    if (sortCols != null) {
      inPart.getSd().setSortCols(sortCols);
    }
    partition=getMSC().add_partition(inPart);
  }
 catch (  Exception e) {
    LOG.error(StringUtils.stringifyException(e));
    throw new HiveException(e);
  }
  return new Partition(tbl,partition);
}
