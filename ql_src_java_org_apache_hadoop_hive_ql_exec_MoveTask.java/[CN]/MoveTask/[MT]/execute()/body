{
  try {
    for (    loadFileDesc lfd : work.getLoadFileWork()) {
      Path targetPath=new Path(lfd.getTargetDir());
      Path sourcePath=new Path(lfd.getSourceDir());
      FileSystem fs=sourcePath.getFileSystem(conf);
      if (lfd.getIsDfsDir()) {
        String mesg="Moving data to: " + lfd.getTargetDir();
        String mesg_detail=" from " + lfd.getSourceDir();
        console.printInfo(mesg,mesg_detail);
        fs.delete(targetPath,true);
        if (fs.exists(sourcePath)) {
          if (!fs.rename(sourcePath,targetPath))           throw new HiveException("Unable to rename: " + sourcePath + " to: "+ targetPath);
        }
 else         if (!fs.mkdirs(targetPath))         throw new HiveException("Unable to make directory: " + targetPath);
      }
 else {
        String mesg="Copying data to local directory " + lfd.getTargetDir();
        String mesg_detail=" from " + lfd.getSourceDir();
        console.printInfo(mesg,mesg_detail);
        LocalFileSystem dstFs=FileSystem.getLocal(conf);
        if (dstFs.delete(targetPath,true) || !dstFs.exists(targetPath)) {
          console.printInfo(mesg,mesg_detail);
          if (fs.exists(sourcePath))           fs.copyToLocalFile(sourcePath,targetPath);
 else {
            if (!dstFs.mkdirs(targetPath))             throw new HiveException("Unable to make local directory: " + targetPath);
          }
        }
 else {
          console.printInfo("Unable to delete the existing destination directory: " + targetPath);
        }
      }
    }
    for (    loadTableDesc tbd : work.getLoadTableWork()) {
      String mesg="Loading data to table " + tbd.getTable().getTableName() + ((tbd.getPartitionSpec().size() > 0) ? " partition " + tbd.getPartitionSpec().toString() : "");
      String mesg_detail=" from " + tbd.getSourceDir();
      console.printInfo(mesg,mesg_detail);
      if (work.getCheckFileFormat()) {
        FileStatus[] dirs;
        ArrayList<FileStatus> files;
        FileSystem fs;
        try {
          fs=FileSystem.get(db.getTable(tbd.getTable().getTableName()).getDataLocation(),conf);
          dirs=fs.globStatus(new Path(tbd.getSourceDir()));
          files=new ArrayList<FileStatus>();
          for (int i=0; (dirs != null && i < dirs.length); i++) {
            files.addAll(Arrays.asList(fs.listStatus(dirs[i].getPath())));
            if (files.size() > 0)             break;
          }
        }
 catch (        IOException e) {
          throw new HiveException("addFiles: filesystem error in check phase",e);
        }
        HiveFileFormatUtils.checkInputFormat(fs,conf,tbd.getTable().getInputFileFormatClass(),files);
      }
      if (tbd.getPartitionSpec().size() == 0) {
        db.loadTable(new Path(tbd.getSourceDir()),tbd.getTable().getTableName(),tbd.getReplace(),new Path(tbd.getTmpDir()));
      }
 else {
        LOG.info("Partition is: " + tbd.getPartitionSpec().toString());
        db.loadPartition(new Path(tbd.getSourceDir()),tbd.getTable().getTableName(),tbd.getPartitionSpec(),tbd.getReplace(),new Path(tbd.getTmpDir()));
      }
    }
    return 0;
  }
 catch (  Exception e) {
    console.printError("Failed with exception " + e.getMessage(),"\n" + StringUtils.stringifyException(e));
    return (1);
  }
}
