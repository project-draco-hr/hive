{
  LOG.info("On root vertex initialized " + inputName);
  try {
    MRInputUserPayloadProto protoPayload=MRInputHelpers.parseMRInputPayload(inputDescriptor.getUserPayload());
    this.conf=TezUtils.createConfFromByteString(protoPayload.getConfigurationBytes());
    MRInputUserPayloadProto updatedPayload=MRInputUserPayloadProto.newBuilder(protoPayload).setGroupingEnabled(true).build();
    inputDescriptor.setUserPayload(UserPayload.create(updatedPayload.toByteString().asReadOnlyByteBuffer()));
  }
 catch (  IOException e) {
    e.printStackTrace();
    throw new RuntimeException(e);
  }
  boolean dataInformationEventSeen=false;
  Map<String,List<FileSplit>> pathFileSplitsMap=new TreeMap<String,List<FileSplit>>();
  for (  Event event : events) {
    if (event instanceof InputConfigureVertexTasksEvent) {
      Preconditions.checkState(dataInformationEventSeen == false);
      InputConfigureVertexTasksEvent cEvent=(InputConfigureVertexTasksEvent)event;
      configureVertexTaskEvent=cEvent;
      dataInformationEvents=Lists.newArrayListWithCapacity(configureVertexTaskEvent.getNumTasks());
    }
    if (event instanceof InputUpdatePayloadEvent) {
      Preconditions.checkState(false);
    }
 else     if (event instanceof InputDataInformationEvent) {
      dataInformationEventSeen=true;
      InputDataInformationEvent diEvent=(InputDataInformationEvent)event;
      dataInformationEvents.add(diEvent);
      FileSplit fileSplit;
      try {
        fileSplit=getFileSplitFromEvent(diEvent);
      }
 catch (      IOException e) {
        throw new RuntimeException("Failed to get file split for event: " + diEvent);
      }
      List<FileSplit> fsList=pathFileSplitsMap.get(fileSplit.getPath().getName());
      if (fsList == null) {
        fsList=new ArrayList<FileSplit>();
        pathFileSplitsMap.put(fileSplit.getPath().getName(),fsList);
      }
      fsList.add(fileSplit);
    }
  }
  Multimap<Integer,InputSplit> bucketToInitialSplitMap=getBucketSplitMapForPath(pathFileSplitsMap);
  try {
    int totalResource=context.getTotalAvailableResource().getMemory();
    int taskResource=context.getVertexTaskResource().getMemory();
    float waves=conf.getFloat(TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES,TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES_DEFAULT);
    int availableSlots=totalResource / taskResource;
    LOG.info("Grouping splits. " + availableSlots + " available slots, "+ waves+ " waves.");
    JobConf jobConf=new JobConf(conf);
    ShimLoader.getHadoopShims().getMergedCredentials(jobConf);
    Multimap<Integer,InputSplit> bucketToGroupedSplitMap=HashMultimap.<Integer,InputSplit>create();
    for (    Integer key : bucketToInitialSplitMap.keySet()) {
      InputSplit[] inputSplitArray=(bucketToInitialSplitMap.get(key).toArray(new InputSplit[0]));
      Multimap<Integer,InputSplit> groupedSplit=HiveSplitGenerator.generateGroupedSplits(jobConf,conf,inputSplitArray,waves,availableSlots,inputName);
      bucketToGroupedSplitMap.putAll(key,groupedSplit.values());
    }
    LOG.info("We have grouped the splits into " + bucketToGroupedSplitMap);
    if ((mainWorkName.isEmpty() == false) && (mainWorkName.compareTo(inputName) != 0)) {
      processAllSideEvents(inputName,bucketToGroupedSplitMap);
    }
 else {
      processAllEvents(inputName,bucketToGroupedSplitMap);
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}
