{
  Preconditions.checkState(rootVertexInitialized == false);
  LOG.info("Root vertex not initialized");
  rootVertexInitialized=true;
  try {
    MRInputUserPayloadProto protoPayload=MRHelpers.parseMRInputPayload(inputDescriptor.getUserPayload());
    this.conf=MRHelpers.createConfFromByteString(protoPayload.getConfigurationBytes());
    this.conf.set("mapred.input.format.class",TezGroupedSplitsInputFormat.class.getName());
    MRInputUserPayloadProto updatedPayload=MRInputUserPayloadProto.newBuilder(protoPayload).setConfigurationBytes(MRHelpers.createByteStringFromConf(conf)).build();
    inputDescriptor.setUserPayload(updatedPayload.toByteArray());
  }
 catch (  IOException e) {
    e.printStackTrace();
    throw new RuntimeException(e);
  }
  boolean dataInformationEventSeen=false;
  Map<String,List<FileSplit>> pathFileSplitsMap=new TreeMap<String,List<FileSplit>>();
  for (  Event event : events) {
    if (event instanceof RootInputConfigureVertexTasksEvent) {
      Preconditions.checkState(dataInformationEventSeen == false);
      Preconditions.checkState(context.getVertexNumTasks(context.getVertexName()) == -1,"Parallelism for the vertex should be set to -1 if the InputInitializer is setting parallelism");
      RootInputConfigureVertexTasksEvent cEvent=(RootInputConfigureVertexTasksEvent)event;
      configureVertexTaskEvent=cEvent;
      dataInformationEvents=Lists.newArrayListWithCapacity(configureVertexTaskEvent.getNumTasks());
    }
    if (event instanceof RootInputUpdatePayloadEvent) {
      Preconditions.checkState(false);
    }
 else     if (event instanceof RootInputDataInformationEvent) {
      dataInformationEventSeen=true;
      RootInputDataInformationEvent diEvent=(RootInputDataInformationEvent)event;
      dataInformationEvents.add(diEvent);
      FileSplit fileSplit;
      try {
        fileSplit=getFileSplitFromEvent(diEvent);
      }
 catch (      IOException e) {
        throw new RuntimeException("Failed to get file split for event: " + diEvent);
      }
      List<FileSplit> fsList=pathFileSplitsMap.get(fileSplit.getPath().getName());
      if (fsList == null) {
        fsList=new ArrayList<FileSplit>();
        pathFileSplitsMap.put(fileSplit.getPath().getName(),fsList);
      }
      fsList.add(fileSplit);
    }
  }
  Multimap<Integer,InputSplit> bucketToInitialSplitMap=getBucketSplitMapForPath(pathFileSplitsMap);
  try {
    int totalResource=context.getTotalAvailableResource().getMemory();
    int taskResource=context.getVertexTaskResource().getMemory();
    float waves=conf.getFloat(TezConfiguration.TEZ_AM_GROUPING_SPLIT_WAVES,TezConfiguration.TEZ_AM_GROUPING_SPLIT_WAVES_DEFAULT);
    int availableSlots=totalResource / taskResource;
    LOG.info("Grouping splits. " + availableSlots + " available slots, "+ waves+ " waves.");
    JobConf jobConf=new JobConf(conf);
    ShimLoader.getHadoopShims().getMergedCredentials(jobConf);
    Multimap<Integer,InputSplit> bucketToGroupedSplitMap=HashMultimap.<Integer,InputSplit>create();
    for (    Integer key : bucketToInitialSplitMap.keySet()) {
      InputSplit[] inputSplitArray=(bucketToInitialSplitMap.get(key).toArray(new InputSplit[0]));
      Multimap<Integer,InputSplit> groupedSplit=HiveSplitGenerator.generateGroupedSplits(jobConf,conf,inputSplitArray,waves,availableSlots);
      bucketToGroupedSplitMap.putAll(key,groupedSplit.values());
    }
    LOG.info("We have grouped the splits into " + bucketToGroupedSplitMap.size() + " tasks");
    processAllEvents(inputName,bucketToGroupedSplitMap);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}
