{
  HBaseSplit hbaseSplit=(HBaseSplit)split;
  TableSplit tableSplit=hbaseSplit.getSplit();
  String hbaseTableName=jobConf.get(HBaseSerDe.HBASE_TABLE_NAME);
  setHTable(new HTable(HBaseConfiguration.create(jobConf),Bytes.toBytes(hbaseTableName)));
  String hbaseColumnsMapping=jobConf.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
  boolean doColumnRegexMatching=jobConf.getBoolean(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING,true);
  List<Integer> readColIDs=ColumnProjectionUtils.getReadColumnIDs(jobConf);
  List<ColumnMapping> columnsMapping=null;
  try {
    columnsMapping=HBaseSerDe.parseColumnsMapping(hbaseColumnsMapping,doColumnRegexMatching);
  }
 catch (  SerDeException e) {
    throw new IOException(e);
  }
  if (columnsMapping.size() < readColIDs.size()) {
    throw new IOException("Cannot read more columns than the given table contains.");
  }
  boolean readAllColumns=ColumnProjectionUtils.isReadAllColumns(jobConf);
  Scan scan=new Scan();
  boolean empty=true;
  List<String> addedFamilies=new ArrayList<String>();
  if (!readAllColumns) {
    for (    int i : readColIDs) {
      ColumnMapping colMap=columnsMapping.get(i);
      if (colMap.hbaseRowKey) {
        continue;
      }
      if (colMap.qualifierName == null) {
        scan.addFamily(colMap.familyNameBytes);
        addedFamilies.add(colMap.familyName);
      }
 else {
        if (!addedFamilies.contains(colMap.familyName)) {
          scan.addColumn(colMap.familyNameBytes,colMap.qualifierNameBytes);
        }
      }
      empty=false;
    }
  }
  if (empty) {
    for (int i=0; i < columnsMapping.size(); i++) {
      ColumnMapping colMap=columnsMapping.get(i);
      if (colMap.hbaseRowKey) {
        continue;
      }
      if (colMap.qualifierName == null) {
        scan.addFamily(colMap.familyNameBytes);
      }
 else {
        scan.addColumn(colMap.familyNameBytes,colMap.qualifierNameBytes);
      }
      if (!readAllColumns) {
        break;
      }
    }
  }
  String scanCache=jobConf.get(HBaseSerDe.HBASE_SCAN_CACHE);
  if (scanCache != null) {
    scan.setCaching(Integer.valueOf(scanCache));
  }
  String scanCacheBlocks=jobConf.get(HBaseSerDe.HBASE_SCAN_CACHEBLOCKS);
  if (scanCacheBlocks != null) {
    scan.setCacheBlocks(Boolean.valueOf(scanCacheBlocks));
  }
  String scanBatch=jobConf.get(HBaseSerDe.HBASE_SCAN_BATCH);
  if (scanBatch != null) {
    scan.setBatch(Integer.valueOf(scanBatch));
  }
  setScan(scan);
  Job job=new Job(jobConf);
  TaskAttemptContext tac=ShimLoader.getHadoopShims().newTaskAttemptContext(job.getConfiguration(),reporter);
  final org.apache.hadoop.mapreduce.RecordReader<ImmutableBytesWritable,Result> recordReader=createRecordReader(tableSplit,tac);
  try {
    recordReader.initialize(tableSplit,tac);
  }
 catch (  InterruptedException e) {
    throw new IOException("Failed to initialize RecordReader",e);
  }
  return new RecordReader<ImmutableBytesWritable,ResultWritable>(){
    @Override public void close() throws IOException {
      recordReader.close();
    }
    @Override public ImmutableBytesWritable createKey(){
      return new ImmutableBytesWritable();
    }
    @Override public ResultWritable createValue(){
      return new ResultWritable(new Result());
    }
    @Override public long getPos() throws IOException {
      return 0;
    }
    @Override public float getProgress() throws IOException {
      float progress=0.0F;
      try {
        progress=recordReader.getProgress();
      }
 catch (      InterruptedException e) {
        throw new IOException(e);
      }
      return progress;
    }
    @Override public boolean next(    ImmutableBytesWritable rowKey,    ResultWritable value) throws IOException {
      boolean next=false;
      try {
        next=recordReader.nextKeyValue();
        if (next) {
          rowKey.set(recordReader.getCurrentValue().getRow());
          value.setResult(recordReader.getCurrentValue());
        }
      }
 catch (      InterruptedException e) {
        throw new IOException(e);
      }
      return next;
    }
  }
;
}
