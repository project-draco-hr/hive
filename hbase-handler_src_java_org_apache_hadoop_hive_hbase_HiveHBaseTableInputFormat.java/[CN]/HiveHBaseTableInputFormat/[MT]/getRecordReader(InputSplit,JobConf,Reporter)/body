{
  HBaseSplit hbaseSplit=(HBaseSplit)split;
  TableSplit tableSplit=hbaseSplit.getTableSplit();
  setHTable(HiveHBaseInputFormatUtil.getTable(jobConf));
  setScan(HiveHBaseInputFormatUtil.getScan(jobConf));
  Job job=new Job(jobConf);
  TaskAttemptContext tac=ShimLoader.getHadoopShims().newTaskAttemptContext(job.getConfiguration(),reporter);
  final org.apache.hadoop.mapreduce.RecordReader<ImmutableBytesWritable,Result> recordReader=createRecordReader(tableSplit,tac);
  try {
    recordReader.initialize(tableSplit,tac);
  }
 catch (  InterruptedException e) {
    throw new IOException("Failed to initialize RecordReader",e);
  }
  return new RecordReader<ImmutableBytesWritable,ResultWritable>(){
    @Override public void close() throws IOException {
      recordReader.close();
    }
    @Override public ImmutableBytesWritable createKey(){
      return new ImmutableBytesWritable();
    }
    @Override public ResultWritable createValue(){
      return new ResultWritable(new Result());
    }
    @Override public long getPos() throws IOException {
      return 0;
    }
    @Override public float getProgress() throws IOException {
      float progress=0.0F;
      try {
        progress=recordReader.getProgress();
      }
 catch (      InterruptedException e) {
        throw new IOException(e);
      }
      return progress;
    }
    @Override public boolean next(    ImmutableBytesWritable rowKey,    ResultWritable value) throws IOException {
      boolean next=false;
      try {
        next=recordReader.nextKeyValue();
        if (next) {
          rowKey.set(recordReader.getCurrentValue().getRow());
          value.setResult(recordReader.getCurrentValue());
        }
      }
 catch (      InterruptedException e) {
        throw new IOException(e);
      }
      return next;
    }
  }
;
}
