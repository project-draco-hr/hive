{
  HBaseSplit hbaseSplit=(HBaseSplit)split;
  TableSplit tableSplit=hbaseSplit.getSplit();
  String hbaseTableName=jobConf.get(HBaseSerDe.HBASE_TABLE_NAME);
  setHTable(new HTable(new HBaseConfiguration(jobConf),Bytes.toBytes(hbaseTableName)));
  String hbaseColumnsMapping=jobConf.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
  List<String> hbaseColumnFamilies=new ArrayList<String>();
  List<String> hbaseColumnQualifiers=new ArrayList<String>();
  List<byte[]> hbaseColumnFamiliesBytes=new ArrayList<byte[]>();
  List<byte[]> hbaseColumnQualifiersBytes=new ArrayList<byte[]>();
  int iKey;
  try {
    iKey=HBaseSerDe.parseColumnMapping(hbaseColumnsMapping,hbaseColumnFamilies,hbaseColumnFamiliesBytes,hbaseColumnQualifiers,hbaseColumnQualifiersBytes);
  }
 catch (  SerDeException se) {
    throw new IOException(se);
  }
  List<Integer> readColIDs=ColumnProjectionUtils.getReadColumnIDs(jobConf);
  if (hbaseColumnFamilies.size() < readColIDs.size()) {
    throw new IOException("Cannot read more columns than the given table contains.");
  }
  boolean addAll=(readColIDs.size() == 0);
  Scan scan=new Scan();
  boolean empty=true;
  if (!addAll) {
    for (    int i : readColIDs) {
      if (i == iKey) {
        continue;
      }
      if (hbaseColumnQualifiers.get(i) == null) {
        scan.addFamily(hbaseColumnFamiliesBytes.get(i));
      }
 else {
        scan.addColumn(hbaseColumnFamiliesBytes.get(i),hbaseColumnQualifiersBytes.get(i));
      }
      empty=false;
    }
  }
  if (empty) {
    for (int i=0; i < hbaseColumnFamilies.size(); i++) {
      if (i == iKey) {
        continue;
      }
      if (hbaseColumnQualifiers.get(i) == null) {
        scan.addFamily(hbaseColumnFamiliesBytes.get(i));
      }
 else {
        scan.addColumn(hbaseColumnFamiliesBytes.get(i),hbaseColumnQualifiersBytes.get(i));
      }
      if (!addAll) {
        break;
      }
    }
  }
  tableSplit=convertFilter(jobConf,scan,tableSplit,iKey);
  setScan(scan);
  Job job=new Job(jobConf);
  TaskAttemptContext tac=ShimLoader.getHadoopShims().newTaskAttemptContext(job.getConfiguration(),reporter);
  final org.apache.hadoop.mapreduce.RecordReader<ImmutableBytesWritable,Result> recordReader=createRecordReader(tableSplit,tac);
  return new RecordReader<ImmutableBytesWritable,Result>(){
    @Override public void close() throws IOException {
      recordReader.close();
    }
    @Override public ImmutableBytesWritable createKey(){
      return new ImmutableBytesWritable();
    }
    @Override public Result createValue(){
      return new Result();
    }
    @Override public long getPos() throws IOException {
      return 0;
    }
    @Override public float getProgress() throws IOException {
      float progress=0.0F;
      try {
        progress=recordReader.getProgress();
      }
 catch (      InterruptedException e) {
        throw new IOException(e);
      }
      return progress;
    }
    @Override public boolean next(    ImmutableBytesWritable rowKey,    Result value) throws IOException {
      boolean next=false;
      try {
        next=recordReader.nextKeyValue();
        if (next) {
          rowKey.set(recordReader.getCurrentValue().getRow());
          Writables.copyWritable(recordReader.getCurrentValue(),value);
        }
      }
 catch (      InterruptedException e) {
        throw new IOException(e);
      }
      return next;
    }
  }
;
}
