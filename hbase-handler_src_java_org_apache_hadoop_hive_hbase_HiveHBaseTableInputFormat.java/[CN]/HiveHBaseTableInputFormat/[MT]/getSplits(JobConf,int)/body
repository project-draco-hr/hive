{
  TableMapReduceUtil.initCredentials(jobConf);
  String hbaseTableName=jobConf.get(HBaseSerDe.HBASE_TABLE_NAME);
  setHTable(new HTable(HBaseConfiguration.create(jobConf),Bytes.toBytes(hbaseTableName)));
  String hbaseColumnsMapping=jobConf.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
  boolean doColumnRegexMatching=jobConf.getBoolean(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING,true);
  if (hbaseColumnsMapping == null) {
    throw new IOException("hbase.columns.mapping required for HBase Table.");
  }
  List<ColumnMapping> columnsMapping=null;
  try {
    columnsMapping=HBaseSerDe.parseColumnsMapping(hbaseColumnsMapping,doColumnRegexMatching);
  }
 catch (  SerDeException e) {
    throw new IOException(e);
  }
  int iKey;
  try {
    iKey=HBaseSerDe.getRowKeyColumnOffset(columnsMapping);
  }
 catch (  SerDeException e) {
    throw new IOException(e);
  }
  Scan scan=new Scan();
  for (int i=0; i < columnsMapping.size(); i++) {
    ColumnMapping colMap=columnsMapping.get(i);
    if (colMap.hbaseRowKey) {
      continue;
    }
    if (colMap.qualifierName == null) {
      scan.addFamily(colMap.familyNameBytes);
    }
 else {
      scan.addColumn(colMap.familyNameBytes,colMap.qualifierNameBytes);
    }
  }
  convertFilter(jobConf,scan,null,iKey,getStorageFormatOfKey(columnsMapping.get(iKey).mappingSpec,jobConf.get(HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE,"string")));
  setScan(scan);
  Job job=new Job(jobConf);
  JobContext jobContext=ShimLoader.getHadoopShims().newJobContext(job);
  Path[] tablePaths=FileInputFormat.getInputPaths(jobContext);
  List<org.apache.hadoop.mapreduce.InputSplit> splits=super.getSplits(jobContext);
  InputSplit[] results=new InputSplit[splits.size()];
  for (int i=0; i < splits.size(); i++) {
    results[i]=new HBaseSplit((TableSplit)splits.get(i),tablePaths[0]);
  }
  return results;
}
