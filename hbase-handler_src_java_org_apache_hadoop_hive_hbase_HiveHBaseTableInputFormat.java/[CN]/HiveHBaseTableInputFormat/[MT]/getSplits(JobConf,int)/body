{
  String hbaseTableName=jobConf.get(HBaseSerDe.HBASE_TABLE_NAME);
  setHTable(new HTable(new HBaseConfiguration(jobConf),Bytes.toBytes(hbaseTableName)));
  String hbaseColumnsMapping=jobConf.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
  if (hbaseColumnsMapping == null) {
    throw new IOException("hbase.columns.mapping required for HBase Table.");
  }
  List<String> hbaseColumnFamilies=new ArrayList<String>();
  List<String> hbaseColumnQualifiers=new ArrayList<String>();
  List<byte[]> hbaseColumnFamiliesBytes=new ArrayList<byte[]>();
  List<byte[]> hbaseColumnQualifiersBytes=new ArrayList<byte[]>();
  int iKey;
  try {
    iKey=HBaseSerDe.parseColumnMapping(hbaseColumnsMapping,hbaseColumnFamilies,hbaseColumnFamiliesBytes,hbaseColumnQualifiers,hbaseColumnQualifiersBytes);
  }
 catch (  SerDeException se) {
    throw new IOException(se);
  }
  Scan scan=new Scan();
  for (int i=0; i < hbaseColumnFamilies.size(); i++) {
    if (i == iKey) {
      continue;
    }
    if (hbaseColumnQualifiers.get(i) == null) {
      scan.addFamily(hbaseColumnFamiliesBytes.get(i));
    }
 else {
      scan.addColumn(hbaseColumnFamiliesBytes.get(i),hbaseColumnQualifiersBytes.get(i));
    }
  }
  setScan(scan);
  Job job=new Job(jobConf);
  JobContext jobContext=new JobContext(job.getConfiguration(),job.getJobID());
  Path[] tablePaths=FileInputFormat.getInputPaths(jobContext);
  List<org.apache.hadoop.mapreduce.InputSplit> splits=getSplits(jobContext);
  InputSplit[] results=new InputSplit[splits.size()];
  for (int i=0; i < splits.size(); i++) {
    results[i]=new HBaseSplit((TableSplit)splits.get(i),tablePaths[0]);
  }
  return results;
}
