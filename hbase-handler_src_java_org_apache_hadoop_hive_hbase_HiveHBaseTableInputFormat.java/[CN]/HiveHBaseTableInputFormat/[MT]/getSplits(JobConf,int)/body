{
  String hbaseTableName=jobConf.get(HBaseSerDe.HBASE_TABLE_NAME);
  setHTable(new HTable(new HBaseConfiguration(jobConf),Bytes.toBytes(hbaseTableName)));
  String hbaseColumnsMapping=jobConf.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
  if (hbaseColumnsMapping == null) {
    throw new IOException("hbase.columns.mapping required for HBase Table.");
  }
  List<String> columns=HBaseSerDe.parseColumnMapping(hbaseColumnsMapping);
  List<byte[]> inputColumns=new ArrayList<byte[]>();
  for (  String column : columns) {
    if (HBaseSerDe.isSpecialColumn(column)) {
      continue;
    }
    inputColumns.add(Bytes.toBytes(column));
  }
  setScan(new Scan().addColumns(inputColumns.toArray(new byte[0][])));
  Job job=new Job(jobConf);
  JobContext jobContext=new JobContext(job.getConfiguration(),job.getJobID());
  Path[] tablePaths=FileInputFormat.getInputPaths(jobContext);
  List<org.apache.hadoop.mapreduce.InputSplit> splits=getSplits(jobContext);
  InputSplit[] results=new InputSplit[splits.size()];
  for (int i=0; i < splits.size(); i++) {
    results[i]=new HBaseSplit((TableSplit)splits.get(i),tablePaths[0]);
  }
  return results;
}
