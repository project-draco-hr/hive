{
  StatsAggregator statsAggregator=null;
  try {
    Warehouse wh=new Warehouse(conf);
    FileSystem fileSys;
    FileStatus[] fileStatus;
    if (!this.getWork().getNoStatsAggregator()) {
      String statsImplementationClass=HiveConf.getVar(conf,HiveConf.ConfVars.HIVESTATSDBCLASS);
      StatsFactory.setImplementation(statsImplementationClass,conf);
      statsAggregator=StatsFactory.getStatsAggregator();
      if (!statsAggregator.connect(conf)) {
        throw new HiveException("StatsAggregator connect failed " + statsImplementationClass);
      }
    }
    TableStatistics tblStats=new TableStatistics();
    org.apache.hadoop.hive.metastore.api.Table tTable=table.getTTable();
    Map<String,String> parameters=tTable.getParameters();
    boolean tableStatsExist=this.existStats(parameters);
    if (parameters.containsKey(StatsSetupConst.ROW_COUNT)) {
      tblStats.setNumRows(Long.parseLong(parameters.get(StatsSetupConst.ROW_COUNT)));
    }
    if (parameters.containsKey(StatsSetupConst.NUM_PARTITIONS)) {
      tblStats.setNumPartitions(Integer.parseInt(parameters.get(StatsSetupConst.NUM_PARTITIONS)));
    }
    if (parameters.containsKey(StatsSetupConst.NUM_FILES)) {
      tblStats.setNumFiles(Integer.parseInt(parameters.get(StatsSetupConst.NUM_FILES)));
    }
    if (parameters.containsKey(StatsSetupConst.TOTAL_SIZE)) {
      tblStats.setSize(Long.parseLong(parameters.get(StatsSetupConst.TOTAL_SIZE)));
    }
    List<Partition> partitions=getPartitionsList();
    boolean atomic=HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_STATS_ATOMIC);
    if (partitions == null) {
      if (!tableStatsExist && atomic) {
        return 0;
      }
      Path tablePath=wh.getDefaultTablePath(table.getDbName(),table.getTableName());
      fileSys=tablePath.getFileSystem(conf);
      fileStatus=Utilities.getFileStatusRecurse(tablePath,1,fileSys);
      tblStats.setNumFiles(fileStatus.length);
      long tableSize=0L;
      for (int i=0; i < fileStatus.length; i++) {
        tableSize+=fileStatus[i].getLen();
      }
      tblStats.setSize(tableSize);
      if (statsAggregator != null) {
        String rows=statsAggregator.aggregateStats(work.getAggKey(),StatsSetupConst.ROW_COUNT);
        if (rows != null) {
          tblStats.setNumRows(Long.parseLong(rows));
        }
 else {
          if (atomic) {
            throw new HiveException("StatsAggregator failed to get numRows.");
          }
        }
      }
    }
 else {
      for (      Partition partn : partitions) {
        org.apache.hadoop.hive.metastore.api.Partition tPart=partn.getTPartition();
        parameters=tPart.getParameters();
        boolean hasStats=this.existStats(parameters);
        if (!hasStats && atomic) {
          continue;
        }
        int nf=parameters.containsKey(StatsSetupConst.NUM_FILES) ? Integer.parseInt(parameters.get(StatsSetupConst.NUM_FILES)) : 0;
        long nr=parameters.containsKey(StatsSetupConst.ROW_COUNT) ? Long.parseLong(parameters.get(StatsSetupConst.ROW_COUNT)) : 0L;
        long sz=parameters.containsKey(StatsSetupConst.TOTAL_SIZE) ? Long.parseLong(parameters.get(StatsSetupConst.TOTAL_SIZE)) : 0L;
        PartitionStatistics newPartStats=new PartitionStatistics();
        String partitionID=work.getAggKey() + Warehouse.makePartPath(partn.getSpec());
        if (statsAggregator != null) {
          String rows=statsAggregator.aggregateStats(partitionID,StatsSetupConst.ROW_COUNT);
          if (rows != null) {
            newPartStats.setNumRows(Long.parseLong(rows));
          }
 else {
            if (atomic) {
              throw new HiveException("StatsAggregator failed to get numRows.");
            }
          }
        }
 else {
          newPartStats.setNumRows(nr);
        }
        fileSys=partn.getPartitionPath().getFileSystem(conf);
        fileStatus=Utilities.getFileStatusRecurse(partn.getPartitionPath(),1,fileSys);
        newPartStats.setNumFiles(fileStatus.length);
        long partitionSize=0L;
        for (int i=0; i < fileStatus.length; i++) {
          partitionSize+=fileStatus[i].getLen();
        }
        newPartStats.setSize(partitionSize);
        if (hasStats) {
          PartitionStatistics oldPartStats=new PartitionStatistics(nf,nr,sz);
          tblStats.updateStats(oldPartStats,newPartStats);
        }
 else {
          tblStats.addPartitionStats(newPartStats);
        }
        parameters.put(StatsSetupConst.ROW_COUNT,Long.toString(newPartStats.getNumRows()));
        parameters.put(StatsSetupConst.NUM_FILES,Integer.toString(newPartStats.getNumFiles()));
        parameters.put(StatsSetupConst.TOTAL_SIZE,Long.toString(newPartStats.getSize()));
        tPart.setParameters(parameters);
        String tableFullName=table.getDbName() + "." + table.getTableName();
        db.alterPartition(tableFullName,new Partition(table,tPart));
        console.printInfo("Partition " + tableFullName + partn.getSpec()+ " stats: ["+ newPartStats.toString()+ ']');
      }
    }
    parameters=tTable.getParameters();
    parameters.put(StatsSetupConst.ROW_COUNT,Long.toString(tblStats.getNumRows()));
    parameters.put(StatsSetupConst.NUM_PARTITIONS,Integer.toString(tblStats.getNumPartitions()));
    parameters.put(StatsSetupConst.NUM_FILES,Integer.toString(tblStats.getNumFiles()));
    parameters.put(StatsSetupConst.TOTAL_SIZE,Long.toString(tblStats.getSize()));
    tTable.setParameters(parameters);
    String tableFullName=table.getDbName() + "." + table.getTableName();
    db.alterTable(tableFullName,new Table(tTable));
    console.printInfo("Table " + tableFullName + " stats: ["+ tblStats.toString()+ ']');
  }
 catch (  Exception e) {
    console.printInfo("[Warning] could not update stats.","Failed with exception " + e.getMessage() + "\n"+ StringUtils.stringifyException(e));
  }
 finally {
    if (statsAggregator != null) {
      statsAggregator.closeConnection();
    }
  }
  return 0;
}
