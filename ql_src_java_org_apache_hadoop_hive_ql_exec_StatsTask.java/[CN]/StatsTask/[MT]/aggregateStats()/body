{
  StatsAggregator statsAggregator=null;
  int ret=0;
  try {
    Warehouse wh=new Warehouse(conf);
    if (!this.getWork().getNoStatsAggregator()) {
      String statsImplementationClass=HiveConf.getVar(conf,HiveConf.ConfVars.HIVESTATSDBCLASS);
      StatsFactory.setImplementation(statsImplementationClass,conf);
      if (work.isNoScanAnalyzeCommand()) {
        StatsPublisher statsPublisher=StatsFactory.getStatsPublisher();
        if (!statsPublisher.init(conf)) {
          if (HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
            throw new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
          }
        }
      }
      statsAggregator=StatsFactory.getStatsAggregator();
      if (!statsAggregator.connect(conf)) {
        throw new HiveException("StatsAggregator connect failed " + statsImplementationClass);
      }
    }
    Statistics tblStats=new Statistics();
    org.apache.hadoop.hive.metastore.api.Table tTable=table.getTTable();
    Map<String,String> parameters=tTable.getParameters();
    boolean tableStatsExist=this.existStats(parameters);
    for (    String statType : supportedStats) {
      if (parameters.containsKey(statType)) {
        tblStats.setStat(statType,Long.parseLong(parameters.get(statType)));
      }
    }
    List<Partition> partitions=getPartitionsList();
    boolean atomic=HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_STATS_ATOMIC);
    int maxPrefixLength=HiveConf.getIntVar(conf,HiveConf.ConfVars.HIVE_STATS_KEY_PREFIX_MAX_LENGTH);
    if (partitions == null) {
      if (!tableStatsExist && atomic) {
        return 0;
      }
      if (statsAggregator != null) {
        String aggKey=Utilities.getHashedStatsPrefix(work.getAggKey(),maxPrefixLength);
        updateStats(collectableStats,tblStats,statsAggregator,parameters,aggKey,atomic);
        statsAggregator.cleanUp(aggKey);
      }
 else       if (work.isClearAggregatorStats()) {
        for (        String statType : collectableStats) {
          if (parameters.containsKey(statType)) {
            tblStats.setStat(statType,0L);
          }
        }
      }
      parameters=tTable.getParameters();
      for (      String statType : collectableStats) {
        parameters.put(statType,Long.toString(tblStats.getStat(statType)));
      }
      tTable.setParameters(parameters);
      String tableFullName=table.getDbName() + "." + table.getTableName();
      db.alterTable(tableFullName,new Table(tTable));
      console.printInfo("Table " + tableFullName + " stats: ["+ tblStats.toString()+ ']');
    }
 else {
      for (      Partition partn : partitions) {
        org.apache.hadoop.hive.metastore.api.Partition tPart=partn.getTPartition();
        parameters=tPart.getParameters();
        boolean hasStats=this.existStats(parameters);
        if (!hasStats && atomic) {
          continue;
        }
        Map<String,Long> currentValues=new HashMap<String,Long>();
        for (        String statType : supportedStats) {
          Long val=parameters.containsKey(statType) ? Long.parseLong(parameters.get(statType)) : 0L;
          currentValues.put(statType,val);
        }
        Statistics newPartStats=new Statistics();
        String partitionID=Utilities.getHashedStatsPrefix(work.getAggKey() + Warehouse.makePartPath(partn.getSpec()),maxPrefixLength);
        LOG.info("Stats aggregator : " + partitionID);
        if (statsAggregator != null) {
          updateStats(collectableStats,newPartStats,statsAggregator,parameters,partitionID,atomic);
          statsAggregator.cleanUp(partitionID);
        }
 else {
          for (          String statType : collectableStats) {
            if (work.isClearAggregatorStats()) {
              if (parameters.containsKey(statType)) {
                newPartStats.setStat(statType,0L);
              }
            }
 else {
              newPartStats.setStat(statType,currentValues.get(statType));
            }
          }
        }
        FileStatus[] partfileStatus=wh.getFileStatusesForPartition(tPart);
        newPartStats.setStat(StatsSetupConst.NUM_FILES,partfileStatus.length);
        long partSize=0L;
        for (int i=0; i < partfileStatus.length; i++) {
          partSize+=partfileStatus[i].getLen();
        }
        newPartStats.setStat(StatsSetupConst.TOTAL_SIZE,partSize);
        for (        String statType : supportedStats) {
          long statValue=newPartStats.getStat(statType);
          if (statValue >= 0) {
            parameters.put(statType,Long.toString(newPartStats.getStat(statType)));
          }
        }
        tPart.setParameters(parameters);
        String tableFullName=table.getDbName() + "." + table.getTableName();
        db.alterPartition(tableFullName,new Partition(table,tPart));
        console.printInfo("Partition " + tableFullName + partn.getSpec()+ " stats: ["+ newPartStats.toString()+ ']');
      }
    }
  }
 catch (  Exception e) {
    console.printInfo("[Warning] could not update stats.","Failed with exception " + e.getMessage() + "\n"+ StringUtils.stringifyException(e));
    if (work.isStatsReliable()) {
      ret=1;
    }
  }
 finally {
    if (statsAggregator != null) {
      statsAggregator.closeConnection();
    }
  }
  return ret;
}
