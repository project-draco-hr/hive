{
  JobClient jc=th.getJobClient();
  RunningJob rj=th.getRunningJob();
  String lastReport="";
  SimpleDateFormat dateFormat=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss,SSS");
  long reportTime=System.currentTimeMillis();
  long maxReportInterval=60 * 1000;
  boolean fatal=false;
  StringBuilder errMsg=new StringBuilder();
  long pullInterval=HiveConf.getLongVar(job,HiveConf.ConfVars.HIVECOUNTERSPULLINTERVAL);
  boolean initializing=true;
  boolean initOutputPrinted=false;
  while (!rj.isComplete()) {
    try {
      Thread.sleep(pullInterval);
    }
 catch (    InterruptedException e) {
    }
    if (initializing && ShimLoader.getHadoopShims().isJobPreparing(rj)) {
      continue;
    }
 else {
      initializing=false;
    }
    if (!initOutputPrinted) {
      SessionState ss=SessionState.get();
      String logMapper;
      String logReducer;
      TaskReport[] mappers=jc.getMapTaskReports(rj.getJobID());
      if (mappers == null) {
        logMapper="no information for number of mappers; ";
      }
 else {
        int numMap=mappers.length;
        if (ss != null) {
          ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_NUM_MAPPERS,Integer.toString(numMap));
        }
        logMapper="number of mappers: " + numMap + "; ";
      }
      TaskReport[] reducers=jc.getReduceTaskReports(rj.getJobID());
      if (reducers == null) {
        logReducer="no information for number of reducers. ";
      }
 else {
        int numReduce=reducers.length;
        if (ss != null) {
          ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_NUM_REDUCERS,Integer.toString(numReduce));
        }
        logReducer="number of reducers: " + numReduce;
      }
      console.printInfo("Hadoop job information for " + getId() + ": "+ logMapper+ logReducer);
      initOutputPrinted=true;
    }
    RunningJob newRj=jc.getJob(rj.getJobID());
    if (newRj == null) {
      throw new IOException("Could not find status of job: + rj.getJobID()");
    }
 else {
      th.setRunningJob(newRj);
      rj=newRj;
    }
    if (fatal) {
      continue;
    }
    Counters ctrs=th.getCounters();
    if (fatal=this.callBackObj.checkFatalErrors(ctrs,errMsg)) {
      console.printError("[Fatal Error] " + errMsg.toString() + ". Killing the job.");
      rj.killJob();
      continue;
    }
    errMsg.setLength(0);
    updateCounters(ctrs,rj);
    String report=" " + getId() + " map = "+ mapProgress+ "%,  reduce = "+ reduceProgress+ "%";
    if (!report.equals(lastReport) || System.currentTimeMillis() >= reportTime + maxReportInterval) {
      String output=dateFormat.format(Calendar.getInstance().getTime()) + report;
      SessionState ss=SessionState.get();
      if (ss != null) {
        ss.getHiveHistory().setTaskCounters(SessionState.get().getQueryId(),getId(),ctrs);
        ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_HADOOP_PROGRESS,output);
        ss.getHiveHistory().progressTask(SessionState.get().getQueryId(),this.task);
        this.callBackObj.logPlanProgress(ss);
      }
      console.printInfo(output);
      lastReport=report;
      reportTime=System.currentTimeMillis();
    }
  }
  boolean success;
  Counters ctrs=th.getCounters();
  if (fatal) {
    success=false;
  }
 else {
    if (checkFatalErrors(ctrs,errMsg)) {
      console.printError("[Fatal Error] " + errMsg.toString());
      success=false;
    }
 else {
      success=rj.isSuccessful();
    }
  }
  this.task.setDone();
  updateCounters(ctrs,rj);
  SessionState ss=SessionState.get();
  if (ss != null) {
    this.callBackObj.logPlanProgress(ss);
  }
  return (success);
}
