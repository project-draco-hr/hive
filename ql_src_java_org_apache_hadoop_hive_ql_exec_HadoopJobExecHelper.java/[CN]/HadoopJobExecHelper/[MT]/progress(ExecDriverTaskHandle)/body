{
  JobClient jc=th.getJobClient();
  RunningJob rj=th.getRunningJob();
  String lastReport="";
  SimpleDateFormat dateFormat=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss,SSS");
  long reportTime=System.currentTimeMillis();
  long maxReportInterval=60 * 1000;
  boolean fatal=false;
  StringBuilder errMsg=new StringBuilder();
  long pullInterval=HiveConf.getLongVar(job,HiveConf.ConfVars.HIVECOUNTERSPULLINTERVAL);
  boolean initializing=true;
  boolean initOutputPrinted=false;
  long cpuMsec=-1;
  int numMap=-1;
  int numReduce=-1;
  while (!rj.isComplete()) {
    try {
      Thread.sleep(pullInterval);
    }
 catch (    InterruptedException e) {
    }
    if (initializing && ShimLoader.getHadoopShims().isJobPreparing(rj)) {
      continue;
    }
 else {
      initializing=false;
    }
    if (!initOutputPrinted) {
      SessionState ss=SessionState.get();
      String logMapper;
      String logReducer;
      TaskReport[] mappers=jc.getMapTaskReports(rj.getJobID());
      if (mappers == null) {
        logMapper="no information for number of mappers; ";
      }
 else {
        numMap=mappers.length;
        if (ss != null) {
          ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_NUM_MAPPERS,Integer.toString(numMap));
        }
        logMapper="number of mappers: " + numMap + "; ";
      }
      TaskReport[] reducers=jc.getReduceTaskReports(rj.getJobID());
      if (reducers == null) {
        logReducer="no information for number of reducers. ";
      }
 else {
        numReduce=reducers.length;
        if (ss != null) {
          ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_NUM_REDUCERS,Integer.toString(numReduce));
        }
        logReducer="number of reducers: " + numReduce;
      }
      console.printInfo("Hadoop job information for " + getId() + ": "+ logMapper+ logReducer);
      initOutputPrinted=true;
    }
    RunningJob newRj=jc.getJob(rj.getJobID());
    if (newRj == null) {
      throw new IOException("Could not find status of job: + rj.getJobID()");
    }
 else {
      th.setRunningJob(newRj);
      rj=newRj;
    }
    if (fatal) {
      continue;
    }
    Counters ctrs=th.getCounters();
    if (fatal=checkFatalErrors(ctrs,errMsg)) {
      console.printError("[Fatal Error] " + errMsg.toString() + ". Killing the job.");
      rj.killJob();
      continue;
    }
    errMsg.setLength(0);
    updateCounters(ctrs,rj);
    String report=" " + getId() + " map = "+ mapProgress+ "%,  reduce = "+ reduceProgress+ "%";
    if (!report.equals(lastReport) || System.currentTimeMillis() >= reportTime + maxReportInterval) {
      Counter counterCpuMsec=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","CPU_MILLISECONDS");
      if (counterCpuMsec != null) {
        long newCpuMSec=counterCpuMsec.getValue();
        if (newCpuMSec > 0) {
          cpuMsec=newCpuMSec;
          report+=", Cumulative CPU " + (cpuMsec / 1000D) + " sec";
        }
      }
      String output=dateFormat.format(Calendar.getInstance().getTime()) + report;
      SessionState ss=SessionState.get();
      if (ss != null) {
        ss.getHiveHistory().setTaskCounters(SessionState.get().getQueryId(),getId(),ctrs);
        ss.getHiveHistory().setTaskProperty(SessionState.get().getQueryId(),getId(),Keys.TASK_HADOOP_PROGRESS,output);
        ss.getHiveHistory().progressTask(SessionState.get().getQueryId(),this.task);
        this.callBackObj.logPlanProgress(ss);
      }
      console.printInfo(output);
      lastReport=report;
      reportTime=System.currentTimeMillis();
    }
  }
  if (cpuMsec > 0) {
    console.printInfo("MapReduce Total cumulative CPU time: " + Utilities.formatMsecToStr(cpuMsec));
  }
  boolean success;
  Counters ctrs=th.getCounters();
  if (fatal) {
    success=false;
  }
 else {
    if (checkFatalErrors(ctrs,errMsg)) {
      console.printError("[Fatal Error] " + errMsg.toString());
      success=false;
    }
 else {
      success=rj.isSuccessful();
    }
  }
  Counter counterCpuMsec=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","CPU_MILLISECONDS");
  if (counterCpuMsec != null) {
    long newCpuMSec=counterCpuMsec.getValue();
    if (newCpuMSec > cpuMsec) {
      cpuMsec=newCpuMSec;
    }
  }
  MapRedStats mapRedStats=new MapRedStats(numMap,numReduce,cpuMsec,success,rj.getID().toString());
  Counter ctr;
  ctr=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","REDUCE_SHUFFLE_BYTES");
  if (ctr != null) {
    mapRedStats.setReduceShuffleBytes(ctr.getValue());
  }
  ctr=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","MAP_INPUT_RECORDS");
  if (ctr != null) {
    mapRedStats.setMapInputRecords(ctr.getValue());
  }
  ctr=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","MAP_OUTPUT_RECORDS");
  if (ctr != null) {
    mapRedStats.setMapOutputRecords(ctr.getValue());
  }
  ctr=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","REDUCE_INPUT_RECORDS");
  if (ctr != null) {
    mapRedStats.setReduceInputRecords(ctr.getValue());
  }
  ctr=ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter","REDUCE_OUTPUT_RECORDS");
  if (ctr != null) {
    mapRedStats.setReduceOutputRecords(ctr.getValue());
  }
  ctr=ctrs.findCounter("FileSystemCounters","HDFS_BYTES_READ");
  if (ctr != null) {
    mapRedStats.setHdfsRead(ctr.getValue());
  }
  ctr=ctrs.findCounter("FileSystemCounters","HDFS_BYTES_WRITTEN");
  if (ctr != null) {
    mapRedStats.setHdfsWrite(ctr.getValue());
  }
  this.task.setDone();
  updateCounters(ctrs,rj);
  SessionState ss=SessionState.get();
  if (ss != null) {
    this.callBackObj.logPlanProgress(ss);
  }
  return mapRedStats;
}
