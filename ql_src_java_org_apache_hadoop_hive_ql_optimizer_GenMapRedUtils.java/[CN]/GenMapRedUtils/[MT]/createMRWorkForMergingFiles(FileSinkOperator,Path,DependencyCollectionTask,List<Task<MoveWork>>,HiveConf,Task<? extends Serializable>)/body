{
  FileSinkDesc fsInputDesc=fsInput.getConf();
  RowSchema inputRS=fsInput.getSchema();
  TableScanOperator tsMerge=GenMapRedUtils.createTemporaryTableScanOperator(fsInput.getCompilationOpContext(),inputRS);
  TableDesc ts=(TableDesc)fsInputDesc.getTableInfo().clone();
  FileSinkDesc fsOutputDesc=new FileSinkDesc(finalName,ts,conf.getBoolVar(ConfVars.COMPRESSRESULT));
  FileSinkOperator fsOutput=(FileSinkOperator)OperatorFactory.getAndMakeChild(fsOutputDesc,inputRS,tsMerge);
  DynamicPartitionCtx dpCtx=fsInputDesc.getDynPartCtx();
  if (dpCtx != null && dpCtx.getNumDPCols() > 0) {
    ArrayList<ColumnInfo> signature=inputRS.getSignature();
    String tblAlias=fsInputDesc.getTableInfo().getTableName();
    for (    String dpCol : dpCtx.getDPColNames()) {
      ColumnInfo colInfo=new ColumnInfo(dpCol,TypeInfoFactory.stringTypeInfo,tblAlias,true);
      signature.add(colInfo);
    }
    inputRS.setSignature(signature);
    DynamicPartitionCtx dpCtx2=new DynamicPartitionCtx(dpCtx);
    fsOutputDesc.setDynPartCtx(dpCtx2);
    usePartitionColumns(fsInputDesc.getTableInfo().getProperties(),dpCtx.getDPColNames());
  }
 else {
    fsInputDesc.getTableInfo().getProperties().remove(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
  }
  MoveWork dummyMv=new MoveWork(null,null,null,new LoadFileDesc(fsInputDesc.getFinalDirName(),finalName,true,null,null),false);
  MapWork cplan;
  Serializable work;
  if ((conf.getBoolVar(ConfVars.HIVEMERGERCFILEBLOCKLEVEL) && fsInputDesc.getTableInfo().getInputFileFormatClass().equals(RCFileInputFormat.class)) || (conf.getBoolVar(ConfVars.HIVEMERGEORCFILESTRIPELEVEL) && fsInputDesc.getTableInfo().getInputFileFormatClass().equals(OrcInputFormat.class))) {
    cplan=GenMapRedUtils.createMergeTask(fsInputDesc,finalName,dpCtx != null && dpCtx.getNumDPCols() > 0,fsInput.getCompilationOpContext());
    if (conf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
      work=new TezWork(conf.getVar(HiveConf.ConfVars.HIVEQUERYID),conf);
      cplan.setName("File Merge");
      ((TezWork)work).add(cplan);
    }
 else     if (conf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals("spark")) {
      work=new SparkWork(conf.getVar(HiveConf.ConfVars.HIVEQUERYID));
      cplan.setName("Spark Merge File Work");
      ((SparkWork)work).add(cplan);
    }
 else {
      work=cplan;
    }
  }
 else {
    cplan=createMRWorkForMergingFiles(conf,tsMerge,fsInputDesc);
    if (conf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
      work=new TezWork(conf.getVar(HiveConf.ConfVars.HIVEQUERYID),conf);
      cplan.setName("File Merge");
      ((TezWork)work).add(cplan);
    }
 else     if (conf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals("spark")) {
      work=new SparkWork(conf.getVar(HiveConf.ConfVars.HIVEQUERYID));
      cplan.setName("Spark Merge File Work");
      ((SparkWork)work).add(cplan);
    }
 else {
      work=new MapredWork();
      ((MapredWork)work).setMapWork(cplan);
    }
  }
  cplan.setInputformat("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat");
  ConditionalTask cndTsk=GenMapRedUtils.createCondTask(conf,currTask,dummyMv,work,fsInputDesc.getFinalDirName().toString());
  ConditionalResolverMergeFilesCtx mrCtx=(ConditionalResolverMergeFilesCtx)cndTsk.getResolverCtx();
  mrCtx.setDPCtx(fsInputDesc.getDynPartCtx());
  mrCtx.setLbCtx(fsInputDesc.getLbCtx());
  linkMoveTask(fsOutput,cndTsk,mvTasks,conf,dependencyTask);
}
