{
  AlterTableDesc alterTblDesc;
switch (ast.getChild(0).getType()) {
case HiveParser.TOK_NOT_CLUSTERED:
    alterTblDesc=new AlterTableDesc(tableName,-1,new ArrayList<String>(),new ArrayList<Order>(),partSpec);
  break;
case HiveParser.TOK_NOT_SORTED:
alterTblDesc=new AlterTableDesc(tableName,true,partSpec);
break;
case HiveParser.TOK_ALTERTABLE_BUCKETS:
ASTNode buckets=(ASTNode)ast.getChild(0);
List<String> bucketCols=getColumnNames((ASTNode)buckets.getChild(0));
List<Order> sortCols=new ArrayList<Order>();
int numBuckets=-1;
if (buckets.getChildCount() == 2) {
numBuckets=Integer.parseInt(buckets.getChild(1).getText());
}
 else {
sortCols=getColumnNamesOrder((ASTNode)buckets.getChild(1));
numBuckets=Integer.parseInt(buckets.getChild(2).getText());
}
if (numBuckets <= 0) {
throw new SemanticException(ErrorMsg.INVALID_BUCKET_NUMBER.getMsg());
}
alterTblDesc=new AlterTableDesc(tableName,numBuckets,bucketCols,sortCols,partSpec);
break;
default :
throw new SemanticException("Invalid operation " + ast.getChild(0).getType());
}
addInputsOutputsAlterTable(tableName,partSpec,alterTblDesc);
rootTasks.add(TaskFactory.get(new DDLWork(getInputs(),getOutputs(),alterTblDesc),conf));
}
