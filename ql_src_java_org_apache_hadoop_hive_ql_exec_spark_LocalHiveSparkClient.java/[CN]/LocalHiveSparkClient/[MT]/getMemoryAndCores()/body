{
  SparkContext sparkContext=sc.sc();
  SparkConf sparkConf=sparkContext.conf();
  int cores=sparkConf.getInt("spark.executor.cores",1);
  double memoryFraction=sparkConf.getDouble("spark.shuffle.memoryFraction",0.2);
  long memoryPerTask=(long)(sparkContext.executorMemory() * memoryFraction * 1024* 1024 / cores);
  int executors=sparkContext.getExecutorMemoryStatus().size();
  int totalCores=executors * cores;
  LOG.info("Spark cluster current has executors: " + executors + ", cores per executor: "+ cores+ ", memory per executor: "+ sparkContext.executorMemory()+ "M, shuffle memoryFraction: "+ memoryFraction);
  return new Tuple2<Long,Integer>(Long.valueOf(memoryPerTask),Integer.valueOf(totalCores));
}
