{
  if (localWork == null || localWorkInited) {
    return;
  }
  localWorkInited=true;
  this.localWork=localWork;
  aliasToMergeQueue=new HashMap<String,MergeQueue>();
  Map<String,FetchWork> aliasToFetchWork=localWork.getAliasToFetchWork();
  Map<String,Operator<? extends OperatorDesc>> aliasToWork=localWork.getAliasToWork();
  Map<String,DummyStoreOperator> aliasToSinkWork=conf.getAliasToSink();
  for (  Map.Entry<String,FetchWork> entry : aliasToFetchWork.entrySet()) {
    String alias=entry.getKey();
    FetchWork fetchWork=entry.getValue();
    JobConf jobClone=new JobConf(hconf);
    if (UserGroupInformation.isSecurityEnabled()) {
      String hadoopAuthToken=System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);
      if (hadoopAuthToken != null) {
        jobClone.set("mapreduce.job.credentials.binary",hadoopAuthToken);
      }
    }
    TableScanOperator ts=(TableScanOperator)aliasToWork.get(alias);
    ColumnProjectionUtils.appendReadColumns(jobClone,ts.getNeededColumnIDs(),ts.getNeededColumns());
    HiveInputFormat.pushFilters(jobClone,ts);
    AcidUtils.setTransactionalTableScan(jobClone,ts.getConf().isAcidTable());
    ts.passExecContext(getExecContext());
    FetchOperator fetchOp=new FetchOperator(fetchWork,jobClone);
    ts.initialize(jobClone,new ObjectInspector[]{fetchOp.getOutputObjectInspector()});
    fetchOp.clearFetchContext();
    DummyStoreOperator sinkOp=aliasToSinkWork.get(alias);
    MergeQueue mergeQueue=new MergeQueue(alias,fetchWork,jobClone,ts,sinkOp);
    aliasToMergeQueue.put(alias,mergeQueue);
    l4j.info("fetch operators for " + alias + " initialized");
  }
}
