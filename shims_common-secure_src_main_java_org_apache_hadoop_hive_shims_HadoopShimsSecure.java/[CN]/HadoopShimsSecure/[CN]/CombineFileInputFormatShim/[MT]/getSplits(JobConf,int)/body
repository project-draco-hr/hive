{
  long minSize=job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZE"),0);
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZEPERNODE"),0) == 0) {
    super.setMinSplitSizeNode(minSize);
  }
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZEPERRACK"),0) == 0) {
    super.setMinSplitSizeRack(minSize);
  }
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMAXSPLITSIZE"),0) == 0) {
    super.setMaxSplitSize(minSize);
  }
  InputSplit[] splits=(InputSplit[])super.getSplits(job,numSplits);
  InputSplitShim[] isplits=new InputSplitShim[splits.length];
  for (int pos=0; pos < splits.length; pos++) {
    isplits[pos]=new InputSplitShim((CombineFileSplit)splits[pos]);
  }
  return isplits;
}
