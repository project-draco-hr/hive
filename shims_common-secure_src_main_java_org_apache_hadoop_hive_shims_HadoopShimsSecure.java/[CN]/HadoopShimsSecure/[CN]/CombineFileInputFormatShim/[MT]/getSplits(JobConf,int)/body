{
  long minSize=job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZE"),0);
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZEPERNODE"),0) == 0) {
    super.setMinSplitSizeNode(minSize);
  }
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMINSPLITSIZEPERRACK"),0) == 0) {
    super.setMinSplitSizeRack(minSize);
  }
  if (job.getLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMAXSPLITSIZE"),0) == 0) {
    super.setMaxSplitSize(minSize);
  }
  InputSplit[] splits=super.getSplits(job,numSplits);
  ArrayList<InputSplitShim> inputSplitShims=new ArrayList<InputSplitShim>();
  for (int pos=0; pos < splits.length; pos++) {
    CombineFileSplit split=(CombineFileSplit)splits[pos];
    Set<Integer> dirIndices=getDirIndices(split.getPaths(),job);
    if (dirIndices.size() != split.getPaths().length) {
      List<Path> prunedPaths=prune(dirIndices,Arrays.asList(split.getPaths()));
      List<Long> prunedStartOffsets=prune(dirIndices,Arrays.asList(ArrayUtils.toObject(split.getStartOffsets())));
      List<Long> prunedLengths=prune(dirIndices,Arrays.asList(ArrayUtils.toObject(split.getLengths())));
      inputSplitShims.add(new InputSplitShim(job,prunedPaths.toArray(new Path[prunedPaths.size()]),Longs.toArray(prunedStartOffsets),Longs.toArray(prunedLengths),split.getLocations()));
    }
  }
  return inputSplitShims.toArray(new InputSplitShim[inputSplitShims.size()]);
}
