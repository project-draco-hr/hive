{
  super(context,baseWriter);
  this.context=context;
  jobInfo=HCatOutputFormat.getJobInfo(context);
  storageHandler=HCatUtil.getStorageHandler(context.getConfiguration(),jobInfo.getTableInfo().getStorerInfo());
  serDe=ReflectionUtils.newInstance(storageHandler.getSerDeClass(),context.getConfiguration());
  objectInspector=InternalUtil.createStructObjectInspector(jobInfo.getOutputSchema());
  try {
    InternalUtil.initializeOutputSerDe(serDe,context.getConfiguration(),jobInfo);
  }
 catch (  SerDeException e) {
    throw new IOException("Failed to inialize SerDe",e);
  }
  partColsToDel=jobInfo.getPosOfPartCols();
  dynamicPartitioningUsed=jobInfo.isDynamicPartitioningUsed();
  dynamicPartCols=jobInfo.getPosOfDynPartCols();
  maxDynamicPartitions=jobInfo.getMaxDynamicPartitions();
  if ((partColsToDel == null) || (dynamicPartitioningUsed && (dynamicPartCols == null))) {
    throw new HCatException("It seems that setSchema() is not called on " + "HCatOutputFormat. Please make sure that method is called.");
  }
  if (!dynamicPartitioningUsed) {
    this.baseDynamicSerDe=null;
    this.baseDynamicWriters=null;
    this.baseDynamicCommitters=null;
    this.dynamicContexts=null;
    this.dynamicObjectInspectors=null;
    this.dynamicOutputJobInfo=null;
  }
 else {
    this.baseDynamicSerDe=new HashMap<String,SerDe>();
    this.baseDynamicWriters=new HashMap<String,org.apache.hadoop.mapred.RecordWriter<? super WritableComparable<?>,? super Writable>>();
    this.baseDynamicCommitters=new HashMap<String,org.apache.hadoop.mapred.OutputCommitter>();
    this.dynamicContexts=new HashMap<String,org.apache.hadoop.mapred.TaskAttemptContext>();
    this.dynamicObjectInspectors=new HashMap<String,ObjectInspector>();
    this.dynamicOutputJobInfo=new HashMap<String,OutputJobInfo>();
  }
}
