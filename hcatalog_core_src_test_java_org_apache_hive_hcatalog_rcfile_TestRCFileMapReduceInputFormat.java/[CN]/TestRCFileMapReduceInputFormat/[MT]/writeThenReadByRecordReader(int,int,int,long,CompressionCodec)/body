{
  Path testDir=new Path(System.getProperty("test.tmp.dir",".") + "/mapred/testsmallfirstsplit");
  Path testFile=new Path(testDir,"test_rcfile");
  fs.delete(testFile,true);
  Configuration cloneConf=new Configuration(conf);
  RCFileOutputFormat.setColumnNumber(cloneConf,bytesArray.length);
  cloneConf.setInt(RCFile.RECORD_INTERVAL_CONF_STR,intervalRecordCount);
  RCFile.Writer writer=new RCFile.Writer(fs,cloneConf,testFile,null,codec);
  BytesRefArrayWritable bytes=new BytesRefArrayWritable(bytesArray.length);
  for (int i=0; i < bytesArray.length; i++) {
    BytesRefWritable cu=null;
    cu=new BytesRefWritable(bytesArray[i],0,bytesArray[i].length);
    bytes.set(i,cu);
  }
  for (int i=0; i < writeCount; i++) {
    writer.append(bytes);
  }
  writer.close();
  RCFileMapReduceInputFormat<LongWritable,BytesRefArrayWritable> inputFormat=new RCFileMapReduceInputFormat<LongWritable,BytesRefArrayWritable>();
  Configuration jonconf=new Configuration(cloneConf);
  jonconf.set("mapred.input.dir",testDir.toString());
  JobContext context=new Job(jonconf);
  context.getConfiguration().setLong(ShimLoader.getHadoopShims().getHadoopConfNames().get("MAPREDMAXSPLITSIZE"),maxSplitSize);
  List<InputSplit> splits=inputFormat.getSplits(context);
  assertEquals("splits length should be " + splitNumber,splits.size(),splitNumber);
  int readCount=0;
  for (int i=0; i < splits.size(); i++) {
    TaskAttemptContext tac=ShimLoader.getHadoopShims().getHCatShim().createTaskAttemptContext(jonconf,new TaskAttemptID());
    RecordReader<LongWritable,BytesRefArrayWritable> rr=inputFormat.createRecordReader(splits.get(i),tac);
    rr.initialize(splits.get(i),tac);
    while (rr.nextKeyValue()) {
      readCount++;
    }
  }
  assertEquals("readCount should be equal to writeCount",readCount,writeCount);
}
