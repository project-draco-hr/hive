{
  Configuration conf=jobContext.getConfiguration();
  InputJobInfo inputJobInfo;
  try {
    inputJobInfo=getJobInfo(conf);
  }
 catch (  Exception e) {
    throw new IOException(e);
  }
  List<InputSplit> splits=new ArrayList<InputSplit>();
  List<PartInfo> partitionInfoList=inputJobInfo.getPartitions();
  if (partitionInfoList == null) {
    return splits;
  }
  HiveStorageHandler storageHandler;
  JobConf jobConf;
  for (  PartInfo partitionInfo : partitionInfoList) {
    jobConf=HCatUtil.getJobConfFromContext(jobContext);
    setInputPath(jobConf,partitionInfo.getLocation());
    Map<String,String> jobProperties=partitionInfo.getJobProperties();
    HCatUtil.copyJobPropertiesToJobConf(jobProperties,jobConf);
    storageHandler=HCatUtil.getStorageHandler(jobConf,partitionInfo);
    Class inputFormatClass=storageHandler.getInputFormatClass();
    org.apache.hadoop.mapred.InputFormat inputFormat=getMapRedInputFormat(jobConf,inputFormatClass);
    int desiredNumSplits=conf.getInt(HCatConstants.HCAT_DESIRED_PARTITION_NUM_SPLITS,0);
    org.apache.hadoop.mapred.InputSplit[] baseSplits=inputFormat.getSplits(jobConf,desiredNumSplits);
    for (    org.apache.hadoop.mapred.InputSplit split : baseSplits) {
      splits.add(new HCatSplit(partitionInfo,split));
    }
  }
  return splits;
}
