{
  Configuration conf=jobContext.getConfiguration();
  InputJobInfo inputJobInfo;
  try {
    inputJobInfo=getJobInfo(conf);
  }
 catch (  Exception e) {
    throw new IOException(e);
  }
  List<InputSplit> splits=new ArrayList<InputSplit>();
  List<PartInfo> partitionInfoList=inputJobInfo.getPartitions();
  if (partitionInfoList == null) {
    return splits;
  }
  HiveStorageHandler storageHandler;
  JobConf jobConf;
  for (  PartInfo partitionInfo : partitionInfoList) {
    jobConf=HCatUtil.getJobConfFromContext(jobContext);
    setInputPath(jobConf,partitionInfo.getLocation());
    Map<String,String> jobProperties=partitionInfo.getJobProperties();
    HCatSchema allCols=new HCatSchema(new LinkedList<HCatFieldSchema>());
    for (    HCatFieldSchema field : inputJobInfo.getTableInfo().getDataColumns().getFields())     allCols.append(field);
    for (    HCatFieldSchema field : inputJobInfo.getTableInfo().getPartitionColumns().getFields())     allCols.append(field);
    HCatUtil.copyJobPropertiesToJobConf(jobProperties,jobConf);
    storageHandler=HCatUtil.getStorageHandler(jobConf,partitionInfo);
    Class inputFormatClass=storageHandler.getInputFormatClass();
    org.apache.hadoop.mapred.InputFormat inputFormat=getMapRedInputFormat(jobConf,inputFormatClass);
    int desiredNumSplits=conf.getInt(HCatConstants.HCAT_DESIRED_PARTITION_NUM_SPLITS,0);
    org.apache.hadoop.mapred.InputSplit[] baseSplits=inputFormat.getSplits(jobConf,desiredNumSplits);
    for (    org.apache.hadoop.mapred.InputSplit split : baseSplits) {
      splits.add(new HCatSplit(partitionInfo,split,allCols));
    }
  }
  return splits;
}
