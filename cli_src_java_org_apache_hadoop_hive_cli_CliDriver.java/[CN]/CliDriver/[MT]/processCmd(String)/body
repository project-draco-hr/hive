{
  CliSessionState ss=(CliSessionState)SessionState.get();
  String cmd_trimmed=cmd.trim();
  String[] tokens=cmd_trimmed.split("\\s+");
  String cmd_1=cmd_trimmed.substring(tokens[0].length()).trim();
  int ret=0;
  if (cmd_trimmed.toLowerCase().equals("quit") || cmd_trimmed.toLowerCase().equals("exit")) {
    ss.close();
    System.exit(0);
  }
 else   if (tokens[0].equalsIgnoreCase("source")) {
    File sourceFile=new File(cmd_1);
    if (!sourceFile.isFile()) {
      console.printError("File: " + cmd_1 + " is not a file.");
      ret=1;
    }
 else {
      try {
        this.processFile(cmd_1);
      }
 catch (      IOException e) {
        console.printError("Failed processing file " + cmd_1 + " "+ e.getLocalizedMessage(),org.apache.hadoop.util.StringUtils.stringifyException(e));
        ret=1;
      }
    }
  }
 else   if (cmd_trimmed.startsWith("!")) {
    String shell_cmd=cmd_trimmed.substring(1);
    try {
      Process executor=Runtime.getRuntime().exec(shell_cmd);
      StreamPrinter outPrinter=new StreamPrinter(executor.getInputStream(),null,ss.out);
      StreamPrinter errPrinter=new StreamPrinter(executor.getErrorStream(),null,ss.err);
      outPrinter.start();
      errPrinter.start();
      ret=executor.waitFor();
      if (ret != 0) {
        console.printError("Command failed with exit code = " + ret);
      }
    }
 catch (    Exception e) {
      console.printError("Exception raised from Shell command " + e.getLocalizedMessage(),org.apache.hadoop.util.StringUtils.stringifyException(e));
      ret=1;
    }
  }
 else   if (tokens[0].toLowerCase().equals("list")) {
    SessionState.ResourceType t;
    if (tokens.length < 2 || (t=SessionState.find_resource_type(tokens[1])) == null) {
      console.printError("Usage: list [" + StringUtils.join(SessionState.ResourceType.values(),"|") + "] [<value> [<value>]*]");
      ret=1;
    }
 else {
      List<String> filter=null;
      if (tokens.length >= 3) {
        System.arraycopy(tokens,2,tokens,0,tokens.length - 2);
        filter=Arrays.asList(tokens);
      }
      Set<String> s=ss.list_resource(t,filter);
      if (s != null && !s.isEmpty()) {
        ss.out.println(StringUtils.join(s,"\n"));
      }
    }
  }
 else   if (ss.isRemoteMode()) {
    HiveClient client=ss.getClient();
    PrintStream out=ss.out;
    PrintStream err=ss.err;
    try {
      client.execute(cmd_trimmed);
      List<String> results;
      do {
        results=client.fetchN(LINES_TO_FETCH);
        for (        String line : results) {
          out.println(line);
        }
      }
 while (results.size() == LINES_TO_FETCH);
    }
 catch (    HiveServerException e) {
      ret=e.getErrorCode();
      if (ret != 0) {
        String errMsg=e.getMessage();
        if (errMsg == null) {
          errMsg=e.toString();
        }
        ret=e.getErrorCode();
        err.println("[Hive Error]: " + errMsg);
      }
    }
catch (    TException e) {
      String errMsg=e.getMessage();
      if (errMsg == null) {
        errMsg=e.toString();
      }
      ret=-10002;
      err.println("[Thrift Error]: " + errMsg);
    }
 finally {
      try {
        client.clean();
      }
 catch (      TException e) {
        String errMsg=e.getMessage();
        if (errMsg == null) {
          errMsg=e.toString();
        }
        err.println("[Thrift Error]: Hive server is not cleaned due to thrift exception: " + errMsg);
      }
    }
  }
 else {
    CommandProcessor proc=CommandProcessorFactory.get(tokens[0],(HiveConf)conf);
    if (proc != null) {
      if (proc instanceof Driver) {
        Driver qp=(Driver)proc;
        PrintStream out=ss.out;
        long start=System.currentTimeMillis();
        if (ss.getIsVerbose()) {
          out.println(cmd);
        }
        ret=qp.run(cmd).getResponseCode();
        if (ret != 0) {
          qp.close();
          return ret;
        }
        ArrayList<String> res=new ArrayList<String>();
        if (HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER)) {
          boolean first_col=true;
          Schema sc=qp.getSchema();
          for (          FieldSchema fs : sc.getFieldSchemas()) {
            if (!first_col) {
              out.print('\t');
            }
            out.print(fs.getName());
            first_col=false;
          }
          out.println();
        }
        try {
          while (qp.getResults(res)) {
            for (            String r : res) {
              out.println(r);
            }
            res.clear();
            if (out.checkError()) {
              break;
            }
          }
        }
 catch (        IOException e) {
          console.printError("Failed with exception " + e.getClass().getName() + ":"+ e.getMessage(),"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
          ret=1;
        }
        int cret=qp.close();
        if (ret == 0) {
          ret=cret;
        }
        long end=System.currentTimeMillis();
        if (end > start) {
          double timeTaken=(end - start) / 1000.0;
          console.printInfo("Time taken: " + timeTaken + " seconds",null);
        }
      }
 else {
        if (ss.getIsVerbose()) {
          ss.out.println(tokens[0] + " " + cmd_1);
        }
        ret=proc.run(cmd_1).getResponseCode();
      }
    }
  }
  return ret;
}
