{
  SparkClient client=getInstance(hiveConf);
  SparkContext sc=client.sc.sc();
  SparkConf sparkConf=sc.conf();
  int cores=sparkConf.getInt("spark.executor.cores",sc.defaultParallelism());
  double memoryFraction=sparkConf.getDouble("spark.shuffle.memoryFraction",0.2);
  long memoryPerTask=(long)(sc.executorMemory() * memoryFraction * 1024* 1024 / cores);
  int executors=sc.getExecutorMemoryStatus().size();
  int totalCores=executors * cores;
  LOG.info("Spark cluster current has executors: " + executors + ", cores per executor: "+ cores+ ", memory per executor: "+ sc.executorMemory()+ "M, shuffle memoryFraction: "+ memoryFraction);
  return new Tuple2<Long,Integer>(Long.valueOf(memoryPerTask),Integer.valueOf(totalCores));
}
