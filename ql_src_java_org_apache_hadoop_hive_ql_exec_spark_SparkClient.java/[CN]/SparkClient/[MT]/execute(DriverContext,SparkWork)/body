{
  Context ctx=driverContext.getCtx();
  HiveConf hiveConf=(HiveConf)ctx.getConf();
  refreshLocalResources(sparkWork,hiveConf);
  JobConf jobConf=new JobConf(hiveConf);
  Path emptyScratchDir;
  try {
    emptyScratchDir=ctx.getMRTmpPath();
    FileSystem fs=emptyScratchDir.getFileSystem(jobConf);
    fs.mkdirs(emptyScratchDir);
  }
 catch (  IOException e) {
    LOG.error("Error launching map-reduce job",e);
    return 5;
  }
  SparkPlanGenerator gen=new SparkPlanGenerator(sc,ctx,jobConf,emptyScratchDir);
  SparkPlan plan;
  try {
    plan=gen.generate(sparkWork);
  }
 catch (  Exception e) {
    LOG.error("Error generating Spark Plan",e);
    return 2;
  }
  try {
    JavaPairRDD<HiveKey,BytesWritable> finalRDD=plan.generateGraph();
    FutureAction future=finalRDD.foreachAsync(HiveVoidFunction.getInstance());
    SimpleSparkJobStatus sparkJobStatus=new SimpleSparkJobStatus((Integer)future.jobIds().last(),jobStateListener,jobProgressListener);
    SparkJobMonitor monitor=new SparkJobMonitor(sparkJobStatus);
    monitor.startMonitor();
  }
 catch (  Exception e) {
    LOG.error("Error executing Spark Plan",e);
    return 1;
  }
  return 0;
}
