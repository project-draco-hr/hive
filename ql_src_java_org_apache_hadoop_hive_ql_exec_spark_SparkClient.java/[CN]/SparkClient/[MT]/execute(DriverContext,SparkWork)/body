{
  HiveConf hiveConf=(HiveConf)driverContext.getCtx().getConf();
  refreshLocalResources(sparkWork,hiveConf);
  MapWork mapWork=sparkWork.getMapWork();
  ReduceWork redWork=sparkWork.getReduceWork();
  JobConf jobConf=new JobConf(hiveConf);
  Context ctx=driverContext.getCtx();
  Path emptyScratchDir;
  try {
    if (ctx == null) {
      ctx=new Context(jobConf);
    }
    emptyScratchDir=ctx.getMRTmpPath();
    FileSystem fs=emptyScratchDir.getFileSystem(jobConf);
    fs.mkdirs(emptyScratchDir);
  }
 catch (  IOException e) {
    e.printStackTrace();
    System.err.println("Error launching map-reduce job" + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  List<Path> inputPaths;
  try {
    inputPaths=Utilities.getInputPaths(jobConf,mapWork,emptyScratchDir,ctx,false);
  }
 catch (  Exception e2) {
    e2.printStackTrace();
    return -1;
  }
  Utilities.setInputPaths(jobConf,inputPaths);
  Utilities.setMapWork(jobConf,mapWork,emptyScratchDir,true);
  if (redWork != null)   Utilities.setReduceWork(jobConf,redWork,emptyScratchDir,true);
  try {
    Utilities.createTmpDirs(jobConf,mapWork);
    Utilities.createTmpDirs(jobConf,redWork);
  }
 catch (  IOException e1) {
    e1.printStackTrace();
  }
  SparkPlanGenerator gen=new SparkPlanGenerator(sc,ctx,jobConf,emptyScratchDir);
  SparkPlan plan;
  try {
    plan=gen.generate(sparkWork);
  }
 catch (  Exception e) {
    e.printStackTrace();
    return 2;
  }
  plan.execute();
  return 0;
}
