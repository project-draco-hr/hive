{
  Context ctx=driverContext.getCtx();
  HiveConf hiveConf=(HiveConf)ctx.getConf();
  refreshLocalResources(sparkWork,hiveConf);
  JobConf jobConf=new JobConf(hiveConf);
  Path emptyScratchDir;
  try {
    emptyScratchDir=ctx.getMRTmpPath();
    FileSystem fs=emptyScratchDir.getFileSystem(jobConf);
    fs.mkdirs(emptyScratchDir);
  }
 catch (  IOException e) {
    e.printStackTrace();
    System.err.println("Error launching map-reduce job" + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  SparkPlanGenerator gen=new SparkPlanGenerator(sc,ctx,jobConf,emptyScratchDir);
  SparkPlan plan;
  try {
    plan=gen.generate(sparkWork);
  }
 catch (  Exception e) {
    e.printStackTrace();
    return 2;
  }
  plan.execute();
  return 0;
}
