{
  SparkConf sparkConf=new SparkConf();
  sparkConf.set("spark.master",SPARK_DEFAULT_MASTER);
  sparkConf.set("spark.app.name",SAPRK_DEFAULT_APP_NAME);
  sparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer");
  sparkConf.set("spark.default.parallelism","1");
  InputStream inputStream=null;
  try {
    inputStream=this.getClass().getClassLoader().getResourceAsStream(SPARK_DEFAULT_CONF_FILE);
    if (inputStream != null) {
      LOG.info("loading spark properties from:" + SPARK_DEFAULT_CONF_FILE);
      Properties properties=new Properties();
      properties.load(inputStream);
      for (      String propertyName : properties.stringPropertyNames()) {
        if (propertyName.startsWith("spark")) {
          String value=properties.getProperty(propertyName);
          sparkConf.set(propertyName,properties.getProperty(propertyName));
          LOG.info(String.format("load spark configuration from %s (%s -> %s).",SPARK_DEFAULT_CONF_FILE,propertyName,value));
        }
      }
    }
  }
 catch (  IOException e) {
    LOG.info("Failed to open spark configuration file:" + SPARK_DEFAULT_CONF_FILE,e);
  }
 finally {
    if (inputStream != null) {
      try {
        inputStream.close();
      }
 catch (      IOException e) {
        LOG.debug("Failed to close inputstream.",e);
      }
    }
  }
  Iterator<Map.Entry<String,String>> iterator=hiveConf.iterator();
  while (iterator.hasNext()) {
    Map.Entry<String,String> entry=iterator.next();
    String propertyName=entry.getKey();
    if (propertyName.startsWith("spark")) {
      String value=entry.getValue();
      sparkConf.set(propertyName,value);
      LOG.info(String.format("load spark configuration from hive configuration (%s -> %s).",propertyName,value));
    }
  }
  return sparkConf;
}
