{
  this.activeJobs=Maps.newConcurrentMap();
  this.jcLock=new Object();
  this.shutdownLock=new Object();
  SparkConf conf=new SparkConf();
  String serverAddress=null;
  int serverPort=-1;
  for (int idx=0; idx < args.length; idx+=2) {
    String key=args[idx];
    if (key.equals("--remote-host")) {
      serverAddress=getArg(args,idx);
    }
 else     if (key.equals("--remote-port")) {
      serverPort=Integer.parseInt(getArg(args,idx));
    }
 else     if (key.equals("--secret")) {
      conf.set(SparkClientFactory.CONF_KEY_SECRET,getArg(args,idx));
    }
 else     if (key.equals("--conf")) {
      String[] val=getArg(args,idx).split("[=]",2);
      conf.set(val[0],val[1]);
    }
 else {
      throw new IllegalArgumentException("Invalid command line: " + Joiner.on(" ").join(args));
    }
  }
  executor=Executors.newCachedThreadPool();
  LOG.info("Connecting to: {}:{}",serverAddress,serverPort);
  Map<String,String> mapConf=Maps.newHashMap();
  for (  Tuple2<String,String> e : conf.getAll()) {
    mapConf.put(e._1(),e._2());
    LOG.debug("Remote Driver configured with: " + e._1() + "="+ e._2());
  }
  String secret=mapConf.get(SparkClientFactory.CONF_KEY_SECRET);
  Preconditions.checkArgument(secret != null,"No secret provided.");
  int threadCount=new RpcConfiguration(mapConf).getRpcThreadCount();
  this.egroup=new NioEventLoopGroup(threadCount,new ThreadFactoryBuilder().setNameFormat("Driver-RPC-Handler-%d").setDaemon(true).build());
  this.protocol=new DriverProtocol();
  this.clientRpc=Rpc.createClient(mapConf,egroup,serverAddress,serverPort,secret,protocol).get();
  this.running=true;
  this.clientRpc.addListener(new Rpc.Listener(){
    @Override public void rpcClosed(    Rpc rpc){
      LOG.warn("Shutting down driver because RPC channel was closed.");
      shutdown(null);
    }
  }
);
  try {
    JavaSparkContext sc=new JavaSparkContext(conf);
    sc.sc().addSparkListener(new ClientListener());
synchronized (jcLock) {
      jc=new JobContextImpl(sc);
      jcLock.notifyAll();
    }
  }
 catch (  Exception e) {
    LOG.error("Failed to start SparkContext.",e);
    shutdown(e);
synchronized (jcLock) {
      jcLock.notifyAll();
    }
    throw e;
  }
synchronized (jcLock) {
    for (Iterator<JobWrapper<?>> it=jobQueue.iterator(); it.hasNext(); ) {
      it.next().submit();
    }
  }
}
