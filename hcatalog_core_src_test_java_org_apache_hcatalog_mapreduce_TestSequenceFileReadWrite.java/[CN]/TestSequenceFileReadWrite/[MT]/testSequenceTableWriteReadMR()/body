{
  Initialize();
  String createTable="CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
  driver.run("drop table demo_table_2");
  int retCode1=driver.run(createTable).getResponseCode();
  assertTrue(retCode1 == 0);
  Configuration conf=new Configuration();
  conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,HCatUtil.serialize(hiveConf.getAllProperties()));
  Job job=new Job(conf,"Write-hcat-seq-table");
  job.setJarByClass(TestSequenceFileReadWrite.class);
  job.setMapperClass(Map.class);
  job.setOutputKeyClass(NullWritable.class);
  job.setOutputValueClass(DefaultHCatRecord.class);
  job.setInputFormatClass(TextInputFormat.class);
  TextInputFormat.setInputPaths(job,INPUT_FILE_NAME);
  HCatOutputFormat.setOutput(job,OutputJobInfo.create(MetaStoreUtils.DEFAULT_DATABASE_NAME,"demo_table_2",null));
  job.setOutputFormatClass(HCatOutputFormat.class);
  HCatOutputFormat.setSchema(job,getSchema());
  job.setNumReduceTasks(0);
  assertTrue(job.waitForCompletion(true));
  if (!HCatUtil.isHadoop23()) {
    new FileOutputCommitterContainer(job,null).commitJob(job);
  }
  assertTrue(job.isSuccessful());
  server.setBatchOn();
  server.registerQuery("C = load 'default.demo_table_2' using org.apache.hcatalog.pig.HCatLoader();");
  server.executeBatch();
  Iterator<Tuple> XIter=server.openIterator("C");
  int numTuplesRead=0;
  while (XIter.hasNext()) {
    Tuple t=XIter.next();
    assertEquals(3,t.size());
    assertEquals(t.get(0).toString(),"" + numTuplesRead);
    assertEquals(t.get(1).toString(),"a" + numTuplesRead);
    assertEquals(t.get(2).toString(),"b" + numTuplesRead);
    numTuplesRead++;
  }
  assertEquals(input.length,numTuplesRead);
}
