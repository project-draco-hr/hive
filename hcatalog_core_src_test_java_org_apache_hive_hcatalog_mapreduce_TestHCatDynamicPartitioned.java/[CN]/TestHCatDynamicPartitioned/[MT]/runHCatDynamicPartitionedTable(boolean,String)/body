{
  generateWriteRecords(NUM_RECORDS,NUM_PARTITIONS,0);
  runMRCreate(null,dataColumns,writeRecords,NUM_RECORDS,true,asSingleMapTask,customDynamicPathPattern);
  runMRRead(NUM_RECORDS);
  runMRRead(4,"p1 = \"0\"");
  runMRRead(8,"p1 = \"1\" or p1 = \"3\"");
  runMRRead(4,"p1 = \"4\"");
  String query="select * from " + tableName;
  int retCode=driver.run(query).getResponseCode();
  if (retCode != 0) {
    throw new Exception("Error " + retCode + " running query "+ query);
  }
  ArrayList<String> res=new ArrayList<String>();
  driver.getResults(res);
  assertEquals(NUM_RECORDS,res.size());
  IOException exc=null;
  try {
    generateWriteRecords(NUM_RECORDS,NUM_PARTITIONS,0);
    Job job=runMRCreate(null,dataColumns,writeRecords,NUM_RECORDS,false,true,customDynamicPathPattern);
    if (HCatUtil.isHadoop23()) {
      Assert.assertTrue(job.isSuccessful() == false);
    }
  }
 catch (  IOException e) {
    exc=e;
  }
  if (!HCatUtil.isHadoop23()) {
    assertTrue(exc != null);
    assertTrue(exc instanceof HCatException);
    assertTrue("Got exception of type [" + ((HCatException)exc).getErrorType().toString() + "] Expected ERROR_PUBLISHING_PARTITION or ERROR_MOVE_FAILED "+ "or ERROR_DUPLICATE_PARTITION",(ErrorType.ERROR_PUBLISHING_PARTITION == ((HCatException)exc).getErrorType()) || (ErrorType.ERROR_MOVE_FAILED == ((HCatException)exc).getErrorType()) || (ErrorType.ERROR_DUPLICATE_PARTITION == ((HCatException)exc).getErrorType()));
  }
  query="show partitions " + tableName;
  retCode=driver.run(query).getResponseCode();
  if (retCode != 0) {
    throw new Exception("Error " + retCode + " running query "+ query);
  }
  res=new ArrayList<String>();
  driver.getResults(res);
  assertEquals(NUM_PARTITIONS,res.size());
  query="select * from " + tableName;
  retCode=driver.run(query).getResponseCode();
  if (retCode != 0) {
    throw new Exception("Error " + retCode + " running query "+ query);
  }
  res=new ArrayList<String>();
  driver.getResults(res);
  assertEquals(NUM_RECORDS,res.size());
}
