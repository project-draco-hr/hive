{
  super.initialize(hconf,reporter);
  Path fpath=new Path((new Path(HiveConf.getVar(hconf,HiveConf.ConfVars.HADOOPMAPFILENAME))).toUri().getPath());
  ArrayList<Operator<? extends Serializable>> todo=new ArrayList<Operator<? extends Serializable>>();
  statsMap.put(Counter.DESERIALIZE_ERRORS,deserialize_error_count);
  for (  String onefile : conf.getPathToAliases().keySet()) {
    Path onepath=new Path(new Path(onefile).toUri().getPath());
    if (!onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {
      List<String> aliases=conf.getPathToAliases().get(onefile);
      for (      String onealias : aliases) {
        LOG.info("Adding alias " + onealias + " to work list for file "+ fpath.toUri().getPath());
        todo.add(conf.getAliasToWork().get(onealias));
      }
      if (deserializer != null) {
        continue;
      }
      partitionDesc pd=conf.getPathToPartitionInfo().get(onefile);
      LinkedHashMap<String,String> partSpec=pd.getPartSpec();
      tableDesc td=pd.getTableDesc();
      Properties p=td.getProperties();
      HiveConf.setVar(hconf,HiveConf.ConfVars.HIVETABLENAME,String.valueOf(p.getProperty("name")));
      HiveConf.setVar(hconf,HiveConf.ConfVars.HIVEPARTITIONNAME,String.valueOf(partSpec));
      try {
        Class sdclass=td.getDeserializerClass();
        if (sdclass == null) {
          String className=td.getSerdeClassName();
          if ((className == "") || (className == null)) {
            throw new HiveException("SerDe class or the SerDe class name is not set for table: " + td.getProperties().getProperty("name"));
          }
          sdclass=MapOperator.class.getClassLoader().loadClass(className);
        }
        deserializer=(Deserializer)sdclass.newInstance();
        deserializer.initialize(hconf,p);
        rowObjectInspector=(StructObjectInspector)deserializer.getObjectInspector();
        String pcols=p.getProperty(org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_PARTITION_COLUMNS);
        if (pcols != null && pcols.length() > 0) {
          partNames=new ArrayList<String>();
          partValues=new ArrayList<String>();
          partObjectInspectors=new ArrayList<ObjectInspector>();
          String[] partKeys=pcols.trim().split("/");
          for (          String key : partKeys) {
            partNames.add(key);
            partValues.add(partSpec.get(key));
            partObjectInspectors.add(ObjectInspectorFactory.getStandardPrimitiveObjectInspector(String.class));
          }
          StructObjectInspector partObjectInspector=ObjectInspectorFactory.getStandardStructObjectInspector(partNames,partObjectInspectors);
          rowWithPart=new Object[2];
          rowWithPart[1]=partValues;
          rowObjectInspector=ObjectInspectorFactory.getUnionStructObjectInspector(Arrays.asList(new StructObjectInspector[]{rowObjectInspector,partObjectInspector}));
        }
 else {
          partNames=null;
          partValues=null;
        }
        LOG.info("Got partitions: " + pcols);
      }
 catch (      SerDeException e) {
        e.printStackTrace();
        throw new HiveException(e);
      }
catch (      InstantiationException e) {
        throw new HiveException(e);
      }
catch (      IllegalAccessException e) {
        throw new HiveException(e);
      }
catch (      ClassNotFoundException e) {
        throw new HiveException(e);
      }
    }
  }
  if (todo.size() == 0) {
    LOG.error("Configuration does not have any alias for path: " + fpath.toUri().getPath());
    throw new HiveException("Configuration and input path are inconsistent");
  }
  this.setChildOperators(todo);
  this.setMapredWork(conf);
  this.setOutputCollector(out);
  for (  Operator op : todo) {
    op.initialize(hconf,reporter);
  }
}
