{
  Path fpath=new Path((new Path(HiveConf.getVar(hconf,HiveConf.ConfVars.HADOOPMAPFILENAME))).toUri().getPath());
  ArrayList<Operator<? extends Serializable>> children=new ArrayList<Operator<? extends Serializable>>();
  opCtxMap=new HashMap<MapInputPath,MapOpCtx>();
  statsMap.put(Counter.DESERIALIZE_ERRORS,deserialize_error_count);
  try {
    boolean done=false;
    for (    String onefile : conf.getPathToAliases().keySet()) {
      MapOpCtx opCtx=initObjectInspector(conf,hconf,onefile);
      Path onepath=new Path(new Path(onefile).toUri().getPath());
      List<String> aliases=conf.getPathToAliases().get(onefile);
      for (      String onealias : aliases) {
        Operator<? extends Serializable> op=conf.getAliasToWork().get(onealias);
        LOG.info("Adding alias " + onealias + " to work list for file "+ fpath.toUri().getPath());
        MapInputPath inp=new MapInputPath(onefile,onealias,op);
        opCtxMap.put(inp,opCtx);
        op.setParentOperators(new ArrayList<Operator<? extends Serializable>>());
        op.getParentOperators().add(this);
        if (!onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {
          children.add(op);
          LOG.info("dump " + op.getName() + " "+ opCtxMap.get(inp).getRowObjectInspector().getTypeName());
          if (!done) {
            deserializer=opCtxMap.get(inp).getDeserializer();
            isPartitioned=opCtxMap.get(inp).isPartitioned();
            rowWithPart=opCtxMap.get(inp).getRowWithPart();
            rowObjectInspector=opCtxMap.get(inp).getRowObjectInspector();
            done=true;
          }
        }
      }
    }
    if (children.size() == 0) {
      LOG.error("Configuration does not have any alias for path: " + fpath.toUri().getPath());
      throw new HiveException("Configuration and input path are inconsistent");
    }
    setChildOperators(children);
  }
 catch (  Exception e) {
    throw new HiveException(e);
  }
}
