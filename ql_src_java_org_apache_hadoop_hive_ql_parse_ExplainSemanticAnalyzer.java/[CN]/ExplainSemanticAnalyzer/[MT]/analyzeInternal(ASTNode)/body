{
  boolean extended=false;
  boolean formatted=false;
  boolean dependency=false;
  boolean logical=false;
  boolean authorize=false;
  for (int i=1; i < ast.getChildCount(); i++) {
    int explainOptions=ast.getChild(i).getType();
    if (explainOptions == HiveParser.KW_FORMATTED) {
      formatted=true;
    }
 else     if (explainOptions == HiveParser.KW_EXTENDED) {
      extended=true;
    }
 else     if (explainOptions == HiveParser.KW_DEPENDENCY) {
      dependency=true;
    }
 else     if (explainOptions == HiveParser.KW_LOGICAL) {
      logical=true;
    }
 else     if (explainOptions == HiveParser.KW_AUTHORIZATION) {
      authorize=true;
    }
  }
  ctx.setExplain(true);
  ctx.setExplainLogical(logical);
  ASTNode input=(ASTNode)ast.getChild(0);
  BaseSemanticAnalyzer sem=SemanticAnalyzerFactory.get(conf,input);
  sem.analyze(input,ctx);
  sem.validate();
  ctx.setResFile(ctx.getLocalTmpPath());
  List<Task<? extends Serializable>> tasks=sem.getRootTasks();
  if (tasks == null) {
    tasks=Collections.emptyList();
  }
  FetchTask fetchTask=sem.getFetchTask();
  if (fetchTask != null) {
    fetchTask.getWork().initializeForFetch();
  }
  ParseContext pCtx=null;
  if (sem instanceof SemanticAnalyzer) {
    pCtx=((SemanticAnalyzer)sem).getParseContext();
  }
  ExplainWork work=new ExplainWork(ctx.getResFile(),pCtx,tasks,fetchTask,input.dump(),sem,extended,formatted,dependency,logical,authorize);
  work.setAppendTaskType(HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVEEXPLAINDEPENDENCYAPPENDTASKTYPES));
  Task<? extends Serializable> explTask=TaskFactory.get(work,conf);
  fieldList=explTask.getResultSchema();
  rootTasks.add(explTask);
}
