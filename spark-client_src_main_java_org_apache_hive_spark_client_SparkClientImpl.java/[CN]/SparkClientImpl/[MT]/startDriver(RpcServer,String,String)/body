{
  Runnable runnable;
  final String serverAddress=rpcServer.getAddress();
  final String serverPort=String.valueOf(rpcServer.getPort());
  if (conf.containsKey(SparkClientFactory.CONF_KEY_IN_PROCESS)) {
    LOG.warn("!!!! Running remote driver in-process. !!!!");
    runnable=new Runnable(){
      @Override public void run(){
        List<String> args=Lists.newArrayList();
        args.add("--remote-host");
        args.add(serverAddress);
        args.add("--remote-port");
        args.add(serverPort);
        args.add("--client-id");
        args.add(clientId);
        args.add("--secret");
        args.add(secret);
        for (        Map.Entry<String,String> e : conf.entrySet()) {
          args.add("--conf");
          args.add(String.format("%s=%s",e.getKey(),conf.get(e.getKey())));
        }
        try {
          RemoteDriver.main(args.toArray(new String[args.size()]));
        }
 catch (        Exception e) {
          LOG.error("Error running driver.",e);
        }
      }
    }
;
  }
 else {
    String sparkHome=Strings.emptyToNull(conf.get(SPARK_HOME_KEY));
    if (sparkHome == null) {
      sparkHome=Strings.emptyToNull(System.getenv(SPARK_HOME_ENV));
    }
    if (sparkHome == null) {
      sparkHome=Strings.emptyToNull(System.getProperty(SPARK_HOME_KEY));
    }
    String sparkLogDir=conf.get("hive.spark.log.dir");
    if (sparkLogDir == null) {
      if (sparkHome == null) {
        sparkLogDir="./target/";
      }
 else {
        sparkLogDir=sparkHome + "/logs/";
      }
    }
    String osxTestOpts="";
    if (Strings.nullToEmpty(System.getProperty("os.name")).toLowerCase().contains("mac")) {
      osxTestOpts=Strings.nullToEmpty(System.getenv(OSX_TEST_OPTS));
    }
    String driverJavaOpts=Joiner.on(" ").skipNulls().join("-Dhive.spark.log.dir=" + sparkLogDir,osxTestOpts,conf.get(DRIVER_OPTS_KEY));
    String executorJavaOpts=Joiner.on(" ").skipNulls().join("-Dhive.spark.log.dir=" + sparkLogDir,osxTestOpts,conf.get(EXECUTOR_OPTS_KEY));
    File properties=File.createTempFile("spark-submit.",".properties");
    if (!properties.setReadable(false) || !properties.setReadable(true,true)) {
      throw new IOException("Cannot change permissions of job properties file.");
    }
    properties.deleteOnExit();
    Properties allProps=new Properties();
    try {
      URL sparkDefaultsUrl=Thread.currentThread().getContextClassLoader().getResource("spark-defaults.conf");
      if (sparkDefaultsUrl != null) {
        LOG.info("Loading spark defaults: " + sparkDefaultsUrl);
        allProps.load(new ByteArrayInputStream(Resources.toByteArray(sparkDefaultsUrl)));
      }
    }
 catch (    Exception e) {
      String msg="Exception trying to load spark-defaults.conf: " + e;
      throw new IOException(msg,e);
    }
    for (    Map.Entry<String,String> e : conf.entrySet()) {
      allProps.put(e.getKey(),conf.get(e.getKey()));
    }
    allProps.put(SparkClientFactory.CONF_CLIENT_ID,clientId);
    allProps.put(SparkClientFactory.CONF_KEY_SECRET,secret);
    allProps.put(DRIVER_OPTS_KEY,driverJavaOpts);
    allProps.put(EXECUTOR_OPTS_KEY,executorJavaOpts);
    String isTesting=conf.get("spark.testing");
    if (isTesting != null && isTesting.equalsIgnoreCase("true")) {
      String hiveHadoopTestClasspath=Strings.nullToEmpty(System.getenv("HIVE_HADOOP_TEST_CLASSPATH"));
      if (!hiveHadoopTestClasspath.isEmpty()) {
        String extraDriverClasspath=Strings.nullToEmpty((String)allProps.get(DRIVER_EXTRA_CLASSPATH));
        if (extraDriverClasspath.isEmpty()) {
          allProps.put(DRIVER_EXTRA_CLASSPATH,hiveHadoopTestClasspath);
        }
 else {
          extraDriverClasspath=extraDriverClasspath.endsWith(File.pathSeparator) ? extraDriverClasspath : extraDriverClasspath + File.pathSeparator;
          allProps.put(DRIVER_EXTRA_CLASSPATH,extraDriverClasspath + hiveHadoopTestClasspath);
        }
        String extraExecutorClasspath=Strings.nullToEmpty((String)allProps.get(EXECUTOR_EXTRA_CLASSPATH));
        if (extraExecutorClasspath.isEmpty()) {
          allProps.put(EXECUTOR_EXTRA_CLASSPATH,hiveHadoopTestClasspath);
        }
 else {
          extraExecutorClasspath=extraExecutorClasspath.endsWith(File.pathSeparator) ? extraExecutorClasspath : extraExecutorClasspath + File.pathSeparator;
          allProps.put(EXECUTOR_EXTRA_CLASSPATH,extraExecutorClasspath + hiveHadoopTestClasspath);
        }
      }
    }
    Writer writer=new OutputStreamWriter(new FileOutputStream(properties),Charsets.UTF_8);
    try {
      allProps.store(writer,"Spark Context configuration");
    }
  finally {
      writer.close();
    }
    String master=conf.get("spark.master");
    Preconditions.checkArgument(master != null,"spark.master is not defined.");
    List<String> argv=Lists.newArrayList();
    if (sparkHome != null) {
      argv.add(new File(sparkHome,"bin/spark-submit").getAbsolutePath());
    }
 else {
      LOG.info("No spark.home provided, calling SparkSubmit directly.");
      argv.add(new File(System.getProperty("java.home"),"bin/java").getAbsolutePath());
      if (master.startsWith("local") || master.startsWith("mesos") || master.endsWith("-client")|| master.startsWith("spark")) {
        String mem=conf.get("spark.driver.memory");
        if (mem != null) {
          argv.add("-Xms" + mem);
          argv.add("-Xmx" + mem);
        }
        String cp=conf.get("spark.driver.extraClassPath");
        if (cp != null) {
          argv.add("-classpath");
          argv.add(cp);
        }
        String libPath=conf.get("spark.driver.extraLibPath");
        if (libPath != null) {
          argv.add("-Djava.library.path=" + libPath);
        }
        String extra=conf.get(DRIVER_OPTS_KEY);
        if (extra != null) {
          for (          String opt : extra.split("[ ]")) {
            if (!opt.trim().isEmpty()) {
              argv.add(opt.trim());
            }
          }
        }
      }
      argv.add("org.apache.spark.deploy.SparkSubmit");
    }
    if ("kerberos".equals(hiveConf.get(HADOOP_SECURITY_AUTHENTICATION))) {
      String principal=SecurityUtil.getServerPrincipal(hiveConf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL),"0.0.0.0");
      String keyTabFile=hiveConf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB);
      argv.add("--principal");
      argv.add(principal);
      argv.add("--keytab");
      argv.add(keyTabFile);
    }
    if (master.equals("yarn-cluster")) {
      String executorCores=conf.get("spark.executor.cores");
      if (executorCores != null) {
        argv.add("--executor-cores");
        argv.add(executorCores);
      }
      String executorMemory=conf.get("spark.executor.memory");
      if (executorMemory != null) {
        argv.add("--executor-memory");
        argv.add(executorMemory);
      }
      String numOfExecutors=conf.get("spark.executor.instances");
      if (numOfExecutors != null) {
        argv.add("--num-executors");
        argv.add(numOfExecutors);
      }
    }
    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {
      try {
        String currentUser=Utils.getUGI().getShortUserName();
        if (!currentUser.equals(System.getProperty("user.name"))) {
          LOG.info("Attempting impersonation of " + currentUser);
          argv.add("--proxy-user");
          argv.add(currentUser);
        }
      }
 catch (      Exception e) {
        String msg="Cannot obtain username: " + e;
        throw new IllegalStateException(msg,e);
      }
    }
    argv.add("--properties-file");
    argv.add(properties.getAbsolutePath());
    argv.add("--class");
    argv.add(RemoteDriver.class.getName());
    String jar="spark-internal";
    if (SparkContext.jarOfClass(this.getClass()).isDefined()) {
      jar=SparkContext.jarOfClass(this.getClass()).get();
    }
    argv.add(jar);
    argv.add("--remote-host");
    argv.add(serverAddress);
    argv.add("--remote-port");
    argv.add(serverPort);
    for (    String hiveSparkConfKey : RpcConfiguration.HIVE_SPARK_RSC_CONFIGS) {
      String value=RpcConfiguration.getValue(hiveConf,hiveSparkConfKey);
      argv.add("--conf");
      argv.add(String.format("%s=%s",hiveSparkConfKey,value));
    }
    String cmd=Joiner.on(" ").join(argv);
    LOG.info("Running client driver with argv: {}",cmd);
    ProcessBuilder pb=new ProcessBuilder("sh","-c",cmd);
    pb.environment().remove("HIVE_HOME");
    pb.environment().remove("HIVE_CONF_DIR");
    if (isTesting != null) {
      pb.environment().put("SPARK_TESTING",isTesting);
    }
    final Process child=pb.start();
    int childId=childIdGenerator.incrementAndGet();
    final List<String> childErrorLog=new ArrayList<String>();
    redirect("stdout-redir-" + childId,new Redirector(child.getInputStream()));
    redirect("stderr-redir-" + childId,new Redirector(child.getErrorStream(),childErrorLog));
    runnable=new Runnable(){
      @Override public void run(){
        try {
          int exitCode=child.waitFor();
          if (exitCode != 0) {
            StringBuilder errStr=new StringBuilder();
            for (            String s : childErrorLog) {
              errStr.append(s);
              errStr.append('\n');
            }
            rpcServer.cancelClient(clientId,"Child process exited before connecting back with error log " + errStr.toString());
            LOG.warn("Child process exited with code {}",exitCode);
          }
        }
 catch (        InterruptedException ie) {
          LOG.warn("Waiting thread interrupted, killing child process.");
          Thread.interrupted();
          child.destroy();
        }
catch (        Exception e) {
          LOG.warn("Exception while waiting for child process.",e);
        }
      }
    }
;
  }
  Thread thread=new Thread(runnable);
  thread.setDaemon(true);
  thread.setName("Driver");
  thread.start();
  return thread;
}
