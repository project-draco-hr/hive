{
  HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,CombineHiveInputFormat.class.getName());
  success=true;
  ShimLoader.getHadoopShims().setNullOutputFormat(job);
  job.setMapperClass(work.getMapperClass());
  Context ctx=driverContext.getCtx();
  boolean ctxCreated=false;
  try {
    if (ctx == null) {
      ctx=new Context(job);
      ctxCreated=true;
    }
  }
 catch (  IOException e) {
    e.printStackTrace();
    console.printError("Error launching map-reduce job","\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  job.setMapOutputKeyClass(NullWritable.class);
  job.setMapOutputValueClass(NullWritable.class);
  if (work.getNumMapTasks() != null) {
    job.setNumMapTasks(work.getNumMapTasks());
  }
  job.setNumReduceTasks(0);
  if (work.getMinSplitSize() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZE,work.getMinSplitSize().longValue());
  }
  if (work.getInputformat() != null) {
    HiveConf.setVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT,work.getInputformat());
  }
  String inpFormat=HiveConf.getVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT);
  if ((inpFormat == null) || (!StringUtils.isNotBlank(inpFormat))) {
    inpFormat=ShimLoader.getHadoopShims().getInputFormatClassName();
  }
  LOG.info("Using " + inpFormat);
  try {
    job.setInputFormat((Class<? extends InputFormat>)(Class.forName(inpFormat)));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage());
  }
  String outputPath=this.work.getOutputDir();
  Path tempOutPath=Utilities.toTempPath(new Path(outputPath));
  try {
    FileSystem fs=tempOutPath.getFileSystem(job);
    if (!fs.exists(tempOutPath)) {
      fs.mkdirs(tempOutPath);
    }
  }
 catch (  IOException e) {
    console.printError("Can't make path " + outputPath + " : "+ e.getMessage());
    return 6;
  }
  RCFileBlockMergeOutputFormat.setMergeOutputPath(job,new Path(outputPath));
  job.setOutputKeyClass(NullWritable.class);
  job.setOutputValueClass(NullWritable.class);
  HiveConf.setBoolVar(job,HiveConf.ConfVars.HIVEMERGECURRENTJOBHASDYNAMICPARTITIONS,work.hasDynamicPartitions());
  int returnVal=0;
  RunningJob rj=null;
  boolean noName=StringUtils.isEmpty(HiveConf.getVar(job,HiveConf.ConfVars.HADOOPJOBNAME));
  String jobName=null;
  if (noName && this.getQueryPlan() != null) {
    int maxlen=conf.getIntVar(HiveConf.ConfVars.HIVEJOBNAMELENGTH);
    jobName=Utilities.abbreviate(this.getQueryPlan().getQueryStr(),maxlen - 6);
  }
  if (noName) {
    HiveConf.setVar(job,HiveConf.ConfVars.HADOOPJOBNAME,jobName != null ? jobName : "JOB" + Utilities.randGen.nextInt());
  }
  try {
    addInputPaths(job,work);
    Utilities.setMapRedWork(job,work,ctx.getMRTmpFileURI());
    String pwd=HiveConf.getVar(job,HiveConf.ConfVars.METASTOREPWD);
    if (pwd != null) {
      HiveConf.setVar(job,HiveConf.ConfVars.METASTOREPWD,"HIVE");
    }
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    rj=jc.submitJob(job);
    returnVal=jobExecHelper.progress(rj,jc);
    success=(returnVal == 0);
  }
 catch (  Exception e) {
    e.printStackTrace();
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    try {
      if (ctxCreated) {
        ctx.clear();
      }
      if (rj != null) {
        if (returnVal != 0) {
          rj.killJob();
        }
        HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());
        jobID=rj.getID().toString();
      }
      RCFileMergeMapper.jobClose(outputPath,success,job,console);
    }
 catch (    Exception e) {
    }
  }
  return (returnVal);
}
