{
  completed=new HashSet<String>();
  boolean running=false;
  boolean done=false;
  int rc=0;
  JobExecutionStatus lastState=null;
  Map<String,SparkStageProgress> lastProgressMap=null;
  long startTime=-1;
  while (true) {
    JobExecutionStatus state=sparkJobStatus.getState();
    try {
      if (LOG.isDebugEnabled()) {
        console.printInfo("state = " + state);
      }
      if (state != null && state != JobExecutionStatus.UNKNOWN && (state != lastState || state == JobExecutionStatus.RUNNING)) {
        lastState=state;
        Map<String,SparkStageProgress> progressMap=sparkJobStatus.getSparkStageProgress();
switch (state) {
case RUNNING:
          if (!running) {
            console.printInfo("\nQuery Hive on Spark job[" + sparkJobStatus.getJobId() + "] stages:");
            for (            int stageId : sparkJobStatus.getStageIds()) {
              console.printInfo(Integer.toString(stageId));
            }
            console.printInfo("\nStatus: Running (Hive on Spark job[" + sparkJobStatus.getJobId() + "])");
            startTime=System.currentTimeMillis();
            running=true;
            console.printInfo("Job Progress Format\nCurrentTime StageId_StageAttemptId: " + "SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]");
          }
        printStatus(progressMap,lastProgressMap);
      lastProgressMap=progressMap;
    break;
case SUCCEEDED:
  printStatus(progressMap,lastProgressMap);
lastProgressMap=progressMap;
if (startTime < 0) {
console.printInfo("Status: Finished successfully within a check interval.");
}
 else {
double duration=(System.currentTimeMillis() - startTime) / 1000.0;
console.printInfo("Status: Finished successfully in " + String.format("%.2f seconds",duration));
}
running=false;
done=true;
break;
case FAILED:
console.printError("Status: Failed");
running=false;
done=true;
rc=2;
break;
}
}
if (!done) {
Thread.sleep(checkInterval);
}
}
 catch (Exception e) {
String msg=" with exception '" + Utilities.getNameMessage(e) + "'";
if (state == null || state.equals(JobExecutionStatus.UNKNOWN)) {
msg="Job Submission failed" + msg;
}
 else {
msg="Ended Job = " + sparkJobStatus.getJobId() + msg;
}
LOG.error(msg,e);
console.printError(msg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
rc=1;
}
 finally {
if (done) {
break;
}
}
}
return rc;
}
