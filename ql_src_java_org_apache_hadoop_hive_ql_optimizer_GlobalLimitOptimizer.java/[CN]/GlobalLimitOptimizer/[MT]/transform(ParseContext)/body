{
  Context ctx=pctx.getContext();
  Map<String,Operator<? extends OperatorDesc>> topOps=pctx.getTopOps();
  GlobalLimitCtx globalLimitCtx=pctx.getGlobalLimitCtx();
  Map<TableScanOperator,ExprNodeDesc> opToPartPruner=pctx.getOpToPartPruner();
  Map<String,SplitSample> nameToSplitSample=pctx.getNameToSplitSample();
  Map<TableScanOperator,Table> topToTable=pctx.getTopToTable();
  QB qb=pctx.getQB();
  HiveConf conf=pctx.getConf();
  QBParseInfo qbParseInfo=qb.getParseInfo();
  if (ctx.getTryCount() == 0 && topOps.size() == 1 && !globalLimitCtx.ifHasTransformOrUDTF() && nameToSplitSample.isEmpty()) {
    Integer tempGlobalLimit=checkQbpForGlobalLimit(qb);
    if (tempGlobalLimit != null && tempGlobalLimit != 0) {
      TableScanOperator ts=(TableScanOperator)topOps.values().toArray()[0];
      Table tab=topToTable.get(ts);
      if (!tab.isPartitioned()) {
        if (qbParseInfo.getDestToWhereExpr().isEmpty()) {
          globalLimitCtx.enableOpt(tempGlobalLimit);
        }
      }
 else {
        if (PartitionPruner.onlyContainsPartnCols(tab,opToPartPruner.get(ts))) {
          PrunedPartitionList partsList;
          try {
            String alias=(String)topOps.keySet().toArray()[0];
            partsList=PartitionPruner.prune(ts,pctx,alias);
          }
 catch (          HiveException e) {
            LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
            throw new SemanticException(e.getMessage(),e);
          }
          if ((partsList.getUnknownPartns().size() == 0)) {
            globalLimitCtx.enableOpt(tempGlobalLimit);
          }
        }
      }
      if (globalLimitCtx.isEnable()) {
        LOG.info("Qualify the optimize that reduces input size for 'limit' for limit " + globalLimitCtx.getGlobalLimit());
      }
    }
  }
  return pctx;
}
