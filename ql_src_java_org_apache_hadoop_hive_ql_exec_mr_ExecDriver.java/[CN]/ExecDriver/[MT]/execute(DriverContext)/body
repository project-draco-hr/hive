{
  IOPrepareCache ioPrepareCache=IOPrepareCache.get();
  ioPrepareCache.clear();
  boolean success=true;
  Context ctx=driverContext.getCtx();
  boolean ctxCreated=false;
  Path emptyScratchDir;
  MapWork mWork=work.getMapWork();
  ReduceWork rWork=work.getReduceWork();
  try {
    if (ctx == null) {
      ctx=new Context(job);
      ctxCreated=true;
    }
    emptyScratchDir=ctx.getMRTmpPath();
    FileSystem fs=emptyScratchDir.getFileSystem(job);
    fs.mkdirs(emptyScratchDir);
  }
 catch (  IOException e) {
    e.printStackTrace();
    console.printError("Error launching map-reduce job","\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    return 5;
  }
  HiveFileFormatUtils.prepareJobOutput(job);
  job.setOutputFormat(HiveOutputFormatImpl.class);
  job.setMapperClass(ExecMapper.class);
  job.setMapOutputKeyClass(HiveKey.class);
  job.setMapOutputValueClass(BytesWritable.class);
  try {
    String partitioner=HiveConf.getVar(job,ConfVars.HIVEPARTITIONER);
    job.setPartitionerClass(JavaUtils.loadClass(partitioner));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage(),e);
  }
  if (mWork.getNumMapTasks() != null) {
    job.setNumMapTasks(mWork.getNumMapTasks().intValue());
  }
  if (mWork.getMaxSplitSize() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMAXSPLITSIZE,mWork.getMaxSplitSize().longValue());
  }
  if (mWork.getMinSplitSize() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZE,mWork.getMinSplitSize().longValue());
  }
  if (mWork.getMinSplitSizePerNode() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZEPERNODE,mWork.getMinSplitSizePerNode().longValue());
  }
  if (mWork.getMinSplitSizePerRack() != null) {
    HiveConf.setLongVar(job,HiveConf.ConfVars.MAPREDMINSPLITSIZEPERRACK,mWork.getMinSplitSizePerRack().longValue());
  }
  job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);
  job.setReducerClass(ExecReducer.class);
  setInputAttributes(job);
  boolean useSpeculativeExecReducers=HiveConf.getBoolVar(job,HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);
  job.setBoolean(MRJobConfig.REDUCE_SPECULATIVE,useSpeculativeExecReducers);
  String inpFormat=HiveConf.getVar(job,HiveConf.ConfVars.HIVEINPUTFORMAT);
  if (mWork.isUseBucketizedHiveInputFormat()) {
    inpFormat=BucketizedHiveInputFormat.class.getName();
  }
  LOG.info("Using " + inpFormat);
  try {
    job.setInputFormat(JavaUtils.loadClass(inpFormat));
  }
 catch (  ClassNotFoundException e) {
    throw new RuntimeException(e.getMessage(),e);
  }
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(Text.class);
  String auxJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEAUXJARS);
  String addedJars=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDJARS);
  if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
    String allJars=StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars + "," + auxJars : auxJars) : addedJars;
    LOG.info("adding libjars: " + allJars);
    initializeFiles("tmpjars",allJars);
  }
  String addedFiles=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDFILES);
  if (StringUtils.isNotBlank(addedFiles)) {
    initializeFiles("tmpfiles",addedFiles);
  }
  int returnVal=0;
  boolean noName=StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));
  if (noName) {
    job.set(MRJobConfig.JOB_NAME,"JOB" + Utilities.randGen.nextInt());
  }
  String addedArchives=HiveConf.getVar(job,HiveConf.ConfVars.HIVEADDEDARCHIVES);
  if (StringUtils.isNotBlank(addedArchives)) {
    initializeFiles("tmparchives",addedArchives);
  }
  try {
    MapredLocalWork localwork=mWork.getMapRedLocalWork();
    if (localwork != null && localwork.hasStagedAlias()) {
      if (!ShimLoader.getHadoopShims().isLocalMode(job)) {
        Path localPath=localwork.getTmpPath();
        Path hdfsPath=mWork.getTmpHDFSPath();
        FileSystem hdfs=hdfsPath.getFileSystem(job);
        FileSystem localFS=localPath.getFileSystem(job);
        FileStatus[] hashtableFiles=localFS.listStatus(localPath);
        int fileNumber=hashtableFiles.length;
        String[] fileNames=new String[fileNumber];
        for (int i=0; i < fileNumber; i++) {
          fileNames[i]=hashtableFiles[i].getPath().getName();
        }
        String stageId=this.getId();
        String archiveFileName=Utilities.generateTarFileName(stageId);
        localwork.setStageID(stageId);
        CompressionUtils.tar(localPath.toUri().getPath(),fileNames,archiveFileName);
        Path archivePath=Utilities.generateTarPath(localPath,stageId);
        LOG.info("Archive " + hashtableFiles.length + " hash table files to "+ archivePath);
        Path hdfsFilePath=Utilities.generateTarPath(hdfsPath,stageId);
        short replication=(short)job.getInt("mapred.submit.replication",10);
        hdfs.copyFromLocalFile(archivePath,hdfsFilePath);
        hdfs.setReplication(hdfsFilePath,replication);
        LOG.info("Upload 1 archive file  from" + archivePath + " to: "+ hdfsFilePath);
        DistributedCache.createSymlink(job);
        DistributedCache.addCacheArchive(hdfsFilePath.toUri(),job);
        LOG.info("Add 1 archive file to distributed cache. Archive file: " + hdfsFilePath.toUri());
      }
    }
    work.configureJobConf(job);
    List<Path> inputPaths=Utilities.getInputPaths(job,mWork,emptyScratchDir,ctx,false);
    Utilities.setInputPaths(job,inputPaths);
    Utilities.setMapRedWork(job,work,ctx.getMRTmpPath());
    if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {
      try {
        handleSampling(ctx,mWork,job);
        job.setPartitionerClass(HiveTotalOrderPartitioner.class);
      }
 catch (      IllegalStateException e) {
        console.printInfo("Not enough sampling data.. Rolling back to single reducer task");
        rWork.setNumReduceTasks(1);
        job.setNumReduceTasks(1);
      }
catch (      Exception e) {
        LOG.error("Sampling error",e);
        console.printError(e.toString(),"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
        rWork.setNumReduceTasks(1);
        job.setNumReduceTasks(1);
      }
    }
    JobClient jc=new JobClient(job);
    Throttle.checkJobTracker(job,LOG);
    if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {
      StatsPublisher statsPublisher;
      StatsFactory factory=StatsFactory.newFactory(job);
      if (factory != null) {
        statsPublisher=factory.getStatsPublisher();
        List<String> statsTmpDir=Utilities.getStatsTmpDirs(mWork,job);
        if (rWork != null) {
          statsTmpDir.addAll(Utilities.getStatsTmpDirs(rWork,job));
        }
        StatsCollectionContext sc=new StatsCollectionContext(job);
        sc.setStatsTmpDirs(statsTmpDir);
        if (!statsPublisher.init(sc)) {
          if (HiveConf.getBoolVar(job,HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
            throw new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
          }
        }
      }
    }
    Utilities.createTmpDirs(job,mWork);
    Utilities.createTmpDirs(job,rWork);
    SessionState ss=SessionState.get();
    if (HiveConf.getVar(job,HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") && ss != null) {
      TezSessionState session=ss.getTezSession();
      TezSessionPoolManager.getInstance().closeIfNotDefault(session,true);
    }
    rj=jc.submitJob(job);
    this.jobID=rj.getJobID();
    returnVal=jobExecHelper.progress(rj,jc);
    success=(returnVal == 0);
  }
 catch (  Exception e) {
    e.printStackTrace();
    String mesg=" with exception '" + Utilities.getNameMessage(e) + "'";
    if (rj != null) {
      mesg="Ended Job = " + rj.getJobID() + mesg;
    }
 else {
      mesg="Job Submission failed" + mesg;
    }
    console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    success=false;
    returnVal=1;
  }
 finally {
    Utilities.clearWork(job);
    try {
      if (ctxCreated) {
        ctx.clear();
      }
      if (rj != null) {
        if (returnVal != 0) {
          rj.killJob();
        }
        jobID=rj.getID().toString();
      }
    }
 catch (    Exception e) {
      LOG.warn("Failed while cleaning up ",e);
    }
 finally {
      HadoopJobExecHelper.runningJobs.remove(rj);
    }
  }
  try {
    if (rj != null) {
      if (mWork.getAliasToWork() != null) {
        for (        Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {
          op.jobClose(job,success);
        }
      }
      if (rWork != null) {
        rWork.getReducer().jobClose(job,success);
      }
    }
  }
 catch (  Exception e) {
    if (success) {
      success=false;
      returnVal=3;
      String mesg="Job Commit failed with exception '" + Utilities.getNameMessage(e) + "'";
      console.printError(mesg,"\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
    }
  }
  return (returnVal);
}
