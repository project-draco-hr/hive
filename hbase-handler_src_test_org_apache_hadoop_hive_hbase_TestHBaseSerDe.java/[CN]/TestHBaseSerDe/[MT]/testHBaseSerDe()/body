{
  HBaseSerDe serDe=new HBaseSerDe();
  Configuration conf=new Configuration();
  Properties tbl=createProperties();
  serDe.initialize(conf,tbl);
  byte[] colabyte="cola:abyte".getBytes();
  byte[] colbshort="colb:ashort".getBytes();
  byte[] colcint="colc:aint".getBytes();
  byte[] colalong="cola:along".getBytes();
  byte[] colbdouble="colb:adouble".getBytes();
  byte[] colcstring="colc:astring".getBytes();
  HbaseMapWritable<byte[],Cell> cells=new HbaseMapWritable<byte[],Cell>();
  cells.put(colabyte,new Cell("123".getBytes(),0));
  cells.put(colbshort,new Cell("456".getBytes(),0));
  cells.put(colcint,new Cell("789".getBytes(),0));
  cells.put(colalong,new Cell("1000".getBytes(),0));
  cells.put(colbdouble,new Cell("5.3".getBytes(),0));
  cells.put(colcstring,new Cell("hive and hadoop".getBytes(),0));
  RowResult rr=new RowResult("test-row1".getBytes(),cells);
  BatchUpdate bu=new BatchUpdate("test-row1".getBytes());
  bu.put(colabyte,"123".getBytes());
  bu.put(colbshort,"456".getBytes());
  bu.put(colcint,"789".getBytes());
  bu.put(colalong,"1000".getBytes());
  bu.put(colbdouble,"5.3".getBytes());
  bu.put(colcstring,"hive and hadoop".getBytes());
  Object[] expectedFieldsData={new Text("test-row1"),new ByteWritable((byte)123),new ShortWritable((short)456),new IntWritable(789),new LongWritable(1000),new DoubleWritable(5.3),new Text("hive and hadoop")};
  deserializeAndSerialize(serDe,rr,bu,expectedFieldsData);
}
