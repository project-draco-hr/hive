{
  HBaseSerDe serDe=new HBaseSerDe();
  Configuration conf=new Configuration();
  Properties tbl=createProperties();
  serDe.initialize(conf,tbl);
  byte[] colabyte="cola:abyte".getBytes();
  byte[] colbshort="colb:ashort".getBytes();
  byte[] colcint="colc:aint".getBytes();
  byte[] colalong="cola:along".getBytes();
  byte[] colbdouble="colb:adouble".getBytes();
  byte[] colcstring="colc:astring".getBytes();
  List<KeyValue> kvs=new ArrayList<KeyValue>();
  kvs.add(new KeyValue(Bytes.toBytes("test-row1"),colabyte,0,Bytes.toBytes("123")));
  kvs.add(new KeyValue(Bytes.toBytes("test-row1"),colbshort,0,Bytes.toBytes("456")));
  kvs.add(new KeyValue(Bytes.toBytes("test-row1"),colcint,0,Bytes.toBytes("789")));
  kvs.add(new KeyValue(Bytes.toBytes("test-row1"),colalong,0,Bytes.toBytes("1000")));
  kvs.add(new KeyValue(Bytes.toBytes("test-row1"),colbdouble,0,Bytes.toBytes("5.3")));
  kvs.add(new KeyValue(Bytes.toBytes("test-row1"),colcstring,0,Bytes.toBytes("hive and hadoop")));
  Result r=new Result(kvs);
  Put p=new Put(Bytes.toBytes("test-row1"));
  p.add(colabyte,0,Bytes.toBytes("123"));
  p.add(colbshort,0,Bytes.toBytes("456"));
  p.add(colcint,0,Bytes.toBytes("789"));
  p.add(colalong,0,Bytes.toBytes("1000"));
  p.add(colbdouble,0,Bytes.toBytes("5.3"));
  p.add(colcstring,0,Bytes.toBytes("hive and hadoop"));
  Object[] expectedFieldsData={new Text("test-row1"),new ByteWritable((byte)123),new ShortWritable((short)456),new IntWritable(789),new LongWritable(1000),new DoubleWritable(5.3),new Text("hive and hadoop")};
  deserializeAndSerialize(serDe,r,p,expectedFieldsData);
}
