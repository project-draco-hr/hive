{
  HiveMetaStoreClient client=null;
  HiveConf hiveConf=null;
  try {
    if (conf != null) {
      hiveConf=HCatUtil.getHiveConf(conf);
    }
 else {
      hiveConf=new HiveConf(HCatInputFormat.class);
    }
    client=HCatUtil.getHiveClient(hiveConf);
    Table table=HCatUtil.getTable(client,inputJobInfo.getDatabaseName(),inputJobInfo.getTableName());
    List<PartInfo> partInfoList=new ArrayList<PartInfo>();
    inputJobInfo.setTableInfo(HCatTableInfo.valueOf(table.getTTable()));
    if (table.getPartitionKeys().size() != 0) {
      List<Partition> parts=client.listPartitionsByFilter(inputJobInfo.getDatabaseName(),inputJobInfo.getTableName(),inputJobInfo.getFilter(),(short)-1);
      int maxPart=hiveConf.getInt("hcat.metastore.maxpartitions",100000);
      if (parts != null && parts.size() > maxPart) {
        throw new HCatException(ErrorType.ERROR_EXCEED_MAXPART,"total number of partitions is " + parts.size());
      }
      for (      Partition ptn : parts) {
        HCatSchema schema=HCatUtil.extractSchema(new org.apache.hadoop.hive.ql.metadata.Partition(table,ptn));
        PartInfo partInfo=extractPartInfo(schema,ptn.getSd(),ptn.getParameters(),conf,inputJobInfo);
        partInfo.setPartitionValues(InternalUtil.createPtnKeyValueMap(table,ptn));
        partInfoList.add(partInfo);
      }
    }
 else {
      HCatSchema schema=HCatUtil.extractSchema(table);
      PartInfo partInfo=extractPartInfo(schema,table.getTTable().getSd(),table.getParameters(),conf,inputJobInfo);
      partInfo.setPartitionValues(new HashMap<String,String>());
      partInfoList.add(partInfo);
    }
    inputJobInfo.setPartitions(partInfoList);
    return inputJobInfo;
  }
  finally {
    HCatUtil.closeHiveClientQuietly(client);
  }
}
