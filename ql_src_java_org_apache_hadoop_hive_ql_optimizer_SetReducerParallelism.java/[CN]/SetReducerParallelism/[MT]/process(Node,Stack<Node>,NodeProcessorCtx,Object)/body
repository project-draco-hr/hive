{
  OptimizeTezProcContext context=(OptimizeTezProcContext)procContext;
  ReduceSinkOperator sink=(ReduceSinkOperator)nd;
  ReduceSinkDesc desc=sink.getConf();
  long bytesPerReducer=context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);
  int maxReducers=context.conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);
  int constantReducers=context.conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);
  if (context.visitedReduceSinks.contains(sink)) {
    LOG.debug("Already processed reduce sink: " + sink.getName());
    return true;
  }
  context.visitedReduceSinks.add(sink);
  if (desc.getNumReducers() <= 0) {
    if (constantReducers > 0) {
      LOG.info("Parallelism for reduce sink " + sink + " set by user to "+ constantReducers);
      desc.setNumReducers(constantReducers);
    }
 else {
      long numberOfBytes=0;
      for (      Operator<? extends OperatorDesc> sibling : sink.getChildOperators().get(0).getParentOperators()) {
        if (sibling.getStatistics() != null) {
          numberOfBytes+=sibling.getStatistics().getDataSize();
        }
 else {
          LOG.warn("No stats available from: " + sibling);
        }
      }
      int numReducers=Utilities.estimateReducers(numberOfBytes,bytesPerReducer,maxReducers,false);
      LOG.info("Set parallelism for reduce sink " + sink + " to: "+ numReducers);
      desc.setNumReducers(numReducers);
      final Collection<ExprNodeDescEqualityWrapper> keyCols=ExprNodeDescEqualityWrapper.transform(desc.getKeyCols());
      final Collection<ExprNodeDescEqualityWrapper> partCols=ExprNodeDescEqualityWrapper.transform(desc.getPartitionCols());
      if (keyCols != null && keyCols.equals(partCols)) {
        desc.setReducerTraits(EnumSet.of(UNIFORM,AUTOPARALLEL));
      }
 else {
        desc.setReducerTraits(EnumSet.of(AUTOPARALLEL));
      }
    }
  }
 else {
    LOG.info("Number of reducers determined to be: " + desc.getNumReducers());
  }
  return false;
}
